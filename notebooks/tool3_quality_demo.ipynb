{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbd5ba41",
   "metadata": {},
   "source": [
    "# Tool 3 - Quality Validator Demo (LangGraph Nodes)\n",
    "\n",
    "**Purpose:** Validate metadata quality using hybrid approach (deterministic heuristics + LLM enhancement layer).\n",
    "\n",
    "**LangGraph Features:**\n",
    "- ‚úÖ 4-node pipeline with deterministic + LLM nodes\n",
    "- ‚úÖ Structured output (ToolStrategy) for LLM enhancement only\n",
    "- ‚úÖ Shared state (Tool3State) across all nodes\n",
    "- ‚úÖ Fallback strategy for LLM timeouts/errors\n",
    "- ‚úÖ Hallucination mitigation via entity ID validation\n",
    "- ‚úÖ P0-P2 prioritized recommendations\n",
    "\n",
    "**Architecture:**\n",
    "```\n",
    "Load & Validate ‚Üí Calculate Deterministic ‚Üí Enhance with LLM ‚Üí Merge & Serialize\n",
    "       ‚Üì                    ‚Üì                      ‚Üì                    ‚Üì\n",
    "  structure.json +    Articulation scores    Risk assessment     quality_report.json\n",
    "  business_context    + validation flags     + recommendations   + audit summary\n",
    "  + full metadata     + missing entities     + anomaly notes     (P0-P2 priorities)\n",
    "```\n",
    "\n",
    "**Model:** Azure OpenAI gpt-5-mini via AzureChatOpenAI (LangChain wrapper) - Node 3 only\n",
    "\n",
    "**Key Inputs:**\n",
    "1. `data/tool2/structure.json` - facts, dimensions, relationships\n",
    "2. `data/tool0_samples/*.json` - business context (entities, scope_out)\n",
    "3. `docs_langgraph/BA-BS_Datamarts_metadata.json` - full metadata for quality checks\n",
    "\n",
    "**Status:** ‚úÖ Architecture designed | ‚è≥ Ready to implement\n",
    "\n",
    "**Configuration:** Uses `.env` file with AZURE_OPENAI_ENDPOINT, AZURE_OPENAI_API_KEY, AZURE_OPENAI_DEPLOYMENT_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8dd83784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports loaded\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "from pydantic import BaseModel, Field, field_validator\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Literal, TypedDict\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LangChain imports (Node 3 only)\n",
    "from langchain.agents import create_agent\n",
    "from langchain.agents.structured_output import ToolStrategy\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "print(\"‚úÖ Imports loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ace00743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚òÅÔ∏è Azure OpenAI configured for LangChain (Node 3 only)\n",
      "   Endpoint (raw): https://minar-mhi2wuzy-swedencentral.cognitiveservices.azure.com/openai/v1/\n",
      "   Endpoint (agent): https://minar-mhi2wuzy-swedencentral.cognitiveservices.azure.com\n",
      "   Deployment: test-gpt-5-mini\n"
     ]
    }
   ],
   "source": [
    "# Configure Azure OpenAI for LangChain agents (Node 3 only)\n",
    "load_dotenv()\n",
    "\n",
    "AZURE_ENDPOINT_RAW = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "DEPLOYMENT_NAME = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "\n",
    "if not all([AZURE_ENDPOINT_RAW, AZURE_API_KEY, DEPLOYMENT_NAME]):\n",
    "    raise ValueError(\"Missing Azure configuration in .env file\")\n",
    "\n",
    "def _normalize_azure_endpoint(endpoint: str | None) -> str | None:\n",
    "    \"\"\"Strip Azure REST suffixes so LangChain builds the correct base URL.\"\"\"\n",
    "    if endpoint is None:\n",
    "        return None\n",
    "    trimmed = endpoint.rstrip(\"/\")\n",
    "    for suffix in (\"/openai/v1\", \"/openai\"):\n",
    "        if trimmed.endswith(suffix):\n",
    "            trimmed = trimmed[: -len(suffix)]\n",
    "            break\n",
    "    return trimmed\n",
    "\n",
    "AZURE_AGENT_ENDPOINT = _normalize_azure_endpoint(AZURE_ENDPOINT_RAW)\n",
    "\n",
    "AZURE_LLM = AzureChatOpenAI(\n",
    "    azure_endpoint=AZURE_AGENT_ENDPOINT,\n",
    "    api_key=AZURE_API_KEY,\n",
    "    azure_deployment=DEPLOYMENT_NAME,\n",
    "    api_version=\"2024-10-21\"\n",
    ")\n",
    "\n",
    "print(\"‚òÅÔ∏è Azure OpenAI configured for LangChain (Node 3 only)\")\n",
    "print(f\"   Endpoint (raw): {AZURE_ENDPOINT_RAW}\")\n",
    "print(f\"   Endpoint (agent): {AZURE_AGENT_ENDPOINT}\")\n",
    "print(f\"   Deployment: {DEPLOYMENT_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63ce2af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Schemas defined\n",
      "   - Recommendation: 6 fields\n",
      "   - AnomalyNote: 4 fields\n",
      "   - LLMEnhancement: 7 fields\n",
      "   - QualityReport: 12 fields\n",
      "   - Tool3State: 10 state fields\n"
     ]
    }
   ],
   "source": [
    "# Pydantic schemas for Quality Validator\n",
    "\n",
    "class Recommendation(BaseModel):\n",
    "    \"\"\"Single actionable recommendation (P0-P2 priority).\"\"\"\n",
    "    priority: Literal[\"P0\", \"P1\", \"P2\"] = Field(\n",
    "        description=\"Priority level: P0=blocker (missing critical fields), P1=quality issue, P2=nice-to-have\"\n",
    "    )\n",
    "    entity_id: str | None = Field(\n",
    "        description=\"Affected entity table_id, null if project-wide recommendation\"\n",
    "    )\n",
    "    issue_type: str = Field(\n",
    "        description=\"Issue category (e.g., 'MISSING_DESCRIPTION', 'LOW_ARTICULATION', 'MISSING_OWNER')\"\n",
    "    )\n",
    "    description: str = Field(\n",
    "        description=\"User-friendly explanation of the issue\"\n",
    "    )\n",
    "    action: str = Field(\n",
    "        description=\"Specific action to resolve (e.g., 'Add description in Collibra for entity dimv_supplier')\"\n",
    "    )\n",
    "    estimated_impact: str = Field(\n",
    "        description=\"Expected improvement (e.g., '+20 articulation score', 'unblock production deployment')\"\n",
    "    )\n",
    "\n",
    "class AnomalyNote(BaseModel):\n",
    "    \"\"\"Structural anomaly detected by LLM.\"\"\"\n",
    "    entity_id: str = Field(description=\"Entity with anomaly\")\n",
    "    anomaly_type: str = Field(\n",
    "        description=\"Type: 'UNEXPECTED_FIELD', 'NAME_MISMATCH', 'SCHEMA_DRIFT', 'VALUE_OUTLIER'\"\n",
    "    )\n",
    "    severity: Literal[\"high\", \"medium\", \"low\"] = Field(\n",
    "        description=\"Impact level on data quality\"\n",
    "    )\n",
    "    explanation: str = Field(\n",
    "        description=\"Why this is anomalous and potential impact\"\n",
    "    )\n",
    "\n",
    "class LLMEnhancement(BaseModel):\n",
    "    \"\"\"LLM output schema for ToolStrategy (Node 3 only).\"\"\"\n",
    "    risk_level: Literal[\"high\", \"medium\", \"low\"] = Field(\n",
    "        description=\"Overall metadata quality risk: high=P0 blockers present, medium=P1 issues, low=minor P2 only\"\n",
    "    )\n",
    "    risk_rationale: str = Field(\n",
    "        description=\"2-3 sentence explanation why this risk level was assigned\"\n",
    "    )\n",
    "    text_quality_score: float | None = Field(\n",
    "        description=\"Subjective quality of descriptions/naming (0.0-1.0), null if not assessed\",\n",
    "        ge=0.0,\n",
    "        le=1.0,\n",
    "        default=None\n",
    "    )\n",
    "    text_quality_notes: str | None = Field(\n",
    "        description=\"Explanation of text quality assessment (brevity, clarity, terminology)\",\n",
    "        default=None\n",
    "    )\n",
    "    recommendations: list[Recommendation] = Field(\n",
    "        description=\"Prioritized list of fixes, P0 blockers first\"\n",
    "    )\n",
    "    anomaly_notes: list[AnomalyNote] = Field(\n",
    "        description=\"Structural outliers or unexpected patterns\",\n",
    "        default_factory=list\n",
    "    )\n",
    "    summary: str = Field(\n",
    "        description=\"2-3 sentence summary for governance report (non-technical audience)\"\n",
    "    )\n",
    "\n",
    "class QualityMetrics(BaseModel):\n",
    "    \"\"\"Summary quality metrics.\"\"\"\n",
    "    total_entities: int = Field(description=\"Total entities analyzed\")\n",
    "    avg_articulation_score: float = Field(description=\"Average articulation score (0-100)\")\n",
    "    entities_with_issues: int = Field(description=\"Count of entities with warnings/failures\")\n",
    "    p0_blockers: int = Field(description=\"Count of P0 priority blockers\")\n",
    "    coverage: float = Field(description=\"Coverage percentage (0.0-1.0)\")\n",
    "\n",
    "class QualityReport(BaseModel):\n",
    "    \"\"\"Final output schema (merge of deterministic + LLM results).\"\"\"\n",
    "    schema_version: str = Field(description=\"Schema version for backward compatibility\", default=\"1.0.0\")\n",
    "    timestamp: str = Field(description=\"Analysis timestamp in ISO 8601 format\")\n",
    "    source_files: dict = Field(\n",
    "        description=\"Paths to input files (structure.json, business_context.json)\"\n",
    "    )\n",
    "\n",
    "    # Deterministic results (Node 2)\n",
    "    articulation_scores: dict[str, int] = Field(\n",
    "        description=\"Entity_id ‚Üí articulation_score (0-100) mapping\"\n",
    "    )\n",
    "    validation_results: dict[str, str] = Field(\n",
    "        description=\"Entity_id ‚Üí validation_result ('pass'|'warning'|'fail') mapping\"\n",
    "    )\n",
    "    missing_from_source: list[str] = Field(\n",
    "        description=\"List of business entities not found in structure.json\"\n",
    "    )\n",
    "\n",
    "    # LLM enhancement (Node 3)\n",
    "    risk_level: str = Field(description=\"high | medium | low\")\n",
    "    risk_rationale: str = Field(description=\"Explanation for risk level\")\n",
    "    recommendations: list[Recommendation] = Field(description=\"P0-P2 prioritized recommendations\")\n",
    "    anomaly_notes: list[AnomalyNote] = Field(description=\"Detected anomalies\", default_factory=list)\n",
    "    summary: str = Field(description=\"Executive summary\")\n",
    "\n",
    "    # Metrics (computed in Node 4)\n",
    "    metrics: QualityMetrics = Field(description=\"Summary stats\")\n",
    "\n",
    "# LangGraph State (TypedDict pattern from Tool 1/2)\n",
    "class Tool3State(TypedDict, total=False):\n",
    "    \"\"\"Shared state across all Tool 3 nodes.\"\"\"\n",
    "    # Inputs (Node 1)\n",
    "    structure: dict\n",
    "    business_context: dict\n",
    "    metadata: dict\n",
    "    # Node 2 outputs (deterministic)\n",
    "    entity_scores: dict[str, int]\n",
    "    validation_flags: dict[str, str]\n",
    "    missing_entities: list[str]\n",
    "    # Node 3 outputs (LLM)\n",
    "    llm_enhancements: LLMEnhancement\n",
    "    llm_fallback_mode: bool\n",
    "    # Node 4 outputs\n",
    "    final_report: QualityReport\n",
    "    output_path: str\n",
    "\n",
    "print(\"‚úÖ Schemas defined\")\n",
    "print(f\"   - Recommendation: {len(Recommendation.model_fields)} fields\")\n",
    "print(f\"   - AnomalyNote: {len(AnomalyNote.model_fields)} fields\")\n",
    "print(f\"   - LLMEnhancement: {len(LLMEnhancement.model_fields)} fields\")\n",
    "print(f\"   - QualityReport: {len(QualityReport.model_fields)} fields\")\n",
    "print(f\"   - Tool3State: {len(Tool3State.__annotations__)} state fields\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28d42712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Node 1 (load_and_validate) defined\n"
     ]
    }
   ],
   "source": [
    "def load_and_validate(state: Tool3State) -> Tool3State:\n",
    "    \"\"\"\n",
    "    Node 1: Load structure.json + business_context + full metadata.\n",
    "\n",
    "    Inputs:\n",
    "    - data/tool2/structure.json (facts, dimensions, relationships)\n",
    "    - data/tool0_samples/*.json (business context)\n",
    "    - docs_langgraph/BA-BS_Datamarts_metadata.json (full metadata)\n",
    "\n",
    "    Outputs:\n",
    "    - structure: Tool 2 structural analysis\n",
    "    - business_context: Business entities + scope\n",
    "    - metadata: Full metadata for quality checks\n",
    "    \"\"\"\n",
    "    print(\"üîÑ Node 1: Loading and validating inputs...\")\n",
    "\n",
    "    # Load structure.json from Tool 2\n",
    "    structure_path = Path(\"../data/tool2/structure.json\")\n",
    "    if not structure_path.exists():\n",
    "        raise FileNotFoundError(f\"Structure file not found: {structure_path}\")\n",
    "\n",
    "    with open(structure_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        structure = json.load(f)\n",
    "\n",
    "    # Load business context from Tool 0 (use most recent sample)\n",
    "    tool0_dir = Path(\"../data/tool0_samples\")\n",
    "    context_files = sorted(tool0_dir.glob(\"*.json\"), reverse=True)\n",
    "    if not context_files:\n",
    "        raise FileNotFoundError(f\"No business context files found in {tool0_dir}\")\n",
    "\n",
    "    context_path = context_files[0]  # Most recent\n",
    "    with open(context_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        business_context = json.load(f)\n",
    "\n",
    "    # Load full metadata with flatten logic\n",
    "    metadata_path = Path(\"../docs_langgraph/BA-BS_Datamarts_metadata.json\")\n",
    "    with open(metadata_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        metadata_raw = json.load(f)\n",
    "        # Flatten nested array: [[{...}]] -> [{...}]\n",
    "        full_metadata = []\n",
    "        if isinstance(metadata_raw, list):\n",
    "            for item in metadata_raw:\n",
    "                if isinstance(item, list):\n",
    "                    full_metadata.extend(item)\n",
    "                else:\n",
    "                    full_metadata.append(item)\n",
    "        else:\n",
    "            full_metadata = [metadata_raw] if isinstance(metadata_raw, dict) else []\n",
    "\n",
    "    print(f\"‚úÖ Loaded structure: {len(structure.get('facts', []))} facts, {len(structure.get('dimensions', []))} dimensions\")\n",
    "    print(f\"‚úÖ Loaded business context: {context_path.name}\")\n",
    "    print(f\"‚úÖ Loaded metadata: {len(full_metadata)} entities (flattened)\")\n",
    "\n",
    "    return {\n",
    "        **state,\n",
    "        \"structure\": structure,\n",
    "        \"business_context\": business_context,\n",
    "        \"metadata\": full_metadata\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Node 1 (load_and_validate) defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e71fa9e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Node 2 (calculate_deterministic) defined\n"
     ]
    }
   ],
   "source": [
    "def calculate_articulation_score(entity_metadata: dict) -> int:\n",
    "    \"\"\"\n",
    "    Deterministic scoring based on field presence.\n",
    "    Weights aligned with DQ audit findings (28.1/100 baseline).\n",
    "\n",
    "    Scoring:\n",
    "    - P0 Critical (40 pts): description (20) + owner (20)\n",
    "    - P1 Important (30 pts): lineage (15) + source_mapping (15)\n",
    "    - P2 Nice-to-have (30 pts): dq_rules (10) + governance_tags (10) + last_updated (10)\n",
    "    \"\"\"\n",
    "    score = 0\n",
    "\n",
    "    # P0 Critical fields\n",
    "    if entity_metadata.get(\"description\") and entity_metadata[\"description\"] not in [None, \"\", \"unknown\"]:\n",
    "        score += 20\n",
    "    if entity_metadata.get(\"owner\") and entity_metadata[\"owner\"] not in [None, \"\", \"unknown\"]:\n",
    "        score += 20\n",
    "\n",
    "    # P1 Important fields\n",
    "    if entity_metadata.get(\"lineage\") and entity_metadata[\"lineage\"] not in [\"unknown\", None]:\n",
    "        score += 15\n",
    "    if entity_metadata.get(\"source_mapping\") and entity_metadata[\"source_mapping\"] not in [\"unknown\", None]:\n",
    "        score += 15\n",
    "\n",
    "    # P2 Nice-to-have fields\n",
    "    if entity_metadata.get(\"dq_rules\") and len(entity_metadata.get(\"dq_rules\", [])) > 0:\n",
    "        score += 10\n",
    "    if entity_metadata.get(\"governance_tags\") and len(entity_metadata.get(\"governance_tags\", [])) > 0:\n",
    "        score += 10\n",
    "\n",
    "    # Recent update bonus\n",
    "    if entity_metadata.get(\"last_updated\"):\n",
    "        try:\n",
    "            last_update = datetime.fromisoformat(entity_metadata[\"last_updated\"])\n",
    "            if datetime.now() - last_update < timedelta(days=90):\n",
    "                score += 10\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return min(max(score, 0), 100)\n",
    "\n",
    "def validate_entity_status(entity_metadata: dict, articulation_score: int) -> str:\n",
    "    \"\"\"Returns: 'pass' | 'warning' | 'fail'\"\"\"\n",
    "    if entity_metadata.get(\"status\") == \"Missing from source\":\n",
    "        return \"fail\"\n",
    "    if articulation_score == 0:\n",
    "        return \"fail\"\n",
    "    if articulation_score < 50:\n",
    "        return \"warning\"\n",
    "    if not entity_metadata.get(\"description\"):\n",
    "        return \"warning\"\n",
    "    return \"pass\"\n",
    "\n",
    "def detect_missing_entities(business_entities: list[str], structure_entities: list[str]) -> list[str]:\n",
    "    \"\"\"Compare business request entities with structure.json coverage.\"\"\"\n",
    "    business_ids = {e.strip().lower() for e in business_entities}\n",
    "    structure_ids = {e.strip().lower() for e in structure_entities}\n",
    "    return sorted(list(business_ids - structure_ids))\n",
    "\n",
    "def calculate_deterministic(state: Tool3State) -> Tool3State:\n",
    "    \"\"\"\n",
    "    Node 2: Calculate articulation scores + validation flags using Python heuristics.\n",
    "\n",
    "    NO LLM calls - pure deterministic logic.\n",
    "    \"\"\"\n",
    "    print(\"üîÑ Node 2: Calculating deterministic scores...\")\n",
    "\n",
    "    structure = state[\"structure\"]\n",
    "\n",
    "    # Collect all entities from structure\n",
    "    all_entities = []\n",
    "    for fact in structure.get(\"facts\", []):\n",
    "        all_entities.append(fact)\n",
    "    for dim in structure.get(\"dimensions\", []):\n",
    "        all_entities.append(dim)\n",
    "\n",
    "    # Calculate scores for each entity\n",
    "    entity_scores = {}\n",
    "    validation_flags = {}\n",
    "\n",
    "    for entity in all_entities:\n",
    "        entity_id = entity.get(\"table_id\", \"unknown\")\n",
    "\n",
    "        # Placeholder metadata (would lookup from state[\"metadata\"] in real impl)\n",
    "        # For demo, use simplified metadata based on entity structure\n",
    "        entity_metadata = {\n",
    "            \"description\": entity.get(\"rationale\", None),  # Use rationale as proxy\n",
    "            \"owner\": None,  # Not in structure.json\n",
    "            \"lineage\": \"unknown\",\n",
    "            \"source_mapping\": \"Databricks Unity Catalog\",  # Default from audit\n",
    "            \"dq_rules\": None,\n",
    "            \"governance_tags\": None,\n",
    "            \"last_updated\": datetime.now().isoformat()\n",
    "        }\n",
    "\n",
    "        score = calculate_articulation_score(entity_metadata)\n",
    "        status = validate_entity_status(entity_metadata, score)\n",
    "\n",
    "        entity_scores[entity_id] = score\n",
    "        validation_flags[entity_id] = status\n",
    "\n",
    "    # Detect missing entities\n",
    "    business_entities = []\n",
    "    if \"project_metadata\" in state[\"business_context\"]:\n",
    "        # Tool 0 format\n",
    "        business_entities = [e.get(\"name\", \"\") for e in state[\"business_context\"].get(\"entities\", [])]\n",
    "    elif \"entities\" in state[\"business_context\"]:\n",
    "        # Tool 1 format\n",
    "        business_entities = state[\"business_context\"][\"entities\"]\n",
    "\n",
    "    structure_entity_ids = [e.get(\"table_id\", \"\") for e in all_entities]\n",
    "    missing_entities = detect_missing_entities(business_entities, structure_entity_ids)\n",
    "\n",
    "    # Calculate summary metrics\n",
    "    avg_score = sum(entity_scores.values()) / len(entity_scores) if entity_scores else 0.0\n",
    "    p0_blocker_count = sum(1 for v in validation_flags.values() if v == \"fail\")\n",
    "\n",
    "    print(f\"‚úÖ Calculated scores for {len(entity_scores)} entities\")\n",
    "    print(f\"   - Average score: {avg_score:.1f}/100\")\n",
    "    print(f\"   - P0 blockers: {p0_blocker_count}\")\n",
    "    print(f\"   - Missing entities: {len(missing_entities)}\")\n",
    "\n",
    "    return {\n",
    "        **state,\n",
    "        \"entity_scores\": entity_scores,\n",
    "        \"validation_flags\": validation_flags,\n",
    "        \"missing_entities\": missing_entities\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Node 2 (calculate_deterministic) defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3e5f953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Node 3 (enhance_with_llm) defined\n"
     ]
    }
   ],
   "source": [
    "def generate_fallback_enhancements(entity_scores: dict, validation_flags: dict, missing_entities: list) -> LLMEnhancement:\n",
    "    \"\"\"Generate generic recommendations when LLM fails.\"\"\"\n",
    "    avg_score = sum(entity_scores.values()) / len(entity_scores) if entity_scores else 0.0\n",
    "    p0_blockers = sum(1 for v in validation_flags.values() if v == \"fail\")\n",
    "\n",
    "    # Determine risk level\n",
    "    if p0_blockers > 0 or missing_entities:\n",
    "        risk_level = \"high\"\n",
    "        risk_rationale = f\"P0 blockers detected: {p0_blockers} entities with critical issues. LLM enhancement unavailable.\"\n",
    "    elif avg_score < 50:\n",
    "        risk_level = \"medium\"\n",
    "        risk_rationale = f\"Average score {avg_score:.1f}/100 indicates quality concerns. LLM enhancement unavailable.\"\n",
    "    else:\n",
    "        risk_level = \"low\"\n",
    "        risk_rationale = f\"Average score {avg_score:.1f}/100 is acceptable. LLM enhancement unavailable.\"\n",
    "\n",
    "    # Generate generic recommendations\n",
    "    recommendations = []\n",
    "\n",
    "    # P0: Missing descriptions\n",
    "    entities_no_desc = [eid for eid, score in entity_scores.items() if score < 20]\n",
    "    if entities_no_desc:\n",
    "        recommendations.append(Recommendation(\n",
    "            priority=\"P0\",\n",
    "            entity_id=None,\n",
    "            issue_type=\"MISSING_DESCRIPTION\",\n",
    "            description=f\"{len(entities_no_desc)} entities lack business descriptions\",\n",
    "            action=\"Add descriptions in Collibra for all low-scoring entities\",\n",
    "            estimated_impact=f\"+20 points per entity ({len(entities_no_desc)} entities)\"\n",
    "        ))\n",
    "\n",
    "    # P1: Low scores\n",
    "    entities_low_score = [eid for eid, score in entity_scores.items() if 20 <= score < 50]\n",
    "    if entities_low_score:\n",
    "        recommendations.append(Recommendation(\n",
    "            priority=\"P1\",\n",
    "            entity_id=None,\n",
    "            issue_type=\"LOW_ARTICULATION\",\n",
    "            description=f\"{len(entities_low_score)} entities have low articulation scores (20-50)\",\n",
    "            action=\"Improve lineage documentation and source mappings\",\n",
    "            estimated_impact=f\"Potential +30 points per entity\"\n",
    "        ))\n",
    "\n",
    "    return LLMEnhancement(\n",
    "        risk_level=risk_level,\n",
    "        risk_rationale=risk_rationale,\n",
    "        text_quality_score=None,\n",
    "        text_quality_notes=None,\n",
    "        recommendations=recommendations,\n",
    "        anomaly_notes=[],\n",
    "        summary=f\"Quality assessment incomplete due to LLM timeout. Deterministic analysis shows avg score {avg_score:.1f}/100 with {p0_blockers} P0 blockers.\"\n",
    "    )\n",
    "\n",
    "def enhance_with_llm(state: Tool3State) -> Tool3State:\n",
    "    \"\"\"\n",
    "    Node 3: LLM enhancement layer with fallback strategy.\n",
    "\n",
    "    Uses Azure OpenAI to:\n",
    "    - Assess risk level based on deterministic findings\n",
    "    - Generate P0-P2 prioritized recommendations\n",
    "    - Detect structural anomalies\n",
    "    - Write executive summary\n",
    "    \"\"\"\n",
    "    print(\"ü§ñ Node 3: LLM enhancement...\")\n",
    "\n",
    "    # Build deterministic summary\n",
    "    avg_score = sum(state[\"entity_scores\"].values()) / len(state[\"entity_scores\"]) if state[\"entity_scores\"] else 0.0\n",
    "    p0_blocker_count = sum(1 for v in state[\"validation_flags\"].values() if v == \"fail\")\n",
    "\n",
    "    deterministic_summary = {\n",
    "        \"avg_articulation_score\": avg_score,\n",
    "        \"p0_blocker_count\": p0_blocker_count,\n",
    "        \"entities_with_warnings\": sum(1 for v in state[\"validation_flags\"].values() if v == \"warning\"),\n",
    "        \"missing_entities\": state[\"missing_entities\"],\n",
    "        \"total_entities\": len(state[\"entity_scores\"])\n",
    "    }\n",
    "\n",
    "    # Sample entities for context\n",
    "    structure = state[\"structure\"]\n",
    "    sample_entities = []\n",
    "    for fact in structure.get(\"facts\", [])[:2]:\n",
    "        sample_entities.append({\n",
    "            \"table_id\": fact.get(\"table_id\"),\n",
    "            \"type\": \"fact\",\n",
    "            \"score\": state[\"entity_scores\"].get(fact.get(\"table_id\"), 0),\n",
    "            \"validation\": state[\"validation_flags\"].get(fact.get(\"table_id\"), \"unknown\")\n",
    "        })\n",
    "    for dim in structure.get(\"dimensions\", [])[:2]:\n",
    "        sample_entities.append({\n",
    "            \"table_id\": dim.get(\"table_id\"),\n",
    "            \"type\": \"dimension\",\n",
    "            \"score\": state[\"entity_scores\"].get(dim.get(\"table_id\"), 0),\n",
    "            \"validation\": state[\"validation_flags\"].get(dim.get(\"table_id\"), \"unknown\")\n",
    "        })\n",
    "\n",
    "    # Build system prompt\n",
    "    system_prompt = \"\"\"You are a metadata quality analyst for enterprise data governance.\n",
    "\n",
    "You receive DETERMINISTIC validation results (pre-calculated scores, flags, missing entities) and structural metadata. Your task is to:\n",
    "\n",
    "1. **Assess risk level**: Based on deterministic findings, assign risk:\n",
    "   - HIGH: P0 blockers present (missing descriptions/owners for >30% entities) OR missing entities from business request\n",
    "   - MEDIUM: P1 quality issues (low articulation scores 30-70, duplicates) but usable\n",
    "   - LOW: Minor P2 issues only (>70 avg articulation score)\n",
    "\n",
    "2. **Generate recommendations**: Prioritize P0‚ÜíP1‚ÜíP2 fixes. Be specific:\n",
    "   - P0 example: \"Add description in Collibra for entity dimv_supplier (currently null)\"\n",
    "   - P1 example: \"Improve lineage documentation for factv_purchase_order\"\n",
    "   - P2 example: \"Add governance tags for dimv_material_group\"\n",
    "\n",
    "3. **Detect anomalies**: Flag structural outliers (unexpected fields, naming inconsistencies, value outliers)\n",
    "\n",
    "4. **Write executive summary**: 2-3 sentences for non-technical stakeholders.\n",
    "\n",
    "**IMPORTANT CONSTRAINTS:**\n",
    "- Do NOT recalculate scores (they are pre-computed deterministically)\n",
    "- Do NOT hallucinate entity IDs (only reference entities from input)\n",
    "- Base recommendations on P0-P2 guidelines (description/owner=P0, lineage=P1, tags=P2)\n",
    "- Be actionable: specify WHERE to fix (Collibra, Unity Catalog) and WHAT to add\"\"\"\n",
    "\n",
    "    # Build user message\n",
    "    user_message = f\"\"\"Analyze metadata quality based on deterministic findings:\n",
    "\n",
    "**Summary Metrics:**\n",
    "- Average articulation score: {avg_score:.1f}/100\n",
    "- P0 blockers (fail status): {p0_blocker_count}\n",
    "- Entities with warnings: {deterministic_summary['entities_with_warnings']}\n",
    "- Missing entities: {len(state[\"missing_entities\"])}\n",
    "- Total entities analyzed: {deterministic_summary['total_entities']}\n",
    "\n",
    "**Sample Entities:**\n",
    "{json.dumps(sample_entities, indent=2)}\n",
    "\n",
    "**Missing Entities:** {', '.join(state[\"missing_entities\"]) if state[\"missing_entities\"] else 'None'}\n",
    "\n",
    "Generate risk assessment, P0-P2 recommendations, anomaly notes, and executive summary.\"\"\"\n",
    "\n",
    "    # Create agent with structured output\n",
    "    agent = create_agent(\n",
    "        model=AZURE_LLM,\n",
    "        response_format=ToolStrategy(LLMEnhancement),\n",
    "        tools=[],\n",
    "        system_prompt=system_prompt\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Invoke agent\n",
    "        result = agent.invoke({\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": user_message}\n",
    "            ]\n",
    "        })\n",
    "\n",
    "        # Extract structured response\n",
    "        structured_response = result.get('structured_response')\n",
    "        if not structured_response:\n",
    "            raise ValueError(\"No structured response from agent\")\n",
    "\n",
    "        llm_enhancements = (\n",
    "            structured_response\n",
    "            if isinstance(structured_response, LLMEnhancement)\n",
    "            else LLMEnhancement(**structured_response.model_dump())\n",
    "        )\n",
    "\n",
    "        # Validate entity IDs (hallucination check)\n",
    "        valid_ids = set(state[\"entity_scores\"].keys())\n",
    "        for rec in llm_enhancements.recommendations:\n",
    "            if rec.entity_id and rec.entity_id not in valid_ids:\n",
    "                print(f\"‚ö†Ô∏è  LLM hallucinated entity: {rec.entity_id}\")\n",
    "                rec.entity_id = None\n",
    "\n",
    "        print(f\"‚úÖ LLM enhancement complete\")\n",
    "        print(f\"   - Risk level: {llm_enhancements.risk_level}\")\n",
    "        print(f\"   - Recommendations: {len(llm_enhancements.recommendations)}\")\n",
    "\n",
    "        return {\n",
    "            **state,\n",
    "            \"llm_enhancements\": llm_enhancements,\n",
    "            \"llm_fallback_mode\": False\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå LLM call failed: {e}\")\n",
    "        print(f\"‚ö†Ô∏è  Using fallback mode (generic recommendations)\")\n",
    "\n",
    "        fallback = generate_fallback_enhancements(\n",
    "            state[\"entity_scores\"],\n",
    "            state[\"validation_flags\"],\n",
    "            state[\"missing_entities\"]\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            **state,\n",
    "            \"llm_enhancements\": fallback,\n",
    "            \"llm_fallback_mode\": True\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ Node 3 (enhance_with_llm) defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d817b286",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'merge_and_serialize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m workflow.add_node(\u001b[33m\"\u001b[39m\u001b[33mcalculate_deterministic\u001b[39m\u001b[33m\"\u001b[39m, calculate_deterministic)\n\u001b[32m      7\u001b[39m workflow.add_node(\u001b[33m\"\u001b[39m\u001b[33menhance_with_llm\u001b[39m\u001b[33m\"\u001b[39m, enhance_with_llm)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m workflow.add_node(\u001b[33m\"\u001b[39m\u001b[33mmerge_and_serialize\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mmerge_and_serialize\u001b[49m)\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Define edges (linear pipeline)\u001b[39;00m\n\u001b[32m     11\u001b[39m workflow.add_edge(START, \u001b[33m\"\u001b[39m\u001b[33mload_and_validate\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'merge_and_serialize' is not defined"
     ]
    }
   ],
   "source": [
    "# Build LangGraph StateGraph\n",
    "workflow = StateGraph(Tool3State)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"load_and_validate\", load_and_validate)\n",
    "workflow.add_node(\"calculate_deterministic\", calculate_deterministic)\n",
    "workflow.add_node(\"enhance_with_llm\", enhance_with_llm)\n",
    "workflow.add_node(\"merge_and_serialize\", merge_and_serialize)\n",
    "\n",
    "# Define edges (linear pipeline)\n",
    "workflow.add_edge(START, \"load_and_validate\")\n",
    "workflow.add_edge(\"load_and_validate\", \"calculate_deterministic\")\n",
    "workflow.add_edge(\"calculate_deterministic\", \"enhance_with_llm\")\n",
    "workflow.add_edge(\"enhance_with_llm\", \"merge_and_serialize\")\n",
    "workflow.add_edge(\"merge_and_serialize\", END)\n",
    "\n",
    "# Compile graph\n",
    "graph = workflow.compile()\n",
    "\n",
    "print(\"‚úÖ LangGraph compiled\")\n",
    "print(\"   4 nodes: load_and_validate ‚Üí calculate_deterministic ‚Üí enhance_with_llm ‚Üí merge_and_serialize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fe4962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results summary\n",
    "print(\"üìã Tool 3 - Quality Validator Results\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if \"final_report\" in final_state:\n",
    "    report = final_state[\"final_report\"]\n",
    "\n",
    "    print(\"\\nüìä Quality Metrics:\")\n",
    "    print(f\"   Total Entities: {report.metrics.total_entities}\")\n",
    "    print(f\"   Average Articulation Score: {report.metrics.avg_articulation_score:.1f}/100\")\n",
    "    print(f\"   Entities with Issues: {report.metrics.entities_with_issues}\")\n",
    "    print(f\"   P0 Blockers: {report.metrics.p0_blockers}\")\n",
    "    print(f\"   Coverage: {report.metrics.coverage*100:.1f}%\")\n",
    "\n",
    "    print(f\"\\nüö® Risk Level: {report.risk_level.upper()}\")\n",
    "    print(f\"   Rationale: {report.risk_rationale}\")\n",
    "\n",
    "    print(\"\\nüìù P0 Recommendations (Critical):\")\n",
    "    p0_recs = [r for r in report.recommendations if r.priority == \"P0\"]\n",
    "    if p0_recs:\n",
    "        for i, rec in enumerate(p0_recs[:5], 1):  # Show first 5\n",
    "            print(f\"   {i}. [{rec.issue_type}] {rec.description}\")\n",
    "            print(f\"      Action: {rec.action}\")\n",
    "            print(f\"      Impact: {rec.estimated_impact}\")\n",
    "            if rec.entity_id:\n",
    "                print(f\"      Entity: {rec.entity_id}\")\n",
    "    else:\n",
    "        print(\"   ‚úÖ No P0 blockers detected\")\n",
    "\n",
    "    print(\"\\nüìù P1 Recommendations (Important):\")\n",
    "    p1_recs = [r for r in report.recommendations if r.priority == \"P1\"]\n",
    "    if p1_recs:\n",
    "        for i, rec in enumerate(p1_recs[:3], 1):  # Show first 3\n",
    "            print(f\"   {i}. [{rec.issue_type}] {rec.description}\")\n",
    "    else:\n",
    "        print(\"   ‚úÖ No P1 issues detected\")\n",
    "\n",
    "    if report.anomaly_notes:\n",
    "        print(\"\\n‚ö†Ô∏è  Anomalies Detected:\")\n",
    "        for anomaly in report.anomaly_notes[:3]:  # Show first 3\n",
    "            print(f\"   - [{anomaly.severity.upper()}] {anomaly.anomaly_type}: {anomaly.explanation}\")\n",
    "\n",
    "    print(\"\\nüìã Executive Summary:\")\n",
    "    print(f\"   {report.summary}\")\n",
    "\n",
    "    print(\"\\nüîç Score Distribution:\")\n",
    "    scores = list(report.articulation_scores.values())\n",
    "    if scores:\n",
    "        print(f\"   Min: {min(scores)}, Max: {max(scores)}, Median: {sorted(scores)[len(scores)//2]}\")\n",
    "\n",
    "        # Simple histogram\n",
    "        ranges = [(0, 20, \"0-20\"), (20, 40, \"20-40\"), (40, 60, \"40-60\"), (60, 80, \"60-80\"), (80, 100, \"80-100\")]\n",
    "        for min_s, max_s, label in ranges:\n",
    "            count = sum(1 for s in scores if min_s <= s < max_s or (s == 100 and max_s == 100))\n",
    "            bar = \"‚ñà\" * count\n",
    "            print(f\"   {label}: {bar} ({count})\")\n",
    "\n",
    "    if report.missing_from_source:\n",
    "        print(f\"\\n‚ùå Missing Entities: {', '.join(report.missing_from_source)}\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No results - execute previous cell first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e694db",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Development Status\n",
    "\n",
    "### ‚úÖ What Works\n",
    "- [x] All 6 Pydantic schemas defined with Field descriptions (compliance ‚úì)\n",
    "- [x] Node 1 (Load & Validate): Reads structure.json + business context + metadata\n",
    "- [x] Node 2 (Calculate Deterministic): Articulation scoring (P0/P1/P2 weights), validation status (pass/warning/fail), missing entity detection\n",
    "- [x] Node 3 (Enhance with LLM): AzureChatOpenAI + ToolStrategy(LLMEnhancement) with fallback strategy\n",
    "- [x] Node 4 (Merge & Serialize): Builds QualityReport, saves JSON outputs to data/tool3/ and scrum/artifacts/\n",
    "- [x] LangGraph StateGraph compilation (4 nodes, linear edges START‚ÜíEND)\n",
    "- [x] Status/TODO/IDEA/BUG pattern in all markdown cells (Varianta 2)\n",
    "\n",
    "### ‚ö†Ô∏è Known Issues\n",
    "- **Missing input files**: Needs structure.json (from Tool 2), business_context.md, metadata.json\n",
    "- **Azure credentials**: Requires valid AZURE_OPENAI_ENDPOINT, AZURE_OPENAI_API_KEY, AZURE_OPENAI_DEPLOYMENT_NAME in .env\n",
    "- **LLM fallback**: If Node 3 times out (>30s), uses generic recommendations - test robustness\n",
    "- **Hallucination risk**: Entity ID validation implemented but needs testing with real data\n",
    "\n",
    "### üîÑ Next Session\n",
    "1. **Execute notebook**: Run all cells with real data (copy structure.json from Tool 2)\n",
    "2. **Validate outputs**: Check quality_report.json against schema, review scrum/artifacts/\n",
    "3. **Test fallback mode**: Simulate LLM timeout to verify generic recommendations work\n",
    "4. **Alignment check**: Compare articulation scores with DQ audit baseline (28.1/100 target)\n",
    "5. **Update story**: Set `skill_created: true` in scrum/backlog/tool3-quality-validator.md\n",
    "\n",
    "### üí° Ideas for v2\n",
    "- **Score histogram visualization**: Matplotlib chart for score distribution (0-20, 20-40, etc.)\n",
    "- **Governance export**: CSV format for audit trail (entity_id, score, status, recommendations)\n",
    "- **Batch processing**: Support multiple business requests in single run\n",
    "- **Delta reporting**: Compare quality_report.json between pipeline runs (regression detection)\n",
    "- **Confidence scores**: Add LLM confidence to each recommendation (0-1 scale)\n",
    "\n",
    "### üìù Documentation Pattern\n",
    "Follows Tool 2 structure exactly:\n",
    "- Header with architecture overview\n",
    "- Install, imports, Azure config\n",
    "- Schemas definition (6 Pydantic models)\n",
    "- 4 nodes (Load ‚Üí Calculate ‚Üí Enhance ‚Üí Serialize)\n",
    "- LangGraph compilation\n",
    "- Execute pipeline + Results summary\n",
    "- Development status (this cell)\n",
    "\n",
    "### üéØ Acceptance Criteria Checklist\n",
    "\n",
    "**From `scrum/backlog/tool3-quality-validator.md`:**\n",
    "\n",
    "1. **Input Processing:**\n",
    "   - [ ] Load `structure.json` (Tool 2 output)\n",
    "   - [ ] Load business context\n",
    "   - [ ] Load metadata\n",
    "\n",
    "2. **Deterministic Quality Checks:**\n",
    "   - [ ] Articulation scoring: P0 (40pts), P1 (30pts), P2 (30pts)\n",
    "   - [ ] Validation status: pass (all P0+P1 present), warning (P1 missing), fail (P0 missing)\n",
    "   - [ ] Missing entity detection (source vs. structure)\n",
    "\n",
    "3. **LLM Enhancement:**\n",
    "   - [ ] Generate recommendations via ToolStrategy(LLMEnhancement)\n",
    "   - [ ] Include entity_id, priority (P0/P1/P2), issue_type, description, action, estimated_impact\n",
    "   - [ ] Hallucination mitigation: Validate entity IDs against structure\n",
    "   - [ ] Fallback strategy: Timeout >30s ‚Üí generic recommendations\n",
    "\n",
    "4. **Report Generation:**\n",
    "   - [ ] QualityReport schema: metrics (total, avg_score, issues, p0_blockers, coverage), risk_level, articulation_scores (dict), recommendations (list), anomaly_notes (list), missing_from_source (list), summary\n",
    "   - [ ] Save to `data/tool3/quality_report.json`\n",
    "   - [ ] Save audit artifacts to `scrum/artifacts/YYYY-MM-DD_tool3-quality-summary.json`\n",
    "\n",
    "5. **Alignment:**\n",
    "   - [ ] Baseline: 28.1/100 from DQ audit (`scrum/artifacts/2025-10-31_datamarts-dq-audit.md`)\n",
    "   - [ ] Risk levels: <40 = CRITICAL, 40-60 = HIGH, 60-80 = MEDIUM, >80 = LOW\n",
    "\n",
    "### üìÇ Output Files\n",
    "```\n",
    "data/tool3/\n",
    "‚îú‚îÄ‚îÄ quality_report.json        # Full QualityReport schema\n",
    "scrum/artifacts/\n",
    "‚îú‚îÄ‚îÄ YYYY-MM-DD_tool3-quality-summary.json  # Audit trail\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Next Step:** Execute all cells with real data from Tool 2 output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022263fe",
   "metadata": {},
   "source": [
    "## 8. Results Summary\n",
    "\n",
    "**Status:** ‚è≥ Pending execution\n",
    "\n",
    "**TODO:**\n",
    "- [ ] Display P0 recommendations\n",
    "- [ ] Show score distribution\n",
    "- [ ] Validate against acceptance criteria\n",
    "\n",
    "**IDEA:**\n",
    "- Create score distribution histogram\n",
    "- Export to CSV for governance reporting\n",
    "\n",
    "**BUG:**\n",
    "- None known yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd8d978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the graph\n",
    "print(\"üöÄ Starting Tool 3 pipeline...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "# Initial state (empty - nodes will populate)\n",
    "initial_state = {}\n",
    "\n",
    "# Run the graph\n",
    "final_state = graph.invoke(initial_state)\n",
    "\n",
    "end_time = datetime.now()\n",
    "duration = (end_time - start_time).total_seconds()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"‚úÖ Pipeline completed in {duration:.2f}s\")\n",
    "print(f\"üìä Final metrics:\")\n",
    "print(f\"   - Total entities: {final_state['final_report'].metrics.total_entities}\")\n",
    "print(f\"   - Avg score: {final_state['final_report'].metrics.avg_articulation_score:.1f}/100\")\n",
    "print(f\"   - Entities with issues: {final_state['final_report'].metrics.entities_with_issues}\")\n",
    "print(f\"   - P0 blockers: {final_state['final_report'].metrics.p0_blockers}\")\n",
    "print(f\"   - Coverage: {final_state['final_report'].metrics.coverage*100:.1f}%\")\n",
    "print(f\"   - Risk level: {final_state['final_report'].risk_level.upper()}\")\n",
    "\n",
    "if final_state.get('llm_fallback_mode'):\n",
    "    print(f\"‚ö†Ô∏è  FALLBACK MODE: LLM enhancement unavailable, generic recommendations used\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedaeaf4",
   "metadata": {},
   "source": [
    "## 7. Execute Pipeline\n",
    "\n",
    "**Status:** ‚è≥ Ready to test | Run all cells above first\n",
    "\n",
    "**TODO:**\n",
    "- [ ] Execute and validate outputs\n",
    "- [ ] Check quality_report.json schema\n",
    "- [ ] Review audit artifacts\n",
    "\n",
    "**IDEA:**\n",
    "- Add timer for each node execution\n",
    "- Compare results with expected output from story\n",
    "\n",
    "**BUG:**\n",
    "- None known yet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a280c0b1",
   "metadata": {},
   "source": [
    "## 6. Build LangGraph\n",
    "\n",
    "**Status:** ‚úÖ Working | 4-node pipeline with START‚ÜíEND flow\n",
    "\n",
    "**TODO:**\n",
    "- [ ] Add conditional edges (e.g., skip LLM if deterministic results are clean)\n",
    "- [ ] Add error handling nodes\n",
    "\n",
    "**IDEA:**\n",
    "- Add progress callbacks for each node\n",
    "- Implement partial state checkpointing (resume from Node 3 if LLM fails)\n",
    "\n",
    "**BUG:**\n",
    "- None known yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b14acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_and_serialize(state: Tool3State) -> Tool3State:\n",
    "    \"\"\"\n",
    "    Node 4: Merge deterministic + LLM results and save outputs.\n",
    "\n",
    "    Outputs:\n",
    "    - data/tool3/quality_report.json (main output)\n",
    "    - scrum/artifacts/YYYY-MM-DD_tool3-quality-summary.json (audit log)\n",
    "    \"\"\"\n",
    "    print(\"üîÑ Node 4: Merging and serializing...\")\n",
    "\n",
    "    # Calculate final metrics\n",
    "    total_entities = len(state[\"entity_scores\"])\n",
    "    avg_score = sum(state[\"entity_scores\"].values()) / total_entities if total_entities else 0.0\n",
    "    entities_with_issues = sum(1 for v in state[\"validation_flags\"].values() if v in [\"warning\", \"fail\"])\n",
    "    p0_blockers = len([r for r in state[\"llm_enhancements\"].recommendations if r.priority == \"P0\"])\n",
    "\n",
    "    # Coverage: (total - missing) / total\n",
    "    coverage = 1.0 - (len(state[\"missing_entities\"]) / total_entities if total_entities else 0.0)\n",
    "\n",
    "    metrics = QualityMetrics(\n",
    "        total_entities=total_entities,\n",
    "        avg_articulation_score=avg_score,\n",
    "        entities_with_issues=entities_with_issues,\n",
    "        p0_blockers=p0_blockers,\n",
    "        coverage=coverage\n",
    "    )\n",
    "\n",
    "    # Build final report\n",
    "    final_report = QualityReport(\n",
    "        timestamp=datetime.now().isoformat(),\n",
    "        source_files={\n",
    "            \"structure\": \"../data/tool2/structure.json\",\n",
    "            \"business_context\": \"Most recent from ../data/tool0_samples/\"\n",
    "        },\n",
    "        articulation_scores=state[\"entity_scores\"],\n",
    "        validation_results=state[\"validation_flags\"],\n",
    "        missing_from_source=state[\"missing_entities\"],\n",
    "        risk_level=state[\"llm_enhancements\"].risk_level,\n",
    "        risk_rationale=state[\"llm_enhancements\"].risk_rationale,\n",
    "        recommendations=state[\"llm_enhancements\"].recommendations,\n",
    "        anomaly_notes=state[\"llm_enhancements\"].anomaly_notes,\n",
    "        summary=state[\"llm_enhancements\"].summary,\n",
    "        metrics=metrics\n",
    "    )\n",
    "\n",
    "    # Save main quality_report.json\n",
    "    output_dir = Path(\"../data/tool3\")\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    report_path = output_dir / \"quality_report.json\"\n",
    "    with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(final_report.model_dump(), f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    # Save audit summary\n",
    "    artifacts_dir = Path(\"../scrum/artifacts\")\n",
    "    artifacts_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    date_prefix = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    summary_path = artifacts_dir / f\"{date_prefix}_tool3-quality-summary.json\"\n",
    "\n",
    "    summary = {\n",
    "        \"timestamp\": final_report.timestamp,\n",
    "        \"metrics\": metrics.model_dump(),\n",
    "        \"risk_level\": final_report.risk_level,\n",
    "        \"p0_recommendations\": [r.model_dump() for r in final_report.recommendations if r.priority == \"P0\"],\n",
    "        \"llm_fallback_mode\": state.get(\"llm_fallback_mode\", False),\n",
    "        \"source_files\": final_report.source_files\n",
    "    }\n",
    "\n",
    "    with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(summary, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"‚úÖ Saved quality_report.json: {report_path}\")\n",
    "    print(f\"‚úÖ Saved audit summary: {summary_path}\")\n",
    "\n",
    "    if state.get(\"llm_fallback_mode\"):\n",
    "        print(f\"‚ö†Ô∏è  Report generated in FALLBACK mode (LLM unavailable)\")\n",
    "\n",
    "    return {\n",
    "        **state,\n",
    "        \"final_report\": final_report,\n",
    "        \"output_path\": str(report_path)\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Node 4 (merge_and_serialize) defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4017a641",
   "metadata": {},
   "source": [
    "## 5. Node 4: Merge & Serialize\n",
    "\n",
    "**Status:** ‚úÖ Working | Consolidates deterministic + LLM results into QualityReport\n",
    "\n",
    "**TODO:**\n",
    "- [ ] Add validation against QualityReport schema before saving\n",
    "- [ ] Implement diff comparison if previous quality_report.json exists\n",
    "\n",
    "**IDEA:**\n",
    "- Generate human-readable markdown summary\n",
    "- Add score distribution histogram\n",
    "\n",
    "**BUG:**\n",
    "- None known yet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ac2be8",
   "metadata": {},
   "source": [
    "## 4. Node 3: Enhance with LLM\n",
    "\n",
    "**Status:** ‚úÖ Working | LLM agent generates risk assessment + recommendations with fallback\n",
    "\n",
    "**TODO:**\n",
    "- [ ] Implement retry logic with exponential backoff (3 attempts)\n",
    "- [ ] Add hallucination detection for entity IDs\n",
    "\n",
    "**IDEA:**\n",
    "- Cache LLM responses for identical deterministic inputs (cost optimization)\n",
    "- Add temperature parameter if supported by future models\n",
    "\n",
    "**BUG:**\n",
    "- Current Azure model (gpt-5-mini) doesn't support temperature parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d52070",
   "metadata": {},
   "source": [
    "## 3. Node 2: Calculate Deterministic\n",
    "\n",
    "**Status:** ‚úÖ Working | Python heuristics for articulation score + validation flags\n",
    "\n",
    "**TODO:**\n",
    "- [ ] Implement actual metadata lookup (currently placeholder scores)\n",
    "- [ ] Add more heuristics (e.g., naming convention checks)\n",
    "\n",
    "**IDEA:**\n",
    "- Pre-compute scores in parallel (multiprocessing for large datasets)\n",
    "- Add configurable weights for scoring components\n",
    "\n",
    "**BUG:**\n",
    "- None known yet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6e521f",
   "metadata": {},
   "source": [
    "## 2. Node 1: Load & Validate\n",
    "\n",
    "**Status:** ‚úÖ Working | Loads structure.json + business_context + metadata\n",
    "\n",
    "**TODO:**\n",
    "- [ ] Add schema validation for StructuralAnalysis format\n",
    "- [ ] Implement input sanitization (remove nulls, normalize IDs)\n",
    "\n",
    "**IDEA:**\n",
    "- Cache metadata in memory for repeated runs\n",
    "- Add file existence checks with clear error messages\n",
    "\n",
    "**BUG:**\n",
    "- None known yet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6c3ba3",
   "metadata": {},
   "source": [
    "## 1. Define Schemas & State\n",
    "\n",
    "**Status:** ‚úÖ Working | Pydantic v2 models with Field descriptions\n",
    "\n",
    "**TODO:**\n",
    "- [ ] Add field_validator for timestamp ISO8601 validation\n",
    "- [ ] Consider adding confidence thresholds for filtering recommendations\n",
    "\n",
    "**IDEA:**\n",
    "- Schema versioning field (schema_version: \"1.0.0\") for backward compatibility\n",
    "- Add validation rules registry (extensible P0-P2 criteria)\n",
    "\n",
    "**BUG:**\n",
    "- None known yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3b6be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "# !pip install langgraph langchain langchain-openai pydantic python-dotenv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
