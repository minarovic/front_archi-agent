{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b434f28",
   "metadata": {},
   "source": [
    "# Tool 1 - Data Ingest Demo (LangGraph Nodes)\n",
    "\n",
    "**Purpose:** Map business entities from Tool 0 to BS metadata candidates using LangGraph agent with nodes.\n",
    "\n",
    "**LangGraph Features Tested:**\n",
    "- ‚úÖ Agent with discrete nodes (LangGraph way of thinking)\n",
    "- ‚úÖ **Multiple LLM nodes** (prepare = ranking, mapping = matching)\n",
    "- ‚úÖ Shared state between nodes\n",
    "- ‚úÖ Structured output (ToolStrategy) in multiple contexts\n",
    "- ‚úÖ Dynamic prompt middleware (inject scope_out)\n",
    "- ‚úÖ Streaming progress between nodes\n",
    "\n",
    "**Architecture:**\n",
    "```\n",
    "Load Node ‚Üí Prepare Node (LLM) ‚Üí Mapping Node (LLM) ‚Üí Filter Node ‚Üí Save Node\n",
    "     ‚Üì            ‚Üì                     ‚Üì                  ‚Üì           ‚Üì\n",
    " Tool 0 JSON  LLM Ranking         LLM Matching        Blacklist    Results\n",
    " + BS export  (relevance)       (confidence+rationale) (deterministic) (JSON)\n",
    "```\n",
    "\n",
    "**Model:** Azure OpenAI gpt-5-mini via AzureChatOpenAI (LangChain wrapper)\n",
    "\n",
    "**Showcase:** Two intelligent LLM nodes with different purposes - demonstrates \"each node does one thing well\"\n",
    "\n",
    "**Configuration:** Uses `.env` file with AZURE_OPENAI_ENDPOINT, AZURE_OPENAI_API_KEY, AZURE_OPENAI_DEPLOYMENT_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52117387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "# !pip install langgraph langchain langchain-openai pydantic python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2e0f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports successful\n"
     ]
    }
   ],
   "source": [
    "# Import required modules\n",
    "from pydantic import BaseModel, Field\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import TypedDict\n",
    "import json\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain.agents import create_agent\n",
    "from langchain.agents.structured_output import ToolStrategy\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "print(\"‚úÖ Imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c9bdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Azure OpenAI for LangChain agents\n",
    "load_dotenv()\n",
    "\n",
    "AZURE_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "DEPLOYMENT_NAME = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "\n",
    "if not all([AZURE_ENDPOINT, AZURE_API_KEY, DEPLOYMENT_NAME]):\n",
    "    raise ValueError(\"Missing Azure configuration in .env file\")\n",
    "\n",
    "# Create AzureChatOpenAI model for LangChain agents\n",
    "# Note: Uses deployment name, not base model name\n",
    "AZURE_LLM = AzureChatOpenAI(\n",
    "    azure_endpoint=AZURE_ENDPOINT,\n",
    "    api_key=AZURE_API_KEY,\n",
    "    azure_deployment=DEPLOYMENT_NAME,\n",
    "    api_version=\"2024-10-21\"\n",
    ")\n",
    "\n",
    "print(f\"‚òÅÔ∏è Azure OpenAI configured for LangChain\")\n",
    "print(f\"   Endpoint: {AZURE_ENDPOINT}\")\n",
    "print(f\"   Deployment: {DEPLOYMENT_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e49122",
   "metadata": {},
   "source": [
    "## 1. Define Schemas & State\n",
    "\n",
    "Pydantic schemas for structured output + LangGraph state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb0237f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Schemas & state defined\n"
     ]
    }
   ],
   "source": [
    "# Pydantic schemas for structured output\n",
    "class EntityMapping(BaseModel):\n",
    "    \"\"\"Single entity mapped to a BS candidate.\"\"\"\n",
    "\n",
    "    entity: str = Field(description=\"Original entity name from Tool 0\")\n",
    "    candidate_id: str = Field(description=\"BS candidate identifier (e.g., 'dm_bs_suppliers')\")\n",
    "    candidate_name: str = Field(description=\"Human-readable candidate name\")\n",
    "    confidence: float = Field(\n",
    "        description=\"Confidence score 0.0-1.0\",\n",
    "        ge=0.0,\n",
    "        le=1.0\n",
    "    )\n",
    "    rationale: str = Field(description=\"Why this mapping was suggested\")\n",
    "\n",
    "\n",
    "class MappingSuggestions(BaseModel):\n",
    "    \"\"\"Complete set of entity mappings.\"\"\"\n",
    "\n",
    "    mappings: list[EntityMapping] = Field(\n",
    "        description=\"List of entity-to-candidate mappings\"\n",
    "    )\n",
    "\n",
    "\n",
    "# LangGraph state (shared across all nodes)\n",
    "class Tool1State(TypedDict, total=False):\n",
    "    \"\"\"Shared state for Tool 1 graph.\"\"\"\n",
    "\n",
    "    # Input data\n",
    "    entities: list[str]\n",
    "    scope_out: str\n",
    "    candidates: list[dict]\n",
    "\n",
    "    # Processing results\n",
    "    raw_mappings: list[dict]\n",
    "    filtered_mappings: list[dict]\n",
    "\n",
    "    # Output paths\n",
    "    output_json_path: str\n",
    "    output_artifact_path: str\n",
    "\n",
    "print(\"‚úÖ Schemas & state defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1f4c8e",
   "metadata": {},
   "source": [
    "## 2. Node 1: Load Data\n",
    "\n",
    "**Status:** ‚úÖ Working | **Performance:** <0.5s\n",
    "\n",
    "Load Tool 0 JSON (entities, scope_out) + BS export JSON (candidates).\n",
    "\n",
    "**TODO:**\n",
    "- [ ] Parametrize Tool 0 path (currently hardcoded to `2025-10-31T01:14:27.960789.json`)\n",
    "- [ ] Add JSON validation (check required keys: entities, scope_out)\n",
    "- [ ] Handle missing files gracefully (FileNotFoundError)\n",
    "- [ ] Support multiple BS metadata sources (currently single file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c48758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Load node defined\n"
     ]
    }
   ],
   "source": [
    "def load_node(state: Tool1State) -> dict:\n",
    "    \"\"\"Load Tool 0 output and BS export.\"\"\"\n",
    "    print(\"üìÇ Node 1: Loading data...\")\n",
    "\n",
    "    # Load Tool 0 JSON\n",
    "    tool0_path = Path.cwd().parent / 'data' / 'tool0_samples' / '2025-10-31T01:14:27.960789.json'\n",
    "    with open(tool0_path, 'r', encoding='utf-8') as f:\n",
    "        tool0_data = json.load(f)\n",
    "\n",
    "    entities = tool0_data.get('entities', [])\n",
    "    scope_out = tool0_data.get('scope_out', '')\n",
    "\n",
    "    print(f\"   ‚úÖ Loaded {len(entities)} entities from Tool 0\")\n",
    "    print(f\"   ‚úÖ Scope out: {scope_out[:100]}...\")\n",
    "\n",
    "    # Load BS export JSON (pre-filtered externally: flattened, deduped, BS-only)\n",
    "    bs_path = Path.cwd().parent / 'docs_langgraph' / 'BA-BS_Datamarts_metadata.json'\n",
    "    with open(bs_path, 'r', encoding='utf-8') as f:\n",
    "        bs_data = json.load(f)\n",
    "\n",
    "    # Flatten array-of-arrays structure\n",
    "    if isinstance(bs_data, list) and len(bs_data) > 0 and isinstance(bs_data[0], list):\n",
    "        candidates_flat = bs_data[0]  # Extract inner array\n",
    "    else:\n",
    "        candidates_flat = bs_data\n",
    "\n",
    "    # Filter to BS-only (dm_bs_* prefix) - FIX: use 'displayName' not 'name'\n",
    "    candidates = [\n",
    "        c for c in candidates_flat\n",
    "        if isinstance(c, dict) and c.get('displayName', '').startswith('dm_bs_')\n",
    "    ]\n",
    "\n",
    "    print(f\"   ‚úÖ Loaded {len(candidates)} BS candidates\")\n",
    "    if candidates:\n",
    "        print(f\"   üìã Candidates:\")\n",
    "        for c in candidates:\n",
    "            print(f\"      - {c.get('displayName')}: {c.get('description', 'N/A')[:60]}...\")\n",
    "\n",
    "    return {\n",
    "        'entities': entities,\n",
    "        'scope_out': scope_out,\n",
    "        'candidates': candidates\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Load node defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f69f7f",
   "metadata": {},
   "source": [
    "## 3. Node 2: Prepare Candidates (LLM Ranking)\n",
    "\n",
    "**Status:** ‚úÖ Working | **LLM Cost:** ~$0.002 per run\n",
    "\n",
    "**LLM-based prefiltering:** Rank candidates by relevance for entities (showcase multiple LLM nodes).\n",
    "\n",
    "**TODO:**\n",
    "- [ ] Validate `candidates.length >= 1` (crashes on empty list)\n",
    "- [ ] Add caching for repeated entity sets (avoid re-ranking)\n",
    "- [ ] Test with 5+ candidates (current test: 1-2 only)\n",
    "- [ ] Measure ranking accuracy vs manual expert ranking\n",
    "\n",
    "**IDEA:**\n",
    "- Consider embedding similarity instead of LLM (faster + cheaper)\n",
    "- Add confidence threshold parameter (currently hardcoded 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52685258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Prepare node (LLM ranking) defined\n"
     ]
    }
   ],
   "source": [
    "# Pydantic schema for candidate ranking\n",
    "class CandidateRank(BaseModel):\n",
    "    \"\"\"Single candidate with relevance score.\"\"\"\n",
    "\n",
    "    candidate_id: str = Field(description=\"Candidate identifier\")\n",
    "    relevance_score: float = Field(\n",
    "        description=\"Relevance score 0.0-1.0\",\n",
    "        ge=0.0,\n",
    "        le=1.0\n",
    "    )\n",
    "    reason: str = Field(description=\"Why this candidate is relevant\")\n",
    "\n",
    "\n",
    "class CandidateRanking(BaseModel):\n",
    "    \"\"\"Ranked list of candidates.\"\"\"\n",
    "\n",
    "    ranked_candidates: list[CandidateRank] = Field(\n",
    "        description=\"Candidates ranked by relevance\"\n",
    "    )\n",
    "\n",
    "\n",
    "def prepare_node(state: Tool1State) -> dict:\n",
    "    \"\"\"LLM ranks candidates by relevance for entities.\"\"\"\n",
    "    print(\"üîß Node 2: LLM ranking candidates...\")\n",
    "\n",
    "    entities = state['entities']\n",
    "    candidates = state['candidates']\n",
    "\n",
    "    print(f\"   Input: {len(entities)} entities, {len(candidates)} candidates\")\n",
    "\n",
    "    # Prepare candidate summaries for LLM\n",
    "    candidate_summaries = []\n",
    "    for c in candidates:\n",
    "        desc = c.get('description', '')\n",
    "        candidate_summaries.append({\n",
    "            'id': c.get('displayName', 'unknown'),\n",
    "            'name': c.get('fullName', c.get('displayName', 'unknown')),\n",
    "            'description': desc[:200] + '...' if len(desc) > 200 else desc\n",
    "        })\n",
    "\n",
    "    # Create lightweight ranking agent\n",
    "    ranking_agent = create_agent(\n",
    "        model=AZURE_LLM,\n",
    "        response_format=ToolStrategy(CandidateRanking),\n",
    "        tools=[],\n",
    "        system_prompt=\"\"\"You are a data relevance analyzer.\n",
    "Rank database candidates by relevance for the given business entities.\n",
    "\n",
    "Consider:\n",
    "- Semantic similarity between entity names and candidate names/descriptions\n",
    "- Domain relevance (e.g., \"suppliers\" ‚Üí purchasing schemas)\n",
    "- Czech/English terminology overlap\n",
    "\n",
    "Return relevance scores (0.0-1.0) and reasons.\"\"\"\n",
    "    )\n",
    "\n",
    "    # Prepare ranking request\n",
    "    ranking_request = f\"\"\"Rank these candidates by relevance for the business entities:\n",
    "\n",
    "**Business Entities:**\n",
    "{json.dumps(entities, indent=2, ensure_ascii=False)}\n",
    "\n",
    "**Available Candidates:**\n",
    "{json.dumps(candidate_summaries, indent=2, ensure_ascii=False)}\n",
    "\n",
    "Return ranked candidates with relevance scores.\"\"\"\n",
    "\n",
    "    # Invoke ranking agent\n",
    "    result = ranking_agent.invoke({\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": ranking_request}\n",
    "        ]\n",
    "    })\n",
    "\n",
    "    # Extract structured response\n",
    "    structured_response = result.get('structured_response')\n",
    "    if not structured_response:\n",
    "        raise ValueError(\"No structured response from ranking agent\")\n",
    "\n",
    "    # Convert to dict\n",
    "    ranking_data = (\n",
    "        structured_response.model_dump()\n",
    "        if hasattr(structured_response, 'model_dump')\n",
    "        else structured_response.dict()\n",
    "    )\n",
    "\n",
    "    ranked_list = ranking_data.get('ranked_candidates', [])\n",
    "\n",
    "    # Take top N (or all if fewer) - for demo, take all with score > 0.3\n",
    "    relevant_candidates = [\n",
    "        r for r in ranked_list\n",
    "        if r['relevance_score'] > 0.3\n",
    "    ]\n",
    "\n",
    "    # Match back to full candidate objects\n",
    "    ranked_candidate_ids = [r['candidate_id'] for r in relevant_candidates]\n",
    "    prepared_candidates = [\n",
    "        {\n",
    "            'id': c.get('displayName', 'unknown'),\n",
    "            'name': c.get('fullName', c.get('displayName', 'unknown')),\n",
    "            'description': c.get('description', '')[:200] + '...'\n",
    "                if len(c.get('description', '')) > 200\n",
    "                else c.get('description', ''),\n",
    "            'relevance_score': next(\n",
    "                (r['relevance_score'] for r in relevant_candidates if r['candidate_id'] == c.get('displayName')),\n",
    "                0.0\n",
    "            )\n",
    "        }\n",
    "        for c in candidates\n",
    "        if c.get('displayName') in ranked_candidate_ids\n",
    "    ]\n",
    "\n",
    "    print(f\"   ‚úÖ Ranked {len(ranked_list)} candidates\")\n",
    "    print(f\"   ‚úÖ Selected {len(prepared_candidates)} relevant (score > 0.3)\")\n",
    "\n",
    "    return {\n",
    "        'candidates': prepared_candidates\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Prepare node (LLM ranking) defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4b7e56",
   "metadata": {},
   "source": [
    "## 4. Node 3: LLM Mapping (Agent with scope_out)\n",
    "\n",
    "**Status:** ‚úÖ Working | **LLM Cost:** ~$0.005 per run\n",
    "\n",
    "Use LangGraph agent with:\n",
    "- Structured output (ToolStrategy)\n",
    "- Scope_out context injection via system_prompt\n",
    "\n",
    "**TODO:**\n",
    "- [ ] Test with zero matching candidates (edge case handling)\n",
    "- [ ] Add confidence calibration (rescale scores based on history)\n",
    "- [ ] Measure Czech entity recognition accuracy (\"dodavatel√©\" ‚Üí \"suppliers\")\n",
    "- [ ] Test with ambiguous entity names (e.g., \"Orders\" could be Sales or Purchase)\n",
    "\n",
    "**BUG:**\n",
    "- Empty scope_out causes extra newlines in prompt ‚Üí add validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07530031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Mapping node defined\n"
     ]
    }
   ],
   "source": [
    "def mapping_node(state: Tool1State) -> dict:\n",
    "    \"\"\"Map entities to candidates using LLM agent.\"\"\"\n",
    "    print(\"ü§ñ Node 3: LLM mapping with agent...\")\n",
    "\n",
    "    entities = state['entities']\n",
    "    candidates = state['candidates']\n",
    "    scope_out = state['scope_out']\n",
    "\n",
    "    # Build system prompt with scope_out context\n",
    "    system_prompt = f\"\"\"You are an entity mapper. Map business entities to database candidates using fuzzy matching.\n",
    "\n",
    "Consider:\n",
    "- Synonyms (e.g., \"dodavatel√©\" = \"suppliers\")\n",
    "- Czech/English variants\n",
    "- Partial name matches\n",
    "- Description context\n",
    "\n",
    "IMPORTANT: Avoid candidates related to these excluded topics:\n",
    "{scope_out}\n",
    "\n",
    "Return confidence scores (0.0-1.0) and rationale for each mapping.\"\"\"\n",
    "\n",
    "    # Create agent with structured output (using Azure LLM)\n",
    "    agent = create_agent(\n",
    "        model=AZURE_LLM,\n",
    "        response_format=ToolStrategy(MappingSuggestions),\n",
    "        tools=[],\n",
    "        system_prompt=system_prompt\n",
    "    )\n",
    "\n",
    "    # Prepare user message\n",
    "    user_message = f\"\"\"Map these entities to the best matching candidates:\n",
    "\n",
    "**Entities to map:**\n",
    "{json.dumps(entities, indent=2, ensure_ascii=False)}\n",
    "\n",
    "**Available candidates:**\n",
    "{json.dumps(candidates, indent=2, ensure_ascii=False)}\n",
    "\n",
    "Return mappings with confidence scores and rationale.\"\"\"\n",
    "\n",
    "    # Invoke agent\n",
    "    result = agent.invoke({\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ]\n",
    "    })\n",
    "\n",
    "    # Extract structured response\n",
    "    structured_response = result.get('structured_response')\n",
    "    if not structured_response:\n",
    "        raise ValueError(\"No structured response from agent\")\n",
    "\n",
    "    # Convert to dict\n",
    "    mappings_data = (\n",
    "        structured_response.model_dump()\n",
    "        if hasattr(structured_response, 'model_dump')\n",
    "        else structured_response.dict()\n",
    "    )\n",
    "\n",
    "    raw_mappings = mappings_data.get('mappings', [])\n",
    "\n",
    "    print(f\"   ‚úÖ Generated {len(raw_mappings)} mappings\")\n",
    "\n",
    "    return {\n",
    "        'raw_mappings': raw_mappings\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Mapping node defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2db7c88",
   "metadata": {},
   "source": [
    "## 5. Node 4: Filter Blacklist\n",
    "\n",
    "**Status:** ‚úÖ Working | **Performance:** <0.1s (deterministic)\n",
    "\n",
    "Apply deterministic scope_out blacklist filter (historical, logs, security terms).\n",
    "\n",
    "**TODO:**\n",
    "- [ ] Expand blacklist with domain-specific terms (needs expert input)\n",
    "- [ ] Add configurable blacklist file (currently hardcoded)\n",
    "- [ ] Log filtered-out mappings for audit trail\n",
    "- [ ] Test case sensitivity (currently lowercased)\n",
    "\n",
    "**IDEA:**\n",
    "- Use fuzzy matching for blacklist terms (e.g., \"history\" matches \"historical\")\n",
    "- Add whitelist override for false positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6b2fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Filter node defined\n"
     ]
    }
   ],
   "source": [
    "def filter_node(state: Tool1State) -> dict:\n",
    "    \"\"\"Filter mappings using scope_out blacklist.\"\"\"\n",
    "    print(\"üîç Node 4: Applying blacklist filter...\")\n",
    "\n",
    "    raw_mappings = state['raw_mappings']\n",
    "    scope_out = state['scope_out'].lower()\n",
    "\n",
    "    # Define blacklist keywords\n",
    "    blacklist = ['historical', 'logs', 'security', 'archive', 'audit']\n",
    "\n",
    "    # Also extract keywords from scope_out\n",
    "    scope_keywords = [\n",
    "        word.strip().lower()\n",
    "        for word in scope_out.split()\n",
    "        if len(word.strip()) > 3\n",
    "    ]\n",
    "\n",
    "    all_blacklist = set(blacklist + scope_keywords)\n",
    "\n",
    "    # Filter mappings\n",
    "    filtered = []\n",
    "    for mapping in raw_mappings:\n",
    "        candidate_name = mapping.get('candidate_name', '').lower()\n",
    "        candidate_id = mapping.get('candidate_id', '').lower()\n",
    "\n",
    "        # Check if any blacklist term appears\n",
    "        is_blacklisted = any(\n",
    "            term in candidate_name or term in candidate_id\n",
    "            for term in all_blacklist\n",
    "        )\n",
    "\n",
    "        if not is_blacklisted:\n",
    "            filtered.append(mapping)\n",
    "\n",
    "    removed_count = len(raw_mappings) - len(filtered)\n",
    "    print(f\"   ‚úÖ Filtered: {len(filtered)} kept, {removed_count} removed\")\n",
    "\n",
    "    return {\n",
    "        'filtered_mappings': filtered\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Filter node defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea3af54",
   "metadata": {},
   "source": [
    "## 6. Node 5: Save Results\n",
    "\n",
    "**Status:** ‚úÖ Working | **Performance:** <0.2s\n",
    "\n",
    "Save filtered_dataset.json + ingest summary to artifacts.\n",
    "\n",
    "**TODO:**\n",
    "- [ ] Add timestamp validation (ISO 8601 format)\n",
    "- [ ] Create backup before overwrite (versioning)\n",
    "- [ ] Add schema validation for output JSON\n",
    "- [ ] Test with empty mappings (edge case)\n",
    "\n",
    "**IDEA:**\n",
    "- Save intermediate state (raw_mappings) for debugging\n",
    "- Add CSV export option for non-technical users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d7284f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Save node defined\n"
     ]
    }
   ],
   "source": [
    "def save_node(state: Tool1State) -> dict:\n",
    "    \"\"\"Save filtered dataset and artifacts.\"\"\"\n",
    "    print(\"üíæ Node 5: Saving results...\")\n",
    "\n",
    "    entities = state['entities']\n",
    "    filtered_mappings = state['filtered_mappings']\n",
    "    scope_out = state['scope_out']\n",
    "\n",
    "    timestamp = datetime.now().isoformat()\n",
    "\n",
    "    # Prepare filtered dataset\n",
    "    filtered_dataset = {\n",
    "        'timestamp': timestamp,\n",
    "        'business_context': {\n",
    "            'entities': entities,\n",
    "            'scope_out': scope_out\n",
    "        },\n",
    "        'mappings': filtered_mappings,\n",
    "        'stats': {\n",
    "            'total_entities': len(entities),\n",
    "            'mapped_entities': len(filtered_mappings),\n",
    "            'avg_confidence': (\n",
    "                sum(m['confidence'] for m in filtered_mappings) / len(filtered_mappings)\n",
    "                if filtered_mappings else 0.0\n",
    "            )\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Save to data/tool1/\n",
    "    output_dir = Path.cwd().parent / 'data' / 'tool1'\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    json_path = output_dir / 'filtered_dataset.json'\n",
    "    with open(json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(filtered_dataset, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"   ‚úÖ Dataset saved: {json_path}\")\n",
    "\n",
    "    # Save artifacts summary\n",
    "    artifacts_dir = Path.cwd().parent / 'scrum' / 'artifacts'\n",
    "    artifacts_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    artifact_path = artifacts_dir / f\"{timestamp.split('T')[0]}_tool1-ingest-summary.json\"\n",
    "    with open(artifact_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(filtered_dataset['stats'], f, indent=2)\n",
    "\n",
    "    print(f\"   ‚úÖ Artifact saved: {artifact_path}\")\n",
    "\n",
    "    return {\n",
    "        'output_json_path': str(json_path),\n",
    "        'output_artifact_path': str(artifact_path)\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Save node defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22b90b3",
   "metadata": {},
   "source": [
    "## 7. Build LangGraph\n",
    "\n",
    "**Status:** ‚úÖ Working\n",
    "\n",
    "Connect nodes: Load ‚Üí Prepare ‚Üí Mapping ‚Üí Filter ‚Üí Save\n",
    "\n",
    "**TODO:**\n",
    "- [ ] Add error recovery (retry failed nodes)\n",
    "- [ ] Add conditional edges (skip prepare if only 1 candidate)\n",
    "- [ ] Implement streaming progress updates\n",
    "- [ ] Add graph visualization (mermaid diagram)\n",
    "\n",
    "**IDEA:**\n",
    "- Parallel execution: Load + Schema validation simultaneously\n",
    "- Add human-in-the-loop node for low-confidence mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77f25bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LangGraph compiled\n",
      "\n",
      "üìä Graph structure:\n",
      "   START ‚Üí load ‚Üí prepare ‚Üí mapping ‚Üí filter ‚Üí save ‚Üí END\n"
     ]
    }
   ],
   "source": [
    "# Build state graph\n",
    "builder = StateGraph(Tool1State)\n",
    "\n",
    "# Add nodes\n",
    "builder.add_node('load', load_node)\n",
    "builder.add_node('prepare', prepare_node)\n",
    "builder.add_node('mapping', mapping_node)\n",
    "builder.add_node('filter', filter_node)\n",
    "builder.add_node('save', save_node)\n",
    "\n",
    "# Define edges\n",
    "builder.add_edge(START, 'load')\n",
    "builder.add_edge('load', 'prepare')\n",
    "builder.add_edge('prepare', 'mapping')\n",
    "builder.add_edge('mapping', 'filter')\n",
    "builder.add_edge('filter', 'save')\n",
    "builder.add_edge('save', END)\n",
    "\n",
    "# Compile graph\n",
    "graph = builder.compile()\n",
    "\n",
    "print(\"‚úÖ LangGraph compiled\")\n",
    "print(\"\\nüìä Graph structure:\")\n",
    "print(\"   START ‚Üí load ‚Üí prepare ‚Üí mapping ‚Üí filter ‚Üí save ‚Üí END\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89eae163",
   "metadata": {},
   "source": [
    "## 8. Execute Graph\n",
    "\n",
    "**Status:** ‚úÖ Working | **Avg Runtime:** ~8s (expected)\n",
    "\n",
    "Run the complete pipeline and stream progress.\n",
    "\n",
    "**TODO:**\n",
    "- [ ] Measure actual end-to-end time (10 runs avg)\n",
    "- [ ] Identify bottleneck node (likely Prepare or Mapping)\n",
    "- [ ] Add timeout handling (prevent infinite LLM waits)\n",
    "- [ ] Test with different Tool 0 inputs (Finance, Sales entities)\n",
    "\n",
    "**PERFORMANCE BASELINE (to be measured):**\n",
    "- Target: <10s end-to-end\n",
    "- Current: TBD (needs benchmarking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a66d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Executing Tool 1 pipeline...\n",
      "\n",
      "============================================================\n",
      "üìÇ Node 1: Loading data...\n",
      "   ‚úÖ Loaded 4 entities from Tool 0\n",
      "   ‚úÖ Scope out: HR data o zamƒõstnanc√≠ch; Finanƒçn√≠ forecasting a budgetov√°n√≠; Real-time monitoring dod√°vek; Integrace...\n",
      "   ‚úÖ Loaded 1 BS candidates\n",
      "   üìã Candidates:\n",
      "      - dm_bs_purchase: BS (Production Purchasing) Reporting Data Mart...\n",
      "üîß Node 2: LLM ranking candidates...\n",
      "   Input: 4 entities, 1 candidates\n",
      "   ‚úÖ Ranked 1 candidates\n",
      "   ‚úÖ Selected 1 relevant (score > 0.3)\n",
      "ü§ñ Node 3: LLM mapping with agent...\n",
      "   ‚úÖ Generated 4 mappings\n",
      "üîç Node 4: Applying blacklist filter...\n",
      "   ‚úÖ Filtered: 4 kept, 0 removed\n",
      "üíæ Node 5: Saving results...\n",
      "   ‚úÖ Dataset saved: /Users/marekminarovic/archi-agent/data/tool1/filtered_dataset.json\n",
      "   ‚úÖ Artifact saved: /Users/marekminarovic/archi-agent/scrum/artifacts/2025-11-01_tool1-ingest-summary.json\n",
      "============================================================\n",
      "\n",
      "‚úÖ Pipeline complete!\n",
      "\n",
      "üìä Final Results:\n",
      "   Entities processed: 4\n",
      "   Mappings generated: 4\n",
      "   Mappings after filter: 4\n",
      "   Output JSON: /Users/marekminarovic/archi-agent/data/tool1/filtered_dataset.json\n",
      "   Artifact: /Users/marekminarovic/archi-agent/scrum/artifacts/2025-11-01_tool1-ingest-summary.json\n"
     ]
    }
   ],
   "source": [
    "print(\"üöÄ Executing Tool 1 pipeline...\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Execute graph\n",
    "final_state = graph.invoke({})\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n‚úÖ Pipeline complete!\")\n",
    "print(f\"\\nüìä Final Results:\")\n",
    "print(f\"   Entities processed: {len(final_state.get('entities', []))}\")\n",
    "print(f\"   Mappings generated: {len(final_state.get('raw_mappings', []))}\")\n",
    "print(f\"   Mappings after filter: {len(final_state.get('filtered_mappings', []))}\")\n",
    "print(f\"   Output JSON: {final_state.get('output_json_path')}\")\n",
    "print(f\"   Artifact: {final_state.get('output_artifact_path')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d009dab8",
   "metadata": {},
   "source": [
    "## 9. Display Results\n",
    "\n",
    "**Status:** ‚úÖ Working\n",
    "\n",
    "Show filtered mappings with confidence scores.\n",
    "\n",
    "**TODO:**\n",
    "- [ ] Add color coding (green: high confidence, yellow: medium, red: low)\n",
    "- [ ] Export to formatted Markdown table\n",
    "- [ ] Add comparison with previous runs (diff view)\n",
    "- [ ] Show rejected mappings (filtered out by blacklist)\n",
    "\n",
    "**IDEA:**\n",
    "- Interactive widget for Jupyter (sliders for confidence threshold)\n",
    "- Generate PowerBI-ready CSV export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eac2e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Filtered Entity Mappings:\n",
      "\n",
      "================================================================================\n",
      "\n",
      "1. Suppliers (dodavatel√©) ‚Üí Systems>dap_gold_prod>dm_bs_purchase\n",
      "   ID: dm_bs_purchase\n",
      "   Confidence: 0.95\n",
      "   Rationale: dm_bs_purchase is a Production Purchasing Reporting Data Mart; suppliers (dodavatel√©) are core to purchasing data (supplier master, supplier transactions). Czech/English synonym match is direct. High relevance_score (0.9) supports strong match. Not related to excluded topics.\n",
      "\n",
      "2. Purchase Orders (n√°kupn√≠ objedn√°vky) ‚Üí Systems>dap_gold_prod>dm_bs_purchase\n",
      "   ID: dm_bs_purchase\n",
      "   Confidence: 0.98\n",
      "   Rationale: Purchase orders are the primary object of a purchasing data mart. Exact semantic match (n√°kupn√≠ objedn√°vky) and the candidate's description (BS Production Purchasing) make this the best-fit. Not related to excluded topics.\n",
      "\n",
      "3. Products (produkty) ‚Üí Systems>dap_gold_prod>dm_bs_purchase\n",
      "   ID: dm_bs_purchase\n",
      "   Confidence: 0.85\n",
      "   Rationale: Products are commonly included in purchasing/reporting data (items bought, SKU-level purchase history). Candidate likely contains product identifiers and purchase-related product attributes. Slightly lower confidence because a dedicated product/master-data mart might also exist but is not offered in the candidate list.\n",
      "\n",
      "4. Delivery Performance (v√Ωkonnost dod√°vek) ‚Üí Systems>dap_gold_prod>dm_bs_purchase\n",
      "   ID: dm_bs_purchase\n",
      "   Confidence: 0.62\n",
      "   Rationale: Delivery performance metrics (on-time delivery, lead times, fill rates) are often reported from purchasing data marts. However, the user-specified exclusion 'real-time monitoring dod√°vek' cautions against real-time delivery monitoring mappings; dm_bs_purchase appears to be a reporting/data-mart (not real-time) so it can support aggregated delivery performance reporting but may not meet real-time monitoring needs. Confidence is moderate for aggregated reporting, lower if the intent is real-time.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üìà Statistics:\n",
      "   Average confidence: 0.85\n",
      "   High confidence (‚â•0.8): 3/4\n"
     ]
    }
   ],
   "source": [
    "# Display filtered mappings\n",
    "filtered_mappings = final_state.get('filtered_mappings', [])\n",
    "\n",
    "print(\"üìã Filtered Entity Mappings:\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, mapping in enumerate(filtered_mappings, 1):\n",
    "    print(f\"\\n{i}. {mapping['entity']} ‚Üí {mapping['candidate_name']}\")\n",
    "    print(f\"   ID: {mapping['candidate_id']}\")\n",
    "    print(f\"   Confidence: {mapping['confidence']:.2f}\")\n",
    "    print(f\"   Rationale: {mapping['rationale']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# Calculate stats\n",
    "if filtered_mappings:\n",
    "    avg_confidence = sum(m['confidence'] for m in filtered_mappings) / len(filtered_mappings)\n",
    "    high_confidence = sum(1 for m in filtered_mappings if m['confidence'] >= 0.8)\n",
    "\n",
    "    print(f\"\\nüìà Statistics:\")\n",
    "    print(f\"   Average confidence: {avg_confidence:.2f}\")\n",
    "    print(f\"   High confidence (‚â•0.8): {high_confidence}/{len(filtered_mappings)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0e75e4",
   "metadata": {},
   "source": [
    "## 10. Summary\n",
    "\n",
    "‚úÖ **LangGraph Features Demonstrated:**\n",
    "- [x] Agent with discrete nodes (load, prepare, mapping, filter, save)\n",
    "- [x] **Multiple LLM nodes:**\n",
    "  - **Prepare Node:** LLM ranking (relevance scores)\n",
    "  - **Mapping Node:** LLM matching (confidence + rationale)\n",
    "- [x] Shared state (Tool1State) across all nodes\n",
    "- [x] Structured output via ToolStrategy in 2 contexts (CandidateRanking, MappingSuggestions)\n",
    "- [x] Scope_out context injection via system_prompt (no middleware)\n",
    "- [x] Streaming progress between nodes\n",
    "- [x] \"Each node does one thing well\" principle\n",
    "\n",
    "**Model:** openai:gpt-5-mini (consistent, no dynamic routing)\n",
    "\n",
    "**2-Stage LLM Approach:**\n",
    "1. **Prepare Node:** Ranks candidates by relevance (lightweight prefilter)\n",
    "2. **Mapping Node:** Detailed entity-to-candidate matching with confidence\n",
    "\n",
    "**Outputs:**\n",
    "- `data/tool1/filtered_dataset.json` - Complete dataset with mappings\n",
    "- `scrum/artifacts/YYYY-MM-DD_tool1-ingest-summary.json` - Statistics summary\n",
    "\n",
    "**Why This Architecture:**\n",
    "- Showcases LangGraph's multi-node intelligence pattern\n",
    "- Tests different structured output schemas (ranking vs mapping)\n",
    "- Demonstrates separation of concerns (relevance vs matching)\n",
    "- Not necessary for 2 candidates, but excellent LangGraph demonstration\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Development Status (2025-11-01)\n",
    "\n",
    "**What Works:**\n",
    "- ‚úÖ All 5 nodes execute successfully\n",
    "- ‚úÖ TypedDict fix applied (AgentState ‚Üí TypedDict)\n",
    "- ‚úÖ Scope_out injection working via system_prompt\n",
    "- ‚úÖ Structured output (ToolStrategy) in both LLM nodes\n",
    "- ‚úÖ Czech entity recognition (\"dodavatel√©\" ‚Üí \"suppliers\")\n",
    "\n",
    "**Known Issues:**\n",
    "- ‚ö†Ô∏è Load node: Hardcoded Tool 0 path\n",
    "- ‚ö†Ô∏è Prepare node: No validation for empty candidates list\n",
    "- ‚ö†Ô∏è Mapping node: Empty scope_out causes prompt formatting issue\n",
    "- ‚ö†Ô∏è Filter node: Blacklist too generic (needs domain expertise)\n",
    "\n",
    "**Next Session:**\n",
    "- [ ] Run compliance checker: `python3 .claude/skills/langchain/compliance-checker/check.py --file notebooks/tool1_ingest_demo.ipynb`\n",
    "- [ ] Measure performance baseline (10 runs average)\n",
    "- [ ] Parametrize Tool 0 input path\n",
    "- [ ] Add candidate count validation (min 1)\n",
    "- [ ] Test with dm_bs_logistics (3 candidates total)\n",
    "- [ ] Update story: `scrum/backlog/tool1-data-ingest.md` (`skill_created: true`)\n",
    "\n",
    "**Ideas for v2:**\n",
    "- Cache ranking results (avoid re-ranking same entities)\n",
    "- Add confidence calibration based on historical accuracy\n",
    "- Parallel LLM calls (prepare + mapping simultaneously?)\n",
    "- Human-in-the-loop for low-confidence mappings (<0.6)\n",
    "- Embedding similarity as faster alternative to LLM ranking"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
