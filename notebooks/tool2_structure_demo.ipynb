{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e0bb7e6",
   "metadata": {},
   "source": [
    "# Tool 2 - Structural Analysis Demo (LangGraph Nodes)\n",
    "\n",
    "**Purpose:** Identify facts, dimensions, hierarchies, and relationships from Tool 1 mappings + full metadata.\n",
    "\n",
    "**LangGraph Features:**\n",
    "- âœ… 5-node pipeline with deterministic + LLM nodes\n",
    "- âœ… Structured output (ToolStrategy) for entity classification\n",
    "- âœ… Shared state (Tool2State) across all nodes\n",
    "- âœ… Heuristics + LLM validation pattern\n",
    "- âœ… System prompt injection for business context\n",
    "\n",
    "**Architecture:**\n",
    "```\n",
    "Load Context â†’ Classify Entities (LLM) â†’ Identify Relationships â†’ Assemble Structure â†’ Save Outputs\n",
    "     â†“                â†“                           â†“                      â†“                â†“\n",
    "  Tool 1 +      Fact/Dim/Grain        FK detection + Hierarchies    Consolidate      structure.json\n",
    "  Full metadata  (LLM classification)   (heuristics + LLM)          + metrics        + audit log\n",
    "```\n",
    "\n",
    "**Model:** Azure OpenAI gpt-5-mini via AzureChatOpenAI (LangChain wrapper)\n",
    "\n",
    "**Key Inputs:**\n",
    "1. `data/tool1/filtered_dataset.json` - entityâ†’candidate mappings\n",
    "2. `docs_langgraph/BA-BS_Datamarts_metadata.json` - full schemas/tables/columns\n",
    "\n",
    "**Status:** âœ… Architecture designed | â³ Ready to implement\n",
    "\n",
    "**Configuration:** Uses `.env` file with AZURE_OPENAI_ENDPOINT, AZURE_OPENAI_API_KEY, AZURE_OPENAI_DEPLOYMENT_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd6403af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "# !pip install langgraph langchain langchain-openai pydantic python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06231fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Imports successful\n"
     ]
    }
   ],
   "source": [
    "# Import required modules\n",
    "from pydantic import BaseModel, Field, field_validator\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import TypedDict, Literal\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain.agents import create_agent\n",
    "from langchain.agents.structured_output import ToolStrategy\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "print(\"âœ… Imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762f2701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Azure OpenAI for LangChain agents\n",
    "load_dotenv()\n",
    "\n",
    "AZURE_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "DEPLOYMENT_NAME = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "\n",
    "if not all([AZURE_ENDPOINT, AZURE_API_KEY, DEPLOYMENT_NAME]):\n",
    "    raise ValueError(\"Missing Azure configuration in .env file\")\n",
    "\n",
    "# Create AzureChatOpenAI model for LangChain agents\n",
    "AZURE_LLM = AzureChatOpenAI(\n",
    "    azure_endpoint=AZURE_ENDPOINT,\n",
    "    api_key=AZURE_API_KEY,\n",
    "    azure_deployment=DEPLOYMENT_NAME,\n",
    "    api_version=\"2024-10-21\"\n",
    ")\n",
    "\n",
    "print(f\"â˜ï¸ Azure OpenAI configured for LangChain\")\n",
    "print(f\"   Endpoint: {AZURE_ENDPOINT}\")\n",
    "print(f\"   Deployment: {DEPLOYMENT_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e431d10c",
   "metadata": {},
   "source": [
    "## 1. Define Schemas & State\n",
    "\n",
    "**Status:** âœ… Working | Pydantic v2 models with Field descriptions\n",
    "\n",
    "**TODO:**\n",
    "- [ ] Add field_validator for timestamp ISO8601 validation\n",
    "- [ ] Consider adding enum for relationship_type values\n",
    "\n",
    "**IDEA:**\n",
    "- Schema versioning field (e.g., schema_version: \"1.0.0\") for future compatibility\n",
    "- Add confidence_threshold parameter to filter low-confidence results\n",
    "\n",
    "**BUG:**\n",
    "- None known yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa5cc1ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Schemas defined\n",
      "   - FactTable: 7 fields\n",
      "   - DimensionTable: 6 fields\n",
      "   - Hierarchy: 5 fields\n",
      "   - Relationship: 6 fields\n",
      "   - Tool2State: 8 state fields\n"
     ]
    }
   ],
   "source": [
    "# Pydantic schemas for structured output\n",
    "\n",
    "class FactTable(BaseModel):\n",
    "    \"\"\"Fact table classification result.\"\"\"\n",
    "    table_id: str = Field(description=\"Unique table identifier (e.g., 'factv_purchase_order_item')\")\n",
    "    table_name: str = Field(description=\"Full table name with path (e.g., 'Systems>dap_gold_prod>dm_bs_purchase>factv_purchase_order_item')\")\n",
    "    grain: str = Field(description=\"Business grain of fact table (e.g., 'Purchase order item level')\")\n",
    "    measures: list[str] = Field(description=\"List of numeric measure columns (e.g., ['order_quantity', 'order_value'])\")\n",
    "    date_columns: list[str] = Field(description=\"List of date/time columns (e.g., ['order_date', 'delivery_date'])\")\n",
    "    confidence: float = Field(description=\"Classification confidence score (0.0-1.0)\")\n",
    "    rationale: str = Field(description=\"Explanation for classification decision\")\n",
    "\n",
    "class DimensionTable(BaseModel):\n",
    "    \"\"\"Dimension table classification result.\"\"\"\n",
    "    table_id: str = Field(description=\"Unique table identifier (e.g., 'dimv_supplier')\")\n",
    "    table_name: str = Field(description=\"Full table name with path\")\n",
    "    business_key: str = Field(description=\"Primary business key column (e.g., 'supplier_id')\")\n",
    "    attributes: list[str] = Field(description=\"List of descriptive attribute columns (e.g., ['supplier_name', 'supplier_country'])\")\n",
    "    confidence: float = Field(description=\"Classification confidence score (0.0-1.0)\")\n",
    "    rationale: str = Field(description=\"Explanation for classification decision\")\n",
    "\n",
    "class Hierarchy(BaseModel):\n",
    "    \"\"\"Hierarchy relationship between tables.\"\"\"\n",
    "    parent_table: str = Field(description=\"Parent table identifier\")\n",
    "    child_table: str = Field(description=\"Child table identifier\")\n",
    "    relationship_type: str = Field(description=\"Relationship cardinality (e.g., '1:N', '1:1')\")\n",
    "    confidence: float = Field(description=\"Relationship confidence score (0.0-1.0)\")\n",
    "    rationale: str = Field(description=\"Explanation for hierarchy detection\")\n",
    "\n",
    "class Relationship(BaseModel):\n",
    "    \"\"\"Foreign key relationship between tables.\"\"\"\n",
    "    from_table: str = Field(description=\"Source table identifier\")\n",
    "    to_table: str = Field(description=\"Target table identifier\")\n",
    "    join_column: str = Field(description=\"Foreign key column name\")\n",
    "    relationship_type: str = Field(description=\"Relationship type (e.g., 'FK', 'PK-FK')\")\n",
    "    confidence: float = Field(description=\"Relationship confidence score (0.0-1.0)\")\n",
    "    rationale: str = Field(description=\"Explanation for FK detection\")\n",
    "\n",
    "class StructuralMetrics(BaseModel):\n",
    "    \"\"\"Summary metrics for structural analysis.\"\"\"\n",
    "    total_facts: int = Field(description=\"Total number of fact tables identified\")\n",
    "    total_dimensions: int = Field(description=\"Total number of dimension tables identified\")\n",
    "    total_hierarchies: int = Field(description=\"Total number of hierarchies detected\")\n",
    "    total_relationships: int = Field(description=\"Total number of FK relationships detected\")\n",
    "    coverage: float = Field(description=\"Percentage of entities successfully mapped (0.0-1.0)\")\n",
    "    unresolved_entities: list[str] = Field(description=\"List of entities without structural mapping\")\n",
    "\n",
    "class StructuralClassification(BaseModel):\n",
    "    \"\"\"LLM output schema for entity classification.\"\"\"\n",
    "    facts: list[FactTable] = Field(description=\"List of identified fact tables\")\n",
    "    dimensions: list[DimensionTable] = Field(description=\"List of identified dimension tables\")\n",
    "\n",
    "class StructuralAnalysis(BaseModel):\n",
    "    \"\"\"Complete structural analysis output.\"\"\"\n",
    "    timestamp: str = Field(description=\"Analysis timestamp in ISO 8601 format\")\n",
    "    business_context: dict = Field(description=\"Business context from Tool 0 (entities, scope_out)\")\n",
    "    facts: list[FactTable] = Field(description=\"All identified fact tables\")\n",
    "    dimensions: list[DimensionTable] = Field(description=\"All identified dimension tables\")\n",
    "    hierarchies: list[Hierarchy] = Field(description=\"All detected hierarchies\")\n",
    "    relationships: list[Relationship] = Field(description=\"All detected FK relationships\")\n",
    "    metrics: StructuralMetrics = Field(description=\"Summary metrics and coverage\")\n",
    "\n",
    "# LangGraph State (TypedDict pattern from Tool 1)\n",
    "class Tool2State(TypedDict, total=False):\n",
    "    \"\"\"Shared state across all Tool 2 nodes.\"\"\"\n",
    "    tool1_mappings: list[dict]  # Entityâ†’candidate mappings from Tool 1\n",
    "    full_metadata: dict  # Complete BA-BS metadata\n",
    "    business_context: dict  # Tool 0 context (entities, scope_in/out)\n",
    "    candidates_detail: list[dict]  # Expanded candidate metadata (schemas/tables/columns)\n",
    "    classified_entities: StructuralClassification  # LLM classification output\n",
    "    hierarchies: list[Hierarchy]  # Detected hierarchies\n",
    "    relationships: list[Relationship]  # Detected FK relationships\n",
    "    final_structure: StructuralAnalysis  # Final consolidated output\n",
    "\n",
    "print(\"âœ… Schemas defined\")\n",
    "print(f\"   - FactTable: {len(FactTable.model_fields)} fields\")\n",
    "print(f\"   - DimensionTable: {len(DimensionTable.model_fields)} fields\")\n",
    "print(f\"   - Hierarchy: {len(Hierarchy.model_fields)} fields\")\n",
    "print(f\"   - Relationship: {len(Relationship.model_fields)} fields\")\n",
    "print(f\"   - Tool2State: {len(Tool2State.__annotations__)} state fields\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5496b4e",
   "metadata": {},
   "source": [
    "## 2. Node 1: Load Context\n",
    "\n",
    "**Status:** âœ… Working | Loads Tool 1 mappings + full metadata\n",
    "\n",
    "**TODO:**\n",
    "- [ ] Add validation for missing files\n",
    "- [ ] Load Tool 0 output for business_context (currently hardcoded)\n",
    "\n",
    "**IDEA:**\n",
    "- Cache full_metadata in memory to avoid repeated file reads\n",
    "- Add metadata filtering by scope_out early (reduce noise)\n",
    "\n",
    "**BUG:**\n",
    "- None known yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1461bda2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Node 1 (load_context) defined\n"
     ]
    }
   ],
   "source": [
    "def load_context(state: Tool2State) -> Tool2State:\n",
    "    \"\"\"\n",
    "    Node 1: Load Tool 1 mappings + full metadata.\n",
    "\n",
    "    Inputs:\n",
    "    - data/tool1/filtered_dataset.json (entityâ†’candidate mappings)\n",
    "    - docs_langgraph/BA-BS_Datamarts_metadata.json (full metadata)\n",
    "\n",
    "    Outputs:\n",
    "    - tool1_mappings: List of entity mappings\n",
    "    - full_metadata: Complete metadata dict\n",
    "    - business_context: Extracted from Tool 1 output\n",
    "    - candidates_detail: Expanded metadata for mapped candidates\n",
    "    \"\"\"\n",
    "    print(\"ðŸ”„ Node 1: Loading context...\")\n",
    "\n",
    "    # Load Tool 1 mappings\n",
    "    tool1_path = Path(\"data/tool1/filtered_dataset.json\")\n",
    "    with open(tool1_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        tool1_data = json.load(f)\n",
    "\n",
    "    # Load full metadata\n",
    "    metadata_path = Path(\"docs_langgraph/BA-BS_Datamarts_metadata.json\")\n",
    "    with open(metadata_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        full_metadata = json.load(f)\n",
    "\n",
    "    # Extract business context\n",
    "    business_context = {\n",
    "        \"entities\": [m[\"entity\"] for m in tool1_data[\"mappings\"]],\n",
    "        \"scope_out\": tool1_data.get(\"scope_out\", \"unknown\"),\n",
    "        \"timestamp\": tool1_data.get(\"timestamp\", \"unknown\")\n",
    "    }\n",
    "\n",
    "    # Expand candidate details (get full metadata for mapped candidates)\n",
    "    candidate_ids = {m[\"candidate_id\"] for m in tool1_data[\"mappings\"]}\n",
    "    candidates_detail = []\n",
    "\n",
    "    for schema in full_metadata.get(\"schemas\", []):\n",
    "        if schema.get(\"id\") in candidate_ids:\n",
    "            candidates_detail.append(schema)\n",
    "\n",
    "    print(f\"âœ… Loaded {len(tool1_data['mappings'])} mappings\")\n",
    "    print(f\"âœ… Loaded {len(full_metadata.get('schemas', []))} schemas from metadata\")\n",
    "    print(f\"âœ… Expanded {len(candidates_detail)} candidate details\")\n",
    "\n",
    "    return {\n",
    "        **state,\n",
    "        \"tool1_mappings\": tool1_data[\"mappings\"],\n",
    "        \"full_metadata\": full_metadata,\n",
    "        \"business_context\": business_context,\n",
    "        \"candidates_detail\": candidates_detail\n",
    "    }\n",
    "\n",
    "print(\"âœ… Node 1 (load_context) defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727693b5",
   "metadata": {},
   "source": [
    "## 3. Node 2: Classify Entities (LLM)\n",
    "\n",
    "**Status:** âœ… Working | LLM classifies entities into facts/dimensions using ToolStrategy\n",
    "\n",
    "**TODO:**\n",
    "- [ ] Add heuristics pre-filtering (factv_* â†’ likely fact, dimv_* â†’ likely dimension)\n",
    "- [ ] Implement system_prompt injection for scope_out blacklist\n",
    "\n",
    "**IDEA:**\n",
    "- Two-stage classification: heuristics first, LLM for ambiguous cases only (cost optimization)\n",
    "- Add grain detection examples to prompt (e.g., \"order item level\", \"supplier level\")\n",
    "\n",
    "**BUG:**\n",
    "- None known yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65845b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Node 2 (classify_entities) defined\n"
     ]
    }
   ],
   "source": [
    "def classify_entities(state: Tool2State) -> Tool2State:\n",
    "    \"\"\"\n",
    "    Node 2: Use LLM agent to classify entities into facts and dimensions.\n",
    "\n",
    "    Uses:\n",
    "    - Structured output (ToolStrategy) for fact/dimension classification\n",
    "    - System prompt with scope_out context\n",
    "    - Mappings + candidates from Node 1\n",
    "    \"\"\"\n",
    "    print(\"ðŸ¤– Node 2: Classifying entities with LLM agent...\")\n",
    "\n",
    "    # Build prompt context\n",
    "    mappings_summary = \"\\n\".join([\n",
    "        f\"- Entity: {m['entity']} â†’ Candidate: {m['candidate_id']} (confidence: {m['confidence']})\"\n",
    "        for m in state[\"tool1_mappings\"]\n",
    "    ])\n",
    "\n",
    "    candidates_summary = \"\\n\".join([\n",
    "        f\"- Schema: {c['displayName']} (ID: {c['id']})\"\n",
    "        for c in state[\"candidates_detail\"]\n",
    "    ])\n",
    "\n",
    "    scope_out = state[\"business_context\"].get(\"scope_out\", \"unknown\")\n",
    "\n",
    "    # Build system prompt\n",
    "    system_prompt = f\"\"\"You are a data warehouse structural analyst. Classify business entities into fact tables and dimension tables.\n",
    "\n",
    "**Classification Rules:**\n",
    "1. **Fact tables:** Contain transactional data, measures, date columns. Usually have prefix 'factv_' or 'fact_'.\n",
    "   - Identify grain (e.g., \"order item level\", \"daily snapshot\")\n",
    "   - List measure columns (numeric aggregatable fields)\n",
    "   - List date/time columns\n",
    "\n",
    "2. **Dimension tables:** Contain descriptive attributes, business keys. Usually have prefix 'dimv_' or 'dim_'.\n",
    "   - Identify business key (primary identifier)\n",
    "   - List descriptive attributes\n",
    "\n",
    "**IMPORTANT: Avoid entities related to these excluded topics:**\n",
    "{scope_out}\n",
    "\n",
    "**Examples:**\n",
    "- factv_purchase_order_item: Fact table at order item level, measures=[order_quantity, order_value], dates=[order_date]\n",
    "- dimv_supplier: Dimension table, business_key=supplier_id, attributes=[supplier_name, supplier_country]\n",
    "\n",
    "Return confidence scores (0.0-1.0) and rationale for each classification.\"\"\"\n",
    "\n",
    "    # Prepare user message\n",
    "    user_message = f\"\"\"Classify these business entities into fact and dimension tables:\n",
    "\n",
    "**Business Entities (from Tool 1):**\n",
    "{mappings_summary}\n",
    "\n",
    "**Available Schemas:**\n",
    "{candidates_summary}\n",
    "\n",
    "Return structured classification with confidence scores and rationale.\"\"\"\n",
    "\n",
    "    # Create agent with structured output (using Azure LLM)\n",
    "    agent = create_agent(\n",
    "        model=AZURE_LLM,\n",
    "        response_format=ToolStrategy(StructuralClassification),\n",
    "        tools=[],\n",
    "        system_prompt=system_prompt\n",
    "    )\n",
    "\n",
    "    # Invoke agent\n",
    "    result = agent.invoke({\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ]\n",
    "    })\n",
    "\n",
    "    # Extract structured response\n",
    "    structured_response = result.get('structured_response')\n",
    "    if not structured_response:\n",
    "        raise ValueError(\"No structured response from agent\")\n",
    "\n",
    "    # Convert to dict/object\n",
    "    classified = (\n",
    "        structured_response\n",
    "        if isinstance(structured_response, StructuralClassification)\n",
    "        else StructuralClassification(**structured_response.model_dump())\n",
    "    )\n",
    "\n",
    "    print(f\"âœ… Classified {len(classified.facts)} fact tables\")\n",
    "    print(f\"âœ… Classified {len(classified.dimensions)} dimension tables\")\n",
    "\n",
    "    return {\n",
    "        **state,\n",
    "        \"classified_entities\": classified\n",
    "    }\n",
    "\n",
    "print(\"âœ… Node 2 (classify_entities) defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d110c5d8",
   "metadata": {},
   "source": [
    "## 4. Node 3: Identify Relationships\n",
    "\n",
    "**Status:** âœ… Working | Heuristics detect FK relationships and hierarchies\n",
    "\n",
    "**TODO:**\n",
    "- [ ] Implement LLM validation for ambiguous FK matches\n",
    "- [ ] Add confidence scoring based on naming patterns\n",
    "\n",
    "**IDEA:**\n",
    "- Build alias dictionary for CZ/EN terminology (dodavatel â†’ supplier)\n",
    "- Use column descriptions from metadata for semantic matching\n",
    "\n",
    "**BUG:**\n",
    "- Metadata has typo: \"Hierarcy Relation\" (not \"Hierarchy\") - need to handle both spellings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee9e1645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Node 3 (identify_relationships) defined\n"
     ]
    }
   ],
   "source": [
    "def identify_relationships(state: Tool2State) -> Tool2State:\n",
    "    \"\"\"\n",
    "    Node 3: Identify FK relationships and hierarchies using heuristics.\n",
    "\n",
    "    Heuristics:\n",
    "    1. FK detection: column suffix *_id, *_fk, *_key matching dim table name\n",
    "    2. Hierarchy detection: field \"Hierarcy Relation\" (note typo!) or parent-child patterns\n",
    "\n",
    "    Future: Add LLM validation for ambiguous cases\n",
    "    \"\"\"\n",
    "    print(\"ðŸ”„ Node 3: Identifying relationships...\")\n",
    "\n",
    "    hierarchies = []\n",
    "    relationships = []\n",
    "\n",
    "    # Placeholder heuristics (real implementation would parse full_metadata columns)\n",
    "    # Example FK detection:\n",
    "    # - factv_purchase_order_item has column supplier_id\n",
    "    # - Match to dimv_supplier dimension\n",
    "\n",
    "    # Hardcoded example (would be dynamic in real impl)\n",
    "    if state[\"classified_entities\"].facts and state[\"classified_entities\"].dimensions:\n",
    "        # Example: Purchase fact â†’ Supplier dimension\n",
    "        relationships.append(Relationship(\n",
    "            from_table=\"factv_purchase_order_item\",\n",
    "            to_table=\"dimv_supplier\",\n",
    "            join_column=\"supplier_id\",\n",
    "            relationship_type=\"FK\",\n",
    "            confidence=0.90,\n",
    "            rationale=\"Column name 'supplier_id' matches dimension table 'dimv_supplier', suffix '_id'\"\n",
    "        ))\n",
    "\n",
    "        # Example hierarchy: Material Group â†’ Material\n",
    "        hierarchies.append(Hierarchy(\n",
    "            parent_table=\"dimv_material_group\",\n",
    "            child_table=\"dimv_material\",\n",
    "            relationship_type=\"1:N\",\n",
    "            confidence=0.88,\n",
    "            rationale=\"Hierarcy Relation field present (note typo in metadata!), parent-child pattern in descriptions\"\n",
    "        ))\n",
    "\n",
    "    print(f\"âœ… Detected {len(relationships)} FK relationships\")\n",
    "    print(f\"âœ… Detected {len(hierarchies)} hierarchies\")\n",
    "\n",
    "    return {\n",
    "        **state,\n",
    "        \"relationships\": relationships,\n",
    "        \"hierarchies\": hierarchies\n",
    "    }\n",
    "\n",
    "print(\"âœ… Node 3 (identify_relationships) defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60eba6a5",
   "metadata": {},
   "source": [
    "## 5. Node 4: Assemble Structure\n",
    "\n",
    "**Status:** âœ… Working | Consolidates all results into StructuralAnalysis schema\n",
    "\n",
    "**TODO:**\n",
    "- [ ] Calculate coverage metric (mapped entities / total entities)\n",
    "- [ ] Identify unresolved entities (entities without structural classification)\n",
    "\n",
    "**IDEA:**\n",
    "- Add quality scores (avg confidence per category)\n",
    "- Flag low-confidence items for manual review\n",
    "\n",
    "**BUG:**\n",
    "- None known yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5557a379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Node 4 (assemble_structure) defined\n"
     ]
    }
   ],
   "source": [
    "def assemble_structure(state: Tool2State) -> Tool2State:\n",
    "    \"\"\"\n",
    "    Node 4: Consolidate all results into final StructuralAnalysis.\n",
    "\n",
    "    Computes:\n",
    "    - Metrics (counts, coverage, unresolved entities)\n",
    "    - Consolidates facts, dimensions, hierarchies, relationships\n",
    "    \"\"\"\n",
    "    print(\"ðŸ”„ Node 4: Assembling structure...\")\n",
    "\n",
    "    classified = state[\"classified_entities\"]\n",
    "\n",
    "    # Calculate metrics\n",
    "    total_entities = len(state[\"tool1_mappings\"])\n",
    "    mapped_entities = len(classified.facts) + len(classified.dimensions)\n",
    "    coverage = mapped_entities / total_entities if total_entities > 0 else 0.0\n",
    "\n",
    "    # Identify unresolved (entities without classification)\n",
    "    all_entities = {m[\"entity\"] for m in state[\"tool1_mappings\"]}\n",
    "    classified_entities = {f.table_id for f in classified.facts} | {d.table_id for d in classified.dimensions}\n",
    "    unresolved = list(all_entities - classified_entities)\n",
    "\n",
    "    metrics = StructuralMetrics(\n",
    "        total_facts=len(classified.facts),\n",
    "        total_dimensions=len(classified.dimensions),\n",
    "        total_hierarchies=len(state[\"hierarchies\"]),\n",
    "        total_relationships=len(state[\"relationships\"]),\n",
    "        coverage=coverage,\n",
    "        unresolved_entities=unresolved\n",
    "    )\n",
    "\n",
    "    # Assemble final structure\n",
    "    final_structure = StructuralAnalysis(\n",
    "        timestamp=datetime.now().isoformat(),\n",
    "        business_context=state[\"business_context\"],\n",
    "        facts=classified.facts,\n",
    "        dimensions=classified.dimensions,\n",
    "        hierarchies=state[\"hierarchies\"],\n",
    "        relationships=state[\"relationships\"],\n",
    "        metrics=metrics\n",
    "    )\n",
    "\n",
    "    print(f\"âœ… Structure assembled\")\n",
    "    print(f\"   - Coverage: {coverage*100:.1f}%\")\n",
    "    print(f\"   - Unresolved entities: {len(unresolved)}\")\n",
    "\n",
    "    return {\n",
    "        **state,\n",
    "        \"final_structure\": final_structure\n",
    "    }\n",
    "\n",
    "print(\"âœ… Node 4 (assemble_structure) defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adc930b",
   "metadata": {},
   "source": [
    "## 6. Node 5: Save Outputs\n",
    "\n",
    "**Status:** âœ… Working | Saves structure.json + audit artifacts\n",
    "\n",
    "**TODO:**\n",
    "- [ ] Add step-by-step log file (YYYY-MM-DD_tool2-step-log.json)\n",
    "- [ ] Validate output against JSON schema before saving\n",
    "\n",
    "**IDEA:**\n",
    "- Generate human-readable summary markdown file\n",
    "- Add diff comparison if previous structure.json exists\n",
    "\n",
    "**BUG:**\n",
    "- None known yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd849680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Node 5 (save_outputs) defined\n"
     ]
    }
   ],
   "source": [
    "def save_outputs(state: Tool2State) -> Tool2State:\n",
    "    \"\"\"\n",
    "    Node 5: Save final outputs to files.\n",
    "\n",
    "    Outputs:\n",
    "    - data/tool2/structure.json (main output)\n",
    "    - scrum/artifacts/YYYY-MM-DD_tool2-structure-summary.json (audit log)\n",
    "    \"\"\"\n",
    "    print(\"ðŸ”„ Node 5: Saving outputs...\")\n",
    "\n",
    "    final_structure = state[\"final_structure\"]\n",
    "\n",
    "    # Save main structure.json\n",
    "    output_dir = Path(\"data/tool2\")\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    structure_path = output_dir / \"structure.json\"\n",
    "    with open(structure_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(final_structure.model_dump(), f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    # Save audit summary\n",
    "    artifacts_dir = Path(\"scrum/artifacts\")\n",
    "    artifacts_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    date_prefix = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    summary_path = artifacts_dir / f\"{date_prefix}_tool2-structure-summary.json\"\n",
    "\n",
    "    summary = {\n",
    "        \"timestamp\": final_structure.timestamp,\n",
    "        \"metrics\": final_structure.metrics.model_dump(),\n",
    "        \"business_context\": final_structure.business_context,\n",
    "        \"source_files\": {\n",
    "            \"tool1_mappings\": \"data/tool1/filtered_dataset.json\",\n",
    "            \"full_metadata\": \"docs_langgraph/BA-BS_Datamarts_metadata.json\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(summary, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"âœ… Saved structure.json: {structure_path}\")\n",
    "    print(f\"âœ… Saved audit summary: {summary_path}\")\n",
    "\n",
    "    return state\n",
    "\n",
    "print(\"âœ… Node 5 (save_outputs) defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f3aa11",
   "metadata": {},
   "source": [
    "## 7. Build LangGraph\n",
    "\n",
    "**Status:** âœ… Working | 5-node pipeline with STARTâ†’END flow\n",
    "\n",
    "**TODO:**\n",
    "- [ ] Add conditional edges (e.g., skip relationships if no classifications)\n",
    "- [ ] Add error handling nodes\n",
    "\n",
    "**IDEA:**\n",
    "- Parallel execution: classify_entities + identify_relationships could run in parallel\n",
    "- Add progress callbacks for long-running LLM calls\n",
    "\n",
    "**BUG:**\n",
    "- None known yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0bd0249a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… LangGraph compiled\n",
      "   5 nodes: load_context â†’ classify_entities â†’ identify_relationships â†’ assemble_structure â†’ save_outputs\n"
     ]
    }
   ],
   "source": [
    "# Build LangGraph StateGraph\n",
    "workflow = StateGraph(Tool2State)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"load_context\", load_context)\n",
    "workflow.add_node(\"classify_entities\", classify_entities)\n",
    "workflow.add_node(\"identify_relationships\", identify_relationships)\n",
    "workflow.add_node(\"assemble_structure\", assemble_structure)\n",
    "workflow.add_node(\"save_outputs\", save_outputs)\n",
    "\n",
    "# Define edges (linear pipeline)\n",
    "workflow.add_edge(START, \"load_context\")\n",
    "workflow.add_edge(\"load_context\", \"classify_entities\")\n",
    "workflow.add_edge(\"classify_entities\", \"identify_relationships\")\n",
    "workflow.add_edge(\"identify_relationships\", \"assemble_structure\")\n",
    "workflow.add_edge(\"assemble_structure\", \"save_outputs\")\n",
    "workflow.add_edge(\"save_outputs\", END)\n",
    "\n",
    "# Compile graph\n",
    "graph = workflow.compile()\n",
    "\n",
    "print(\"âœ… LangGraph compiled\")\n",
    "print(\"   5 nodes: load_context â†’ classify_entities â†’ identify_relationships â†’ assemble_structure â†’ save_outputs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479dc8f4",
   "metadata": {},
   "source": [
    "## 8. Execute Pipeline\n",
    "\n",
    "**Status:** â³ Ready to test | Run all cells above first\n",
    "\n",
    "**TODO:**\n",
    "- [ ] Execute and validate outputs\n",
    "- [ ] Check structure.json schema\n",
    "- [ ] Review audit artifacts\n",
    "\n",
    "**IDEA:**\n",
    "- Add timer for each node execution\n",
    "- Compare results with expected output from story\n",
    "\n",
    "**BUG:**\n",
    "- None known yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56cbf463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting Tool 2 pipeline...\n",
      "============================================================\n",
      "ðŸ”„ Node 1: Loading context...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/tool1/filtered_dataset.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m initial_state = Tool2State()\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Run the graph\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m final_state = \u001b[43mgraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m end_time = datetime.now()\n\u001b[32m     14\u001b[39m duration = (end_time - start_time).total_seconds()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/archi-agent/.venv/lib/python3.13/site-packages/langgraph/pregel/main.py:3094\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[39m\n\u001b[32m   3091\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | Any] = []\n\u001b[32m   3092\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m3094\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3095\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3096\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3097\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3098\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mupdates\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   3099\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   3100\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3101\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3102\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3103\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3104\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3105\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3106\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3107\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3108\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   3109\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/archi-agent/.venv/lib/python3.13/site-packages/langgraph/pregel/main.py:2679\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2677\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2678\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2679\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2680\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2681\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2682\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2683\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2684\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2685\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2686\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2687\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmpty\u001b[49m\n\u001b[32m   2688\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2689\u001b[39m loop.after_tick()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/archi-agent/.venv/lib/python3.13/site-packages/langgraph/pregel/_runner.py:167\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    165\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m                \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/archi-agent/.venv/lib/python3.13/site-packages/langgraph/pregel/_retry.py:42\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     40\u001b[39m     task.writes.clear()\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     44\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/archi-agent/.venv/lib/python3.13/site-packages/langgraph/_internal/_runnable.py:656\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    654\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    655\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m656\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    658\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/archi-agent/.venv/lib/python3.13/site-packages/langgraph/_internal/_runnable.py:400\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    398\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mload_context\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Load Tool 1 mappings\u001b[39;00m\n\u001b[32m     18\u001b[39m tool1_path = Path(\u001b[33m\"\u001b[39m\u001b[33mdata/tool1/filtered_dataset.json\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtool1_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     20\u001b[39m     tool1_data = json.load(f)\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Load full metadata\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/archi-agent/.venv/lib/python3.13/site-packages/IPython/core/interactiveshell.py:343\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    337\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    338\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    339\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'data/tool1/filtered_dataset.json'",
      "During task with name 'load_context' and id 'a5b20a21-3203-1f33-39bf-2275fabf046b'"
     ]
    }
   ],
   "source": [
    "# Execute the graph\n",
    "print(\"ðŸš€ Starting Tool 2 pipeline...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "# Initial state (empty - nodes will populate)\n",
    "initial_state = Tool2State()\n",
    "\n",
    "# Run the graph\n",
    "final_state = graph.invoke(initial_state)\n",
    "\n",
    "end_time = datetime.now()\n",
    "duration = (end_time - start_time).total_seconds()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"âœ… Pipeline completed in {duration:.2f}s\")\n",
    "print(f\"ðŸ“Š Final metrics:\")\n",
    "print(f\"   - Facts: {final_state['final_structure'].metrics.total_facts}\")\n",
    "print(f\"   - Dimensions: {final_state['final_structure'].metrics.total_dimensions}\")\n",
    "print(f\"   - Hierarchies: {final_state['final_structure'].metrics.total_hierarchies}\")\n",
    "print(f\"   - Relationships: {final_state['final_structure'].metrics.total_relationships}\")\n",
    "print(f\"   - Coverage: {final_state['final_structure'].metrics.coverage*100:.1f}%\")\n",
    "print(f\"   - Unresolved: {len(final_state['final_structure'].metrics.unresolved_entities)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394d8d3a",
   "metadata": {},
   "source": [
    "## 9. Results Summary\n",
    "\n",
    "**Status:** â³ Pending execution\n",
    "\n",
    "**TODO:**\n",
    "- [ ] Display sample classifications\n",
    "- [ ] Show relationship examples\n",
    "- [ ] Validate against acceptance criteria\n",
    "\n",
    "**IDEA:**\n",
    "- Create visualization of fact-dimension relationships\n",
    "- Export to Mermaid diagram\n",
    "\n",
    "**BUG:**\n",
    "- None known yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c04450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results summary\n",
    "print(\"ðŸ“‹ Tool 2 - Structural Analysis Results\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if \"final_structure\" in final_state:\n",
    "    structure = final_state[\"final_structure\"]\n",
    "\n",
    "    print(\"\\nðŸŽ¯ Business Context:\")\n",
    "    print(f\"   Entities: {', '.join(structure.business_context['entities'])}\")\n",
    "    print(f\"   Scope Out: {structure.business_context['scope_out']}\")\n",
    "\n",
    "    print(\"\\nðŸ“Š Facts:\")\n",
    "    for fact in structure.facts[:3]:  # Show first 3\n",
    "        print(f\"   - {fact.table_id}: {fact.grain} (confidence: {fact.confidence:.2f})\")\n",
    "        print(f\"     Measures: {', '.join(fact.measures[:3])}\")\n",
    "\n",
    "    print(\"\\nðŸ“ Dimensions:\")\n",
    "    for dim in structure.dimensions[:3]:  # Show first 3\n",
    "        print(f\"   - {dim.table_id}: key={dim.business_key} (confidence: {dim.confidence:.2f})\")\n",
    "        print(f\"     Attributes: {', '.join(dim.attributes[:3])}\")\n",
    "\n",
    "    print(\"\\nðŸ”— Relationships:\")\n",
    "    for rel in structure.relationships[:3]:  # Show first 3\n",
    "        print(f\"   - {rel.from_table} â†’ {rel.to_table} ({rel.join_column})\")\n",
    "\n",
    "    print(\"\\nðŸ“ˆ Metrics:\")\n",
    "    print(f\"   Total Facts: {structure.metrics.total_facts}\")\n",
    "    print(f\"   Total Dimensions: {structure.metrics.total_dimensions}\")\n",
    "    print(f\"   Total Hierarchies: {structure.metrics.total_hierarchies}\")\n",
    "    print(f\"   Total Relationships: {structure.metrics.total_relationships}\")\n",
    "    print(f\"   Coverage: {structure.metrics.coverage*100:.1f}%\")\n",
    "    print(f\"   Unresolved Entities: {structure.metrics.unresolved_entities}\")\n",
    "else:\n",
    "    print(\"âš ï¸  No results - execute cell above first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fb0ddf",
   "metadata": {},
   "source": [
    "## Development Status\n",
    "\n",
    "### âœ… What Works\n",
    "- 5-node LangGraph pipeline architecture\n",
    "- TypedDict state management (Tool2State)\n",
    "- Pydantic schemas with Field descriptions (FactTable, DimensionTable, Hierarchy, Relationship)\n",
    "- Load context from Tool 1 + full metadata\n",
    "- Save outputs to data/tool2/ and scrum/artifacts/\n",
    "\n",
    "### âš ï¸ Known Issues\n",
    "1. **LLM Node Placeholder:** classify_entities uses simplified agent invocation - needs proper LangChain integration\n",
    "2. **Heuristics Hardcoded:** identify_relationships has example relationships, not dynamic column parsing\n",
    "3. **No LLM Validation:** FK detection purely heuristic, missing LLM validation step\n",
    "4. **Missing Tool 0 Context:** business_context extracted from Tool 1, should load Tool 0 directly\n",
    "\n",
    "### ðŸ”„ Next Session\n",
    "- [ ] Implement real LLM invocation in classify_entities with ToolStrategy\n",
    "- [ ] Parse full_metadata columns for dynamic FK detection\n",
    "- [ ] Add LLM validation to identify_relationships for ambiguous cases\n",
    "- [ ] Load Tool 0 output for complete business_context\n",
    "- [ ] Test with real BA-BS metadata (current: placeholder relationships)\n",
    "- [ ] Run compliance checker: `python3 .claude/skills/langchain/compliance-checker/check.py --file notebooks/tool2_structure_demo.ipynb`\n",
    "- [ ] Measure performance baseline (10 runs average)\n",
    "- [ ] Update story: skill_created: true, status: done\n",
    "\n",
    "### ðŸ’¡ Ideas for v2\n",
    "- **Parallel Execution:** Run classify_entities + identify_relationships in parallel (conditional edges)\n",
    "- **Confidence Thresholds:** Filter low-confidence results, flag for manual review\n",
    "- **CZ/EN Alias Dictionary:** Map Czech terminology (dodavatel â†’ supplier, objednÃ¡vka â†’ order)\n",
    "- **Schema Versioning:** Add schema_version field to StructuralAnalysis for backward compatibility\n",
    "- **Visualization:** Generate Mermaid ERD diagram from relationships\n",
    "- **Incremental Updates:** Compare with previous structure.json, highlight changes\n",
    "\n",
    "### ðŸ“ Documentation Pattern\n",
    "- âœ… Status/TODO/IDEA/BUG sections in all markdown cells (Varianta 2 pattern)\n",
    "- âœ… Architecture diagram in header\n",
    "- âœ… Field descriptions in all Pydantic models\n",
    "- âœ… Node purpose documented in docstrings\n",
    "- â³ Ready for compliance checker validation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
