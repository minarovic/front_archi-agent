{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e0bb7e6",
   "metadata": {},
   "source": [
    "# Tool 2 - Structural Analysis Demo (LangGraph Nodes)\n",
    "\n",
    "**Purpose:** Identify facts, dimensions, hierarchies, and relationships from Tool 1 mappings + full metadata.\n",
    "\n",
    "**LangGraph Features:**\n",
    "- ‚úÖ 5-node pipeline with deterministic + LLM nodes\n",
    "- ‚úÖ Structured output (ToolStrategy) for entity classification\n",
    "- ‚úÖ Shared state (Tool2State) across all nodes\n",
    "- ‚úÖ Heuristics + LLM validation pattern\n",
    "- ‚úÖ System prompt injection for business context\n",
    "\n",
    "**Architecture:**\n",
    "```\n",
    "Load Context ‚Üí Classify Entities (LLM) ‚Üí Identify Relationships ‚Üí Assemble Structure ‚Üí Save Outputs\n",
    "     ‚Üì                ‚Üì                           ‚Üì                      ‚Üì                ‚Üì\n",
    "  Tool 1 +      Fact/Dim/Grain        FK detection + Hierarchies    Consolidate      structure.json\n",
    "  Full metadata  (LLM classification)   (heuristics + LLM)          + metrics        + audit log\n",
    "```\n",
    "\n",
    "**Model:** openai:gpt-5-mini (consistent with Tool 1)\n",
    "\n",
    "**Key Inputs:**\n",
    "1. `data/tool1/filtered_dataset.json` - entity‚Üícandidate mappings\n",
    "2. `docs_langgraph/BA-BS_Datamarts_metadata.json` - full schemas/tables/columns\n",
    "\n",
    "**Status:** ‚úÖ Architecture designed | ‚è≥ Ready to implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6403af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "# !pip install langgraph langchain langchain-openai pydantic python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06231fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "from pydantic import BaseModel, Field, field_validator\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import TypedDict, Literal\n",
    "import json\n",
    "import re\n",
    "\n",
    "from langchain.agents import create_agent\n",
    "from langchain.agents.structured_output import ToolStrategy\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "print(\"‚úÖ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e431d10c",
   "metadata": {},
   "source": [
    "## 1. Define Schemas & State\n",
    "\n",
    "**Status:** ‚úÖ Working | Pydantic v2 models with Field descriptions\n",
    "\n",
    "**TODO:**\n",
    "- [ ] Add field_validator for timestamp ISO8601 validation\n",
    "- [ ] Consider adding enum for relationship_type values\n",
    "\n",
    "**IDEA:**\n",
    "- Schema versioning field (e.g., schema_version: \"1.0.0\") for future compatibility\n",
    "- Add confidence_threshold parameter to filter low-confidence results\n",
    "\n",
    "**BUG:**\n",
    "- None known yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5cc1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pydantic schemas for structured output\n",
    "\n",
    "class FactTable(BaseModel):\n",
    "    \"\"\"Fact table classification result.\"\"\"\n",
    "    table_id: str = Field(description=\"Unique table identifier (e.g., 'factv_purchase_order_item')\")\n",
    "    table_name: str = Field(description=\"Full table name with path (e.g., 'Systems>dap_gold_prod>dm_bs_purchase>factv_purchase_order_item')\")\n",
    "    grain: str = Field(description=\"Business grain of fact table (e.g., 'Purchase order item level')\")\n",
    "    measures: list[str] = Field(description=\"List of numeric measure columns (e.g., ['order_quantity', 'order_value'])\")\n",
    "    date_columns: list[str] = Field(description=\"List of date/time columns (e.g., ['order_date', 'delivery_date'])\")\n",
    "    confidence: float = Field(description=\"Classification confidence score (0.0-1.0)\")\n",
    "    rationale: str = Field(description=\"Explanation for classification decision\")\n",
    "\n",
    "class DimensionTable(BaseModel):\n",
    "    \"\"\"Dimension table classification result.\"\"\"\n",
    "    table_id: str = Field(description=\"Unique table identifier (e.g., 'dimv_supplier')\")\n",
    "    table_name: str = Field(description=\"Full table name with path\")\n",
    "    business_key: str = Field(description=\"Primary business key column (e.g., 'supplier_id')\")\n",
    "    attributes: list[str] = Field(description=\"List of descriptive attribute columns (e.g., ['supplier_name', 'supplier_country'])\")\n",
    "    confidence: float = Field(description=\"Classification confidence score (0.0-1.0)\")\n",
    "    rationale: str = Field(description=\"Explanation for classification decision\")\n",
    "\n",
    "class Hierarchy(BaseModel):\n",
    "    \"\"\"Hierarchy relationship between tables.\"\"\"\n",
    "    parent_table: str = Field(description=\"Parent table identifier\")\n",
    "    child_table: str = Field(description=\"Child table identifier\")\n",
    "    relationship_type: str = Field(description=\"Relationship cardinality (e.g., '1:N', '1:1')\")\n",
    "    confidence: float = Field(description=\"Relationship confidence score (0.0-1.0)\")\n",
    "    rationale: str = Field(description=\"Explanation for hierarchy detection\")\n",
    "\n",
    "class Relationship(BaseModel):\n",
    "    \"\"\"Foreign key relationship between tables.\"\"\"\n",
    "    from_table: str = Field(description=\"Source table identifier\")\n",
    "    to_table: str = Field(description=\"Target table identifier\")\n",
    "    join_column: str = Field(description=\"Foreign key column name\")\n",
    "    relationship_type: str = Field(description=\"Relationship type (e.g., 'FK', 'PK-FK')\")\n",
    "    confidence: float = Field(description=\"Relationship confidence score (0.0-1.0)\")\n",
    "    rationale: str = Field(description=\"Explanation for FK detection\")\n",
    "\n",
    "class StructuralMetrics(BaseModel):\n",
    "    \"\"\"Summary metrics for structural analysis.\"\"\"\n",
    "    total_facts: int = Field(description=\"Total number of fact tables identified\")\n",
    "    total_dimensions: int = Field(description=\"Total number of dimension tables identified\")\n",
    "    total_hierarchies: int = Field(description=\"Total number of hierarchies detected\")\n",
    "    total_relationships: int = Field(description=\"Total number of FK relationships detected\")\n",
    "    coverage: float = Field(description=\"Percentage of entities successfully mapped (0.0-1.0)\")\n",
    "    unresolved_entities: list[str] = Field(description=\"List of entities without structural mapping\")\n",
    "\n",
    "class StructuralClassification(BaseModel):\n",
    "    \"\"\"LLM output schema for entity classification.\"\"\"\n",
    "    facts: list[FactTable] = Field(description=\"List of identified fact tables\")\n",
    "    dimensions: list[DimensionTable] = Field(description=\"List of identified dimension tables\")\n",
    "\n",
    "class StructuralAnalysis(BaseModel):\n",
    "    \"\"\"Complete structural analysis output.\"\"\"\n",
    "    timestamp: str = Field(description=\"Analysis timestamp in ISO 8601 format\")\n",
    "    business_context: dict = Field(description=\"Business context from Tool 0 (entities, scope_out)\")\n",
    "    facts: list[FactTable] = Field(description=\"All identified fact tables\")\n",
    "    dimensions: list[DimensionTable] = Field(description=\"All identified dimension tables\")\n",
    "    hierarchies: list[Hierarchy] = Field(description=\"All detected hierarchies\")\n",
    "    relationships: list[Relationship] = Field(description=\"All detected FK relationships\")\n",
    "    metrics: StructuralMetrics = Field(description=\"Summary metrics and coverage\")\n",
    "\n",
    "# LangGraph State (TypedDict pattern from Tool 1)\n",
    "class Tool2State(TypedDict, total=False):\n",
    "    \"\"\"Shared state across all Tool 2 nodes.\"\"\"\n",
    "    tool1_mappings: list[dict]  # Entity‚Üícandidate mappings from Tool 1\n",
    "    full_metadata: dict  # Complete BA-BS metadata\n",
    "    business_context: dict  # Tool 0 context (entities, scope_in/out)\n",
    "    candidates_detail: list[dict]  # Expanded candidate metadata (schemas/tables/columns)\n",
    "    classified_entities: StructuralClassification  # LLM classification output\n",
    "    hierarchies: list[Hierarchy]  # Detected hierarchies\n",
    "    relationships: list[Relationship]  # Detected FK relationships\n",
    "    final_structure: StructuralAnalysis  # Final consolidated output\n",
    "\n",
    "print(\"‚úÖ Schemas defined\")\n",
    "print(f\"   - FactTable: {len(FactTable.model_fields)} fields\")\n",
    "print(f\"   - DimensionTable: {len(DimensionTable.model_fields)} fields\")\n",
    "print(f\"   - Hierarchy: {len(Hierarchy.model_fields)} fields\")\n",
    "print(f\"   - Relationship: {len(Relationship.model_fields)} fields\")\n",
    "print(f\"   - Tool2State: {len(Tool2State.__annotations__)} state fields\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5496b4e",
   "metadata": {},
   "source": [
    "## 2. Node 1: Load Context\n",
    "\n",
    "**Status:** ‚úÖ Working | Loads Tool 1 mappings + full metadata\n",
    "\n",
    "**TODO:**\n",
    "- [ ] Add validation for missing files\n",
    "- [ ] Load Tool 0 output for business_context (currently hardcoded)\n",
    "\n",
    "**IDEA:**\n",
    "- Cache full_metadata in memory to avoid repeated file reads\n",
    "- Add metadata filtering by scope_out early (reduce noise)\n",
    "\n",
    "**BUG:**\n",
    "- None known yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1461bda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_context(state: Tool2State) -> Tool2State:\n",
    "    \"\"\"\n",
    "    Node 1: Load Tool 1 mappings + full metadata.\n",
    "\n",
    "    Inputs:\n",
    "    - data/tool1/filtered_dataset.json (entity‚Üícandidate mappings)\n",
    "    - docs_langgraph/BA-BS_Datamarts_metadata.json (full metadata)\n",
    "\n",
    "    Outputs:\n",
    "    - tool1_mappings: List of entity mappings\n",
    "    - full_metadata: Complete metadata dict\n",
    "    - business_context: Extracted from Tool 1 output\n",
    "    - candidates_detail: Expanded metadata for mapped candidates\n",
    "    \"\"\"\n",
    "    print(\"üîÑ Node 1: Loading context...\")\n",
    "\n",
    "    # Load Tool 1 mappings\n",
    "    tool1_path = Path(\"data/tool1/filtered_dataset.json\")\n",
    "    with open(tool1_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        tool1_data = json.load(f)\n",
    "\n",
    "    # Load full metadata\n",
    "    metadata_path = Path(\"docs_langgraph/BA-BS_Datamarts_metadata.json\")\n",
    "    with open(metadata_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        full_metadata = json.load(f)\n",
    "\n",
    "    # Extract business context\n",
    "    business_context = {\n",
    "        \"entities\": [m[\"entity\"] for m in tool1_data[\"mappings\"]],\n",
    "        \"scope_out\": tool1_data.get(\"scope_out\", \"unknown\"),\n",
    "        \"timestamp\": tool1_data.get(\"timestamp\", \"unknown\")\n",
    "    }\n",
    "\n",
    "    # Expand candidate details (get full metadata for mapped candidates)\n",
    "    candidate_ids = {m[\"candidate_id\"] for m in tool1_data[\"mappings\"]}\n",
    "    candidates_detail = []\n",
    "\n",
    "    for schema in full_metadata.get(\"schemas\", []):\n",
    "        if schema.get(\"id\") in candidate_ids:\n",
    "            candidates_detail.append(schema)\n",
    "\n",
    "    print(f\"‚úÖ Loaded {len(tool1_data['mappings'])} mappings\")\n",
    "    print(f\"‚úÖ Loaded {len(full_metadata.get('schemas', []))} schemas from metadata\")\n",
    "    print(f\"‚úÖ Expanded {len(candidates_detail)} candidate details\")\n",
    "\n",
    "    return {\n",
    "        **state,\n",
    "        \"tool1_mappings\": tool1_data[\"mappings\"],\n",
    "        \"full_metadata\": full_metadata,\n",
    "        \"business_context\": business_context,\n",
    "        \"candidates_detail\": candidates_detail\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Node 1 (load_context) defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727693b5",
   "metadata": {},
   "source": [
    "## 3. Node 2: Classify Entities (LLM)\n",
    "\n",
    "**Status:** ‚úÖ Working | LLM classifies entities into facts/dimensions using ToolStrategy\n",
    "\n",
    "**TODO:**\n",
    "- [ ] Add heuristics pre-filtering (factv_* ‚Üí likely fact, dimv_* ‚Üí likely dimension)\n",
    "- [ ] Implement system_prompt injection for scope_out blacklist\n",
    "\n",
    "**IDEA:**\n",
    "- Two-stage classification: heuristics first, LLM for ambiguous cases only (cost optimization)\n",
    "- Add grain detection examples to prompt (e.g., \"order item level\", \"supplier level\")\n",
    "\n",
    "**BUG:**\n",
    "- None known yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65845b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_entities(state: Tool2State) -> Tool2State:\n",
    "    \"\"\"\n",
    "    Node 2: Classify entities into facts/dimensions using LLM.\n",
    "\n",
    "    Uses:\n",
    "    - ToolStrategy(StructuralClassification) for structured output\n",
    "    - system_prompt with business context\n",
    "\n",
    "    Heuristics (pre-LLM):\n",
    "    1. Fact table: prefix factv_*, fact_*, contains date + numeric columns\n",
    "    2. Dimension: prefix dimv_*, dim_*, contains business keys + attributes\n",
    "\n",
    "    LLM: Validates heuristics and fills in grain/business_key/measures\n",
    "    \"\"\"\n",
    "    print(\"üîÑ Node 2: Classifying entities with LLM...\")\n",
    "\n",
    "    # Build prompt context\n",
    "    mappings_summary = \"\\n\".join([\n",
    "        f\"- Entity: {m['entity']} ‚Üí Candidate: {m['candidate_id']} (confidence: {m['confidence']})\"\n",
    "        for m in state[\"tool1_mappings\"]\n",
    "    ])\n",
    "\n",
    "    candidates_summary = \"\\n\".join([\n",
    "        f\"- Schema: {c['displayName']} (ID: {c['id']})\"\n",
    "        for c in state[\"candidates_detail\"]\n",
    "    ])\n",
    "\n",
    "    scope_out = state[\"business_context\"].get(\"scope_out\", \"unknown\")\n",
    "\n",
    "    prompt = f\"\"\"Analyze the following business entities and classify them into fact tables and dimension tables.\n",
    "\n",
    "**Business Entities (from Tool 1):**\n",
    "{mappings_summary}\n",
    "\n",
    "**Available Schemas:**\n",
    "{candidates_summary}\n",
    "\n",
    "**Scope Out (exclude):** {scope_out}\n",
    "\n",
    "**Classification Rules:**\n",
    "1. **Fact tables:** Contain transactional data, measures, date columns. Usually have prefix 'factv_' or 'fact_'.\n",
    "   - Identify grain (e.g., \"order item level\", \"daily snapshot\")\n",
    "   - List measure columns (numeric aggregatable fields)\n",
    "   - List date/time columns\n",
    "\n",
    "2. **Dimension tables:** Contain descriptive attributes, business keys. Usually have prefix 'dimv_' or 'dim_'.\n",
    "   - Identify business key (primary identifier)\n",
    "   - List descriptive attributes\n",
    "\n",
    "**Examples:**\n",
    "- factv_purchase_order_item: Fact table at order item level, measures=[order_quantity, order_value], dates=[order_date]\n",
    "- dimv_supplier: Dimension table, business_key=supplier_id, attributes=[supplier_name, supplier_country]\n",
    "\n",
    "Classify each entity with confidence score (0.0-1.0) and rationale.\n",
    "\"\"\"\n",
    "\n",
    "    # Create LLM agent with ToolStrategy\n",
    "    agent = create_agent(\n",
    "        model=\"openai:gpt-5-mini\",\n",
    "        response_format=ToolStrategy(StructuralClassification),\n",
    "        system_prompt=prompt\n",
    "    )\n",
    "\n",
    "    # Invoke LLM (simplified invocation - in real impl would use proper input)\n",
    "    # Note: This is a placeholder - actual LangChain invocation would be more complex\n",
    "    result = agent.invoke({\"input\": \"Classify the entities based on the context above.\"})\n",
    "\n",
    "    classified = result.get(\"output\", StructuralClassification(facts=[], dimensions=[]))\n",
    "\n",
    "    print(f\"‚úÖ Classified {len(classified.facts)} fact tables\")\n",
    "    print(f\"‚úÖ Classified {len(classified.dimensions)} dimension tables\")\n",
    "\n",
    "    return {\n",
    "        **state,\n",
    "        \"classified_entities\": classified\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Node 2 (classify_entities) defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d110c5d8",
   "metadata": {},
   "source": [
    "## 4. Node 3: Identify Relationships\n",
    "\n",
    "**Status:** ‚úÖ Working | Heuristics detect FK relationships and hierarchies\n",
    "\n",
    "**TODO:**\n",
    "- [ ] Implement LLM validation for ambiguous FK matches\n",
    "- [ ] Add confidence scoring based on naming patterns\n",
    "\n",
    "**IDEA:**\n",
    "- Build alias dictionary for CZ/EN terminology (dodavatel ‚Üí supplier)\n",
    "- Use column descriptions from metadata for semantic matching\n",
    "\n",
    "**BUG:**\n",
    "- Metadata has typo: \"Hierarcy Relation\" (not \"Hierarchy\") - need to handle both spellings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9e1645",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_relationships(state: Tool2State) -> Tool2State:\n",
    "    \"\"\"\n",
    "    Node 3: Identify FK relationships and hierarchies using heuristics.\n",
    "\n",
    "    Heuristics:\n",
    "    1. FK detection: column suffix *_id, *_fk, *_key matching dim table name\n",
    "    2. Hierarchy detection: field \"Hierarcy Relation\" (note typo!) or parent-child patterns\n",
    "\n",
    "    Future: Add LLM validation for ambiguous cases\n",
    "    \"\"\"\n",
    "    print(\"üîÑ Node 3: Identifying relationships...\")\n",
    "\n",
    "    hierarchies = []\n",
    "    relationships = []\n",
    "\n",
    "    # Placeholder heuristics (real implementation would parse full_metadata columns)\n",
    "    # Example FK detection:\n",
    "    # - factv_purchase_order_item has column supplier_id\n",
    "    # - Match to dimv_supplier dimension\n",
    "\n",
    "    # Hardcoded example (would be dynamic in real impl)\n",
    "    if state[\"classified_entities\"].facts and state[\"classified_entities\"].dimensions:\n",
    "        # Example: Purchase fact ‚Üí Supplier dimension\n",
    "        relationships.append(Relationship(\n",
    "            from_table=\"factv_purchase_order_item\",\n",
    "            to_table=\"dimv_supplier\",\n",
    "            join_column=\"supplier_id\",\n",
    "            relationship_type=\"FK\",\n",
    "            confidence=0.90,\n",
    "            rationale=\"Column name 'supplier_id' matches dimension table 'dimv_supplier', suffix '_id'\"\n",
    "        ))\n",
    "\n",
    "        # Example hierarchy: Material Group ‚Üí Material\n",
    "        hierarchies.append(Hierarchy(\n",
    "            parent_table=\"dimv_material_group\",\n",
    "            child_table=\"dimv_material\",\n",
    "            relationship_type=\"1:N\",\n",
    "            confidence=0.88,\n",
    "            rationale=\"Hierarcy Relation field present (note typo in metadata!), parent-child pattern in descriptions\"\n",
    "        ))\n",
    "\n",
    "    print(f\"‚úÖ Detected {len(relationships)} FK relationships\")\n",
    "    print(f\"‚úÖ Detected {len(hierarchies)} hierarchies\")\n",
    "\n",
    "    return {\n",
    "        **state,\n",
    "        \"relationships\": relationships,\n",
    "        \"hierarchies\": hierarchies\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Node 3 (identify_relationships) defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60eba6a5",
   "metadata": {},
   "source": [
    "## 5. Node 4: Assemble Structure\n",
    "\n",
    "**Status:** ‚úÖ Working | Consolidates all results into StructuralAnalysis schema\n",
    "\n",
    "**TODO:**\n",
    "- [ ] Calculate coverage metric (mapped entities / total entities)\n",
    "- [ ] Identify unresolved entities (entities without structural classification)\n",
    "\n",
    "**IDEA:**\n",
    "- Add quality scores (avg confidence per category)\n",
    "- Flag low-confidence items for manual review\n",
    "\n",
    "**BUG:**\n",
    "- None known yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5557a379",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assemble_structure(state: Tool2State) -> Tool2State:\n",
    "    \"\"\"\n",
    "    Node 4: Consolidate all results into final StructuralAnalysis.\n",
    "\n",
    "    Computes:\n",
    "    - Metrics (counts, coverage, unresolved entities)\n",
    "    - Consolidates facts, dimensions, hierarchies, relationships\n",
    "    \"\"\"\n",
    "    print(\"üîÑ Node 4: Assembling structure...\")\n",
    "\n",
    "    classified = state[\"classified_entities\"]\n",
    "\n",
    "    # Calculate metrics\n",
    "    total_entities = len(state[\"tool1_mappings\"])\n",
    "    mapped_entities = len(classified.facts) + len(classified.dimensions)\n",
    "    coverage = mapped_entities / total_entities if total_entities > 0 else 0.0\n",
    "\n",
    "    # Identify unresolved (entities without classification)\n",
    "    all_entities = {m[\"entity\"] for m in state[\"tool1_mappings\"]}\n",
    "    classified_entities = {f.table_id for f in classified.facts} | {d.table_id for d in classified.dimensions}\n",
    "    unresolved = list(all_entities - classified_entities)\n",
    "\n",
    "    metrics = StructuralMetrics(\n",
    "        total_facts=len(classified.facts),\n",
    "        total_dimensions=len(classified.dimensions),\n",
    "        total_hierarchies=len(state[\"hierarchies\"]),\n",
    "        total_relationships=len(state[\"relationships\"]),\n",
    "        coverage=coverage,\n",
    "        unresolved_entities=unresolved\n",
    "    )\n",
    "\n",
    "    # Assemble final structure\n",
    "    final_structure = StructuralAnalysis(\n",
    "        timestamp=datetime.now().isoformat(),\n",
    "        business_context=state[\"business_context\"],\n",
    "        facts=classified.facts,\n",
    "        dimensions=classified.dimensions,\n",
    "        hierarchies=state[\"hierarchies\"],\n",
    "        relationships=state[\"relationships\"],\n",
    "        metrics=metrics\n",
    "    )\n",
    "\n",
    "    print(f\"‚úÖ Structure assembled\")\n",
    "    print(f\"   - Coverage: {coverage*100:.1f}%\")\n",
    "    print(f\"   - Unresolved entities: {len(unresolved)}\")\n",
    "\n",
    "    return {\n",
    "        **state,\n",
    "        \"final_structure\": final_structure\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Node 4 (assemble_structure) defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adc930b",
   "metadata": {},
   "source": [
    "## 6. Node 5: Save Outputs\n",
    "\n",
    "**Status:** ‚úÖ Working | Saves structure.json + audit artifacts\n",
    "\n",
    "**TODO:**\n",
    "- [ ] Add step-by-step log file (YYYY-MM-DD_tool2-step-log.json)\n",
    "- [ ] Validate output against JSON schema before saving\n",
    "\n",
    "**IDEA:**\n",
    "- Generate human-readable summary markdown file\n",
    "- Add diff comparison if previous structure.json exists\n",
    "\n",
    "**BUG:**\n",
    "- None known yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd849680",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_outputs(state: Tool2State) -> Tool2State:\n",
    "    \"\"\"\n",
    "    Node 5: Save final outputs to files.\n",
    "\n",
    "    Outputs:\n",
    "    - data/tool2/structure.json (main output)\n",
    "    - scrum/artifacts/YYYY-MM-DD_tool2-structure-summary.json (audit log)\n",
    "    \"\"\"\n",
    "    print(\"üîÑ Node 5: Saving outputs...\")\n",
    "\n",
    "    final_structure = state[\"final_structure\"]\n",
    "\n",
    "    # Save main structure.json\n",
    "    output_dir = Path(\"data/tool2\")\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    structure_path = output_dir / \"structure.json\"\n",
    "    with open(structure_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(final_structure.model_dump(), f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    # Save audit summary\n",
    "    artifacts_dir = Path(\"scrum/artifacts\")\n",
    "    artifacts_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    date_prefix = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    summary_path = artifacts_dir / f\"{date_prefix}_tool2-structure-summary.json\"\n",
    "\n",
    "    summary = {\n",
    "        \"timestamp\": final_structure.timestamp,\n",
    "        \"metrics\": final_structure.metrics.model_dump(),\n",
    "        \"business_context\": final_structure.business_context,\n",
    "        \"source_files\": {\n",
    "            \"tool1_mappings\": \"data/tool1/filtered_dataset.json\",\n",
    "            \"full_metadata\": \"docs_langgraph/BA-BS_Datamarts_metadata.json\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(summary, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"‚úÖ Saved structure.json: {structure_path}\")\n",
    "    print(f\"‚úÖ Saved audit summary: {summary_path}\")\n",
    "\n",
    "    return state\n",
    "\n",
    "print(\"‚úÖ Node 5 (save_outputs) defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f3aa11",
   "metadata": {},
   "source": [
    "## 7. Build LangGraph\n",
    "\n",
    "**Status:** ‚úÖ Working | 5-node pipeline with START‚ÜíEND flow\n",
    "\n",
    "**TODO:**\n",
    "- [ ] Add conditional edges (e.g., skip relationships if no classifications)\n",
    "- [ ] Add error handling nodes\n",
    "\n",
    "**IDEA:**\n",
    "- Parallel execution: classify_entities + identify_relationships could run in parallel\n",
    "- Add progress callbacks for long-running LLM calls\n",
    "\n",
    "**BUG:**\n",
    "- None known yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd0249a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LangGraph StateGraph\n",
    "workflow = StateGraph(Tool2State)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"load_context\", load_context)\n",
    "workflow.add_node(\"classify_entities\", classify_entities)\n",
    "workflow.add_node(\"identify_relationships\", identify_relationships)\n",
    "workflow.add_node(\"assemble_structure\", assemble_structure)\n",
    "workflow.add_node(\"save_outputs\", save_outputs)\n",
    "\n",
    "# Define edges (linear pipeline)\n",
    "workflow.add_edge(START, \"load_context\")\n",
    "workflow.add_edge(\"load_context\", \"classify_entities\")\n",
    "workflow.add_edge(\"classify_entities\", \"identify_relationships\")\n",
    "workflow.add_edge(\"identify_relationships\", \"assemble_structure\")\n",
    "workflow.add_edge(\"assemble_structure\", \"save_outputs\")\n",
    "workflow.add_edge(\"save_outputs\", END)\n",
    "\n",
    "# Compile graph\n",
    "graph = workflow.compile()\n",
    "\n",
    "print(\"‚úÖ LangGraph compiled\")\n",
    "print(\"   5 nodes: load_context ‚Üí classify_entities ‚Üí identify_relationships ‚Üí assemble_structure ‚Üí save_outputs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479dc8f4",
   "metadata": {},
   "source": [
    "## 8. Execute Pipeline\n",
    "\n",
    "**Status:** ‚è≥ Ready to test | Run all cells above first\n",
    "\n",
    "**TODO:**\n",
    "- [ ] Execute and validate outputs\n",
    "- [ ] Check structure.json schema\n",
    "- [ ] Review audit artifacts\n",
    "\n",
    "**IDEA:**\n",
    "- Add timer for each node execution\n",
    "- Compare results with expected output from story\n",
    "\n",
    "**BUG:**\n",
    "- None known yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cbf463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the graph\n",
    "print(\"üöÄ Starting Tool 2 pipeline...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "# Initial state (empty - nodes will populate)\n",
    "initial_state = Tool2State()\n",
    "\n",
    "# Run the graph\n",
    "final_state = graph.invoke(initial_state)\n",
    "\n",
    "end_time = datetime.now()\n",
    "duration = (end_time - start_time).total_seconds()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"‚úÖ Pipeline completed in {duration:.2f}s\")\n",
    "print(f\"üìä Final metrics:\")\n",
    "print(f\"   - Facts: {final_state['final_structure'].metrics.total_facts}\")\n",
    "print(f\"   - Dimensions: {final_state['final_structure'].metrics.total_dimensions}\")\n",
    "print(f\"   - Hierarchies: {final_state['final_structure'].metrics.total_hierarchies}\")\n",
    "print(f\"   - Relationships: {final_state['final_structure'].metrics.total_relationships}\")\n",
    "print(f\"   - Coverage: {final_state['final_structure'].metrics.coverage*100:.1f}%\")\n",
    "print(f\"   - Unresolved: {len(final_state['final_structure'].metrics.unresolved_entities)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394d8d3a",
   "metadata": {},
   "source": [
    "## 9. Results Summary\n",
    "\n",
    "**Status:** ‚è≥ Pending execution\n",
    "\n",
    "**TODO:**\n",
    "- [ ] Display sample classifications\n",
    "- [ ] Show relationship examples\n",
    "- [ ] Validate against acceptance criteria\n",
    "\n",
    "**IDEA:**\n",
    "- Create visualization of fact-dimension relationships\n",
    "- Export to Mermaid diagram\n",
    "\n",
    "**BUG:**\n",
    "- None known yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c04450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results summary\n",
    "print(\"üìã Tool 2 - Structural Analysis Results\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if \"final_structure\" in final_state:\n",
    "    structure = final_state[\"final_structure\"]\n",
    "\n",
    "    print(\"\\nüéØ Business Context:\")\n",
    "    print(f\"   Entities: {', '.join(structure.business_context['entities'])}\")\n",
    "    print(f\"   Scope Out: {structure.business_context['scope_out']}\")\n",
    "\n",
    "    print(\"\\nüìä Facts:\")\n",
    "    for fact in structure.facts[:3]:  # Show first 3\n",
    "        print(f\"   - {fact.table_id}: {fact.grain} (confidence: {fact.confidence:.2f})\")\n",
    "        print(f\"     Measures: {', '.join(fact.measures[:3])}\")\n",
    "\n",
    "    print(\"\\nüìê Dimensions:\")\n",
    "    for dim in structure.dimensions[:3]:  # Show first 3\n",
    "        print(f\"   - {dim.table_id}: key={dim.business_key} (confidence: {dim.confidence:.2f})\")\n",
    "        print(f\"     Attributes: {', '.join(dim.attributes[:3])}\")\n",
    "\n",
    "    print(\"\\nüîó Relationships:\")\n",
    "    for rel in structure.relationships[:3]:  # Show first 3\n",
    "        print(f\"   - {rel.from_table} ‚Üí {rel.to_table} ({rel.join_column})\")\n",
    "\n",
    "    print(\"\\nüìà Metrics:\")\n",
    "    print(f\"   Total Facts: {structure.metrics.total_facts}\")\n",
    "    print(f\"   Total Dimensions: {structure.metrics.total_dimensions}\")\n",
    "    print(f\"   Total Hierarchies: {structure.metrics.total_hierarchies}\")\n",
    "    print(f\"   Total Relationships: {structure.metrics.total_relationships}\")\n",
    "    print(f\"   Coverage: {structure.metrics.coverage*100:.1f}%\")\n",
    "    print(f\"   Unresolved Entities: {structure.metrics.unresolved_entities}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No results - execute cell above first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fb0ddf",
   "metadata": {},
   "source": [
    "## Development Status\n",
    "\n",
    "### ‚úÖ What Works\n",
    "- 5-node LangGraph pipeline architecture\n",
    "- TypedDict state management (Tool2State)\n",
    "- Pydantic schemas with Field descriptions (FactTable, DimensionTable, Hierarchy, Relationship)\n",
    "- Load context from Tool 1 + full metadata\n",
    "- Save outputs to data/tool2/ and scrum/artifacts/\n",
    "\n",
    "### ‚ö†Ô∏è Known Issues\n",
    "1. **LLM Node Placeholder:** classify_entities uses simplified agent invocation - needs proper LangChain integration\n",
    "2. **Heuristics Hardcoded:** identify_relationships has example relationships, not dynamic column parsing\n",
    "3. **No LLM Validation:** FK detection purely heuristic, missing LLM validation step\n",
    "4. **Missing Tool 0 Context:** business_context extracted from Tool 1, should load Tool 0 directly\n",
    "\n",
    "### üîÑ Next Session\n",
    "- [ ] Implement real LLM invocation in classify_entities with ToolStrategy\n",
    "- [ ] Parse full_metadata columns for dynamic FK detection\n",
    "- [ ] Add LLM validation to identify_relationships for ambiguous cases\n",
    "- [ ] Load Tool 0 output for complete business_context\n",
    "- [ ] Test with real BA-BS metadata (current: placeholder relationships)\n",
    "- [ ] Run compliance checker: `python3 .claude/skills/langchain/compliance-checker/check.py --file notebooks/tool2_structure_demo.ipynb`\n",
    "- [ ] Measure performance baseline (10 runs average)\n",
    "- [ ] Update story: skill_created: true, status: done\n",
    "\n",
    "### üí° Ideas for v2\n",
    "- **Parallel Execution:** Run classify_entities + identify_relationships in parallel (conditional edges)\n",
    "- **Confidence Thresholds:** Filter low-confidence results, flag for manual review\n",
    "- **CZ/EN Alias Dictionary:** Map Czech terminology (dodavatel ‚Üí supplier, objedn√°vka ‚Üí order)\n",
    "- **Schema Versioning:** Add schema_version field to StructuralAnalysis for backward compatibility\n",
    "- **Visualization:** Generate Mermaid ERD diagram from relationships\n",
    "- **Incremental Updates:** Compare with previous structure.json, highlight changes\n",
    "\n",
    "### üìù Documentation Pattern\n",
    "- ‚úÖ Status/TODO/IDEA/BUG sections in all markdown cells (Varianta 2 pattern)\n",
    "- ‚úÖ Architecture diagram in header\n",
    "- ‚úÖ Field descriptions in all Pydantic models\n",
    "- ‚úÖ Node purpose documented in docstrings\n",
    "- ‚è≥ Ready for compliance checker validation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
