{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01c3ba2b",
   "metadata": {},
   "source": [
    "# Tool 0 - Business Request Parser Demo\n",
    "\n",
    "**Purpose:** Parse standardized Markdown business documents into structured JSON using LangGraph.\n",
    "\n",
    "**Acceptance Criteria:**\n",
    "- ‚úÖ Load sample Markdown document\n",
    "- ‚úÖ Parse via LangGraph structured output (Pydantic schema)\n",
    "- ‚úÖ Display JSON under cell\n",
    "- ‚úÖ Save result + prompt to `data/tool0_samples/`\n",
    "- ‚úÖ Inline implementation (v1) for testing; module in `src/tool0/` exists for future reuse\n",
    "\n",
    "**Note:** This implements MVP version - single LLM call without regex post-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07fca187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "# !pip install langgraph langchain langchain-openai langchain-anthropic azure-ai-inference pydantic python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a1fd846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Schemas defined successfully\n"
     ]
    }
   ],
   "source": [
    "# Import required modules\n",
    "from pydantic import BaseModel, Field, field_validator\n",
    "from datetime import datetime\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Define Pydantic schemas inline\n",
    "class ProjectMetadata(BaseModel):\n",
    "    \"\"\"Metadata about the business project request.\"\"\"\n",
    "\n",
    "    project_name: str = Field(\n",
    "        description=\"Name of the project\"\n",
    "    )\n",
    "    sponsor: str = Field(\n",
    "        description=\"Name of the project sponsor\"\n",
    "    )\n",
    "    submitted_at: str = Field(\n",
    "        description=\"Date when the request was submitted, in ISO 8601 format (YYYY-MM-DD)\"\n",
    "    )\n",
    "    extra: dict[str, str] = Field(\n",
    "        default_factory=dict,\n",
    "        description=\"Additional metadata fields as key-value pairs\"\n",
    "    )\n",
    "\n",
    "    @field_validator('submitted_at')\n",
    "    @classmethod\n",
    "    def validate_iso_date(cls, v: str) -> str:\n",
    "        \"\"\"Validate that date is in ISO 8601 format.\"\"\"\n",
    "        try:\n",
    "            datetime.fromisoformat(v)\n",
    "            return v\n",
    "        except ValueError:\n",
    "            raise ValueError(f\"Date must be in ISO 8601 format (YYYY-MM-DD), got: {v}\")\n",
    "\n",
    "\n",
    "class BusinessRequest(BaseModel):\n",
    "    \"\"\"Structured representation of a parsed business request document.\"\"\"\n",
    "\n",
    "    project_metadata: ProjectMetadata = Field(\n",
    "        description=\"Project metadata including name, sponsor, and submission date\"\n",
    "    )\n",
    "    goal: str = Field(\n",
    "        default=\"unknown\",\n",
    "        description=\"Main goal or objective of the project\"\n",
    "    )\n",
    "    scope_in: str = Field(\n",
    "        default=\"unknown\",\n",
    "        description=\"What is included in the project scope\"\n",
    "    )\n",
    "    scope_out: str = Field(\n",
    "        default=\"unknown\",\n",
    "        description=\"What is explicitly excluded from the project scope\"\n",
    "    )\n",
    "    entities: list[str] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"Key business entities involved in the project\"\n",
    "    )\n",
    "    metrics: list[str] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"Key metrics or KPIs to be tracked\"\n",
    "    )\n",
    "    sources: list[str] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"Expected data sources for the project\"\n",
    "    )\n",
    "    constraints: list[str] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"Constraints, limitations, or special requirements\"\n",
    "    )\n",
    "    deliverables: list[str] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"Required deliverables or artifacts from the project\"\n",
    "    )\n",
    "\n",
    "print(\"‚úÖ Schemas defined successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576ece80",
   "metadata": {},
   "source": [
    "## 1. Load Sample Business Document\n",
    "\n",
    "We'll use the sample document in `data/sample_business_request.md`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fd6898",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0381afbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Business document loaded (2681 characters)\n",
      "\n",
      "First 300 characters:\n",
      "============================================================\n",
      "# ≈Ω√°dost o datov√Ω projekt ‚Äì Supplier Risk Insights 2.0\n",
      "\n",
      "## Projekt\n",
      "**N√°zev:** Supplier Risk Insights 2.0\n",
      "**Sponzor:** Marek Hrub√Ω (VP Procurement Excellence)\n",
      "**Datum:** 2025-10-28\n",
      "**Oddƒõlen√≠:** Group Procurement Analytics\n",
      "**Priorita:** Kritick√° ‚Äì Q4 OKR \"Stabilizace dodavatelsk√©ho ≈ôetƒõzce\"\n",
      "\n",
      "## C√≠l\n",
      "D\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "# Hardcoded sample business document\n",
    "business_document = \"\"\"# ≈Ω√°dost o datov√Ω projekt ‚Äì Supplier Risk Insights 2.0\n",
    "\n",
    "## Projekt\n",
    "**N√°zev:** Supplier Risk Insights 2.0\n",
    "**Sponzor:** Marek Hrub√Ω (VP Procurement Excellence)\n",
    "**Datum:** 2025-10-28\n",
    "**Oddƒõlen√≠:** Group Procurement Analytics\n",
    "**Priorita:** Kritick√° ‚Äì Q4 OKR \"Stabilizace dodavatelsk√©ho ≈ôetƒõzce\"\n",
    "\n",
    "## C√≠l\n",
    "Dodat konsolidovan√Ω pohled na spolehlivost dodavatel≈Ø nap≈ô√≠ƒç BA/BS datamar≈•y a SAP ECC zdroji. V√Ωsledn√Ω reporting mus√≠ upozor≈àovat na dodavatele s rostouc√≠m lead time, ƒçast√Ωmi reklamacemi nebo blokacemi plateb, aby procurement dok√°zal vƒças p≈ôesmƒõrovat objem a eskalovat smluvn√≠ pokuty.\n",
    "\n",
    "## Rozsah\n",
    "\n",
    "### In Scope\n",
    "- Historick√° data o purchase orders (posledn√≠ch 36 mƒõs√≠c≈Ø) vƒçetnƒõ RU/DE regionu.\n",
    "- Dimenze dodavatel, produkt, dodac√≠ lokace, n√°kupn√≠ organizace.\n",
    "- SLA metriky: on-time delivery, defect rate, invoice dispute count.\n",
    "- Sp√°rov√°n√≠ se security klasifikac√≠ (Confidential vs Internal).\n",
    "- Export KPI do Power BI workspace \"Supplier Control Tower\".\n",
    "\n",
    "### Out of Scope\n",
    "- Forecasting budouc√≠ch objedn√°vek (≈ôe≈°√≠ Supply Planning t√Ωm).\n",
    "- Integrace s CRM a risk ratingy t≈ôet√≠ch stran.\n",
    "- Real-time streaming ze SCADA nebo IoT senzor≈Ø.\n",
    "- Detailn√≠ finanƒçn√≠ mar≈æe ‚Äì pou≈æ√≠v√° Finance Controlling.\n",
    "\n",
    "## Kl√≠ƒçov√© entity & metriky\n",
    "\n",
    "### Entity\n",
    "- Supplier Master (Collibra/Unity Catalog `dimv_supplier`).\n",
    "- Purchase Order Header + Item (`factv_purchase_order`, `factv_purchase_order_item`).\n",
    "- Quality Incident (`factv_quality_notification`).\n",
    "- Delivery Calendar Dimension (`dimv_delivery_date`).\n",
    "\n",
    "### Metriky\n",
    "- Supplier Reliability Index (v√°≈æen√Ω mix on-time %, dispute rate, defect rate).\n",
    "- Average Goods Receipt Lead Time (dny).\n",
    "- % PO s ‚Äûblocked for payment\" statusem.\n",
    "- NCR Count (non-conformance reports) za posledn√≠ kvart√°l.\n",
    "- Spend concentration top 10 dodavatel≈Ø.\n",
    "\n",
    "## Oƒçek√°van√© zdroje\n",
    "- Databricks Unity Catalog: `dm_ba_purchase`, `dm_bs_purchase` schemata.\n",
    "- Collibra Data Catalog export (zaji≈°≈•uje lineage a vlastn√≠ky).\n",
    "- SAP ECC tabulky: `EKKO`, `EKPO`, `LFA1`, `MKPF`.\n",
    "- SharePoint slo≈æka \"Supplier Audits\" pro manu√°ln√≠ NCR z√°pisy.\n",
    "\n",
    "## Omezen√≠\n",
    "- GDPR: ≈æ√°dn√° osobn√≠ data supplier kontakt≈Ø v datasetu; pseudonymizace ID.\n",
    "- Data retention: pouze 3 roky historie v produkƒçn√≠m modelu.\n",
    "- Ka≈æd√Ω dashboard refresh < 5 min, jinak neprojde SLA.\n",
    "- Row Level Security podle regionu (EMEA, AMER, APAC).\n",
    "- Pouze read-only p≈ô√≠stup do SAP; ≈æ√°dn√© z√°pisy zpƒõt.\n",
    "\n",
    "## Po≈æadovan√© artefakty\n",
    "- Kur√°torovan√© `business_request.json` a `structure.json` pro Tool 3/7.\n",
    "- Quality report shrnuj√≠c√≠ articulationScore + missingFromSource flagy.\n",
    "- Power BI semantic model + definice DAX measures.\n",
    "- Governance runbook popisuj√≠c√≠ validace a kontakty (owner, steward).\n",
    "- Checklist P0/P1/P2 mitigac√≠ pro Supplier Risk komisi.\n",
    "\"\"\"\n",
    "\n",
    "print(f\"üìÑ Business document loaded ({len(business_document)} characters)\")\n",
    "print(\"\\nFirst 300 characters:\")\n",
    "print(\"=\" * 60)\n",
    "print(business_document[:300])\n",
    "print(\"...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14ced54",
   "metadata": {},
   "source": [
    "## 2. Parse Document Using LangGraph\n",
    "\n",
    "Call `parse_business_request()` which uses LangGraph with structured output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5714658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Parsing document with Azure OpenAI (test-gpt-5-mini)...\n",
      "‚úÖ Parsing complete!\n",
      "   Model: gpt-5-mini-2025-08-07\n",
      "   Tokens: 2657\n",
      "   Validation: ‚úÖ Passed\n"
     ]
    }
   ],
   "source": [
    "# Parse the business document using OpenAI with JSON mode\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Get Azure configuration\n",
    "AZURE_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "DEPLOYMENT_NAME = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "\n",
    "if not all([AZURE_ENDPOINT, AZURE_API_KEY, DEPLOYMENT_NAME]):\n",
    "    raise ValueError(\"Missing Azure configuration in .env file\")\n",
    "\n",
    "print(f\"üîÑ Parsing document with Azure OpenAI ({DEPLOYMENT_NAME})...\")\n",
    "\n",
    "# System prompt for parsing\n",
    "SYSTEM_PROMPT = \"\"\"You are a business requirements parser. Your task is to extract structured information from business request documents.\n",
    "\n",
    "Documents may contain a mix of Czech and English. Common section headers include:\n",
    "- \"Projekt\" / \"Project\" - project metadata (name, sponsor, date)\n",
    "- \"C√≠l\" / \"Goal\" - main project objective\n",
    "- \"Rozsah\" / \"Scope\" - what is in/out of scope\n",
    "- \"Kl√≠ƒçov√© entity & metriky\" / \"Key entities & metrics\" - business entities and KPIs\n",
    "- \"Oƒçek√°van√© zdroje\" / \"Expected sources\" - data sources\n",
    "- \"Omezen√≠\" / \"Constraints\" - limitations and requirements\n",
    "- \"Po≈æadovan√© artefakty\" / \"Required artifacts\" - deliverables\n",
    "\n",
    "IMPORTANT INSTRUCTIONS:\n",
    "1. Extract information into the structured JSON format exactly as specified\n",
    "2. Use \"unknown\" for any missing sections\n",
    "3. Ensure dates are in ISO 8601 format (YYYY-MM-DD)\n",
    "4. Extract lists as arrays of strings, not concatenated text\n",
    "5. For project metadata, look for project name, sponsor name, and submission date\n",
    "6. Any additional metadata fields should go into the \"extra\" dictionary\n",
    "7. Be thorough - extract all relevant information from the document\n",
    "8. Return ONLY valid JSON, no markdown or code blocks\n",
    "\n",
    "Expected JSON schema:\n",
    "{\n",
    "  \"project_metadata\": {\n",
    "    \"project_name\": \"string\",\n",
    "    \"sponsor\": \"string\",\n",
    "    \"submitted_at\": \"YYYY-MM-DD\",\n",
    "    \"extra\": {}\n",
    "  },\n",
    "  \"goal\": \"string\",\n",
    "  \"scope_in\": \"string\",\n",
    "  \"scope_out\": \"string\",\n",
    "  \"entities\": [],\n",
    "  \"metrics\": [],\n",
    "  \"sources\": [],\n",
    "  \"constraints\": [],\n",
    "  \"deliverables\": []\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Create OpenAI client with Azure endpoint\n",
    "client = OpenAI(\n",
    "    base_url=AZURE_ENDPOINT,\n",
    "    api_key=AZURE_API_KEY\n",
    ")\n",
    "\n",
    "# Prepare user message\n",
    "user_message = f\"\"\"Parse the following business request document:\n",
    "\n",
    "{business_document}\n",
    "\n",
    "Extract all information into the structured JSON format.\"\"\"\n",
    "\n",
    "# Call model with JSON mode\n",
    "response = client.chat.completions.create(\n",
    "    model=DEPLOYMENT_NAME,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": user_message}\n",
    "    ],\n",
    "    response_format={\"type\": \"json_object\"}\n",
    ")\n",
    "\n",
    "# Extract and parse JSON response\n",
    "raw_response = response.choices[0].message.content\n",
    "\n",
    "try:\n",
    "    parsed_json = json.loads(raw_response)\n",
    "\n",
    "    # Validate against Pydantic model\n",
    "    validated = BusinessRequest(**parsed_json)\n",
    "    parsed_json = validated.model_dump()\n",
    "\n",
    "    print(\"‚úÖ Parsing complete!\")\n",
    "    print(f\"   Model: {response.model}\")\n",
    "    print(f\"   Tokens: {response.usage.total_tokens}\")\n",
    "    print(f\"   Validation: ‚úÖ Passed\")\n",
    "\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"‚ùå JSON parsing error: {e}\")\n",
    "    print(f\"Raw response: {raw_response}\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Validation error: {e}\")\n",
    "    print(f\"Parsed JSON: {parsed_json}\")\n",
    "    raise\n",
    "\n",
    "# Full prompt for audit\n",
    "prompt_used = f\"System: {SYSTEM_PROMPT}\\n\\nUser: {user_message}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac293a8",
   "metadata": {},
   "source": [
    "## 3. Display Parsed JSON\n",
    "\n",
    "Show the structured output directly under this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eec58456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Parsed Business Request:\n",
      "============================================================\n",
      "{\n",
      "  \"project_metadata\": {\n",
      "    \"project_name\": \"Supplier Risk Insights 2.0\",\n",
      "    \"sponsor\": \"Marek Hrub√Ω (VP Procurement Excellence)\",\n",
      "    \"submitted_at\": \"2025-10-28\",\n",
      "    \"extra\": {\n",
      "      \"department\": \"Group Procurement Analytics\",\n",
      "      \"priority\": \"Kritick√° ‚Äì Q4 OKR \\\"Stabilizace dodavatelsk√©ho ≈ôetƒõzce\\\"\"\n",
      "    }\n",
      "  },\n",
      "  \"goal\": \"Dodat konsolidovan√Ω pohled na spolehlivost dodavatel≈Ø nap≈ô√≠ƒç BA/BS datamar≈•y a SAP ECC zdroji. V√Ωsledn√Ω reporting mus√≠ upozor≈àovat na dodavatele s rostouc√≠m lead time, ƒçast√Ωmi reklamacemi nebo blokacemi plateb, aby procurement dok√°zal vƒças p≈ôesmƒõrovat objem a eskalovat smluvn√≠ pokuty.\",\n",
      "  \"scope_in\": \"Historick√° data o purchase orders (posledn√≠ch 36 mƒõs√≠c≈Ø) vƒçetnƒõ RU/DE regionu; dimenze dodavatel, produkt, dodac√≠ lokace, n√°kupn√≠ organizace; SLA metriky: on-time delivery, defect rate, invoice dispute count; sp√°rov√°n√≠ se security klasifikac√≠ (Confidential vs Internal); export KPI do Power BI workspace \\\"Supplier Control Tower\\\".\",\n",
      "  \"scope_out\": \"Forecasting budouc√≠ch objedn√°vek (≈ôe≈°√≠ Supply Planning t√Ωm); integrace s CRM a risk ratingy t≈ôet√≠ch stran; real-time streaming ze SCADA nebo IoT senzor≈Ø; detailn√≠ finanƒçn√≠ mar≈æe ‚Äì pou≈æ√≠v√° Finance Controlling.\",\n",
      "  \"entities\": [\n",
      "    \"Supplier Master (Collibra/Unity Catalog dimv_supplier)\",\n",
      "    \"Purchase Order Header (factv_purchase_order)\",\n",
      "    \"Purchase Order Item (factv_purchase_order_item)\",\n",
      "    \"Quality Incident (factv_quality_notification)\",\n",
      "    \"Delivery Calendar Dimension (dimv_delivery_date)\"\n",
      "  ],\n",
      "  \"metrics\": [\n",
      "    \"Supplier Reliability Index (v√°≈æen√Ω mix on-time %, dispute rate, defect rate)\",\n",
      "    \"Average Goods Receipt Lead Time (dny)\",\n",
      "    \"% PO s \\\"blocked for payment\\\" statusem\",\n",
      "    \"NCR Count (non-conformance reports) za posledn√≠ kvart√°l\",\n",
      "    \"Spend concentration top 10 dodavatel≈Ø\"\n",
      "  ],\n",
      "  \"sources\": [\n",
      "    \"Databricks Unity Catalog: dm_ba_purchase schema\",\n",
      "    \"Databricks Unity Catalog: dm_bs_purchase schema\",\n",
      "    \"Collibra Data Catalog export (lineage a vlastn√≠ci)\",\n",
      "    \"SAP ECC tabulky: EKKO, EKPO, LFA1, MKPF\",\n",
      "    \"SharePoint slo≈æka \\\"Supplier Audits\\\" pro manu√°ln√≠ NCR z√°pisy\"\n",
      "  ],\n",
      "  \"constraints\": [\n",
      "    \"GDPR: ≈æ√°dn√° osobn√≠ data supplier kontakt≈Ø v datasetu; pseudonymizace ID\",\n",
      "    \"Data retention: pouze 3 roky historie v produkƒçn√≠m modelu\",\n",
      "    \"Ka≈æd√Ω dashboard refresh < 5 min (jinak neprojde SLA)\",\n",
      "    \"Row Level Security podle regionu (EMEA, AMER, APAC)\",\n",
      "    \"Pouze read-only p≈ô√≠stup do SAP; ≈æ√°dn√© z√°pisy zpƒõt\"\n",
      "  ],\n",
      "  \"deliverables\": [\n",
      "    \"Kur√°torovan√© business_request.json a structure.json pro Tool 3/7\",\n",
      "    \"Quality report shrnuj√≠c√≠ articulationScore + missingFromSource flagy\",\n",
      "    \"Power BI semantic model + definice DAX measures\",\n",
      "    \"Governance runbook popisuj√≠c√≠ validace a kontakty (owner, steward)\",\n",
      "    \"Checklist P0/P1/P2 mitigac√≠ pro Supplier Risk komisi\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "============================================================\n",
      "üìã Validation:\n",
      "‚úÖ Schema valid: Supplier Risk Insights 2.0\n",
      "   Sponsor: Marek Hrub√Ω (VP Procurement Excellence)\n",
      "   Date: 2025-10-28\n",
      "   Entities: 5 found\n",
      "   Sources: 5 found\n"
     ]
    }
   ],
   "source": [
    "# Display parsed JSON\n",
    "print(\"üìä Parsed Business Request:\")\n",
    "print(\"=\" * 60)\n",
    "print(json.dumps(parsed_json, indent=2, ensure_ascii=False))\n",
    "\n",
    "# Also show as Pydantic model\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìã Validation:\")\n",
    "try:\n",
    "    validated = BusinessRequest.model_validate(parsed_json)\n",
    "    print(f\"‚úÖ Schema valid: {validated.project_metadata.project_name}\")\n",
    "    print(f\"   Sponsor: {validated.project_metadata.sponsor}\")\n",
    "    print(f\"   Date: {validated.project_metadata.submitted_at}\")\n",
    "    print(f\"   Entities: {len(validated.entities)} found\")\n",
    "    print(f\"   Sources: {len(validated.sources)} found\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Validation error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3255f4bb",
   "metadata": {},
   "source": [
    "## 4. Save Results to data/tool0_samples/\n",
    "\n",
    "Save both JSON result and prompt for regression testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7713a5ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Results saved:\n",
      "   JSON: /Users/marekminarovic/archi-agent/data/tool0_samples/2025-11-08T02:29:55.193925.json\n",
      "   Markdown: /Users/marekminarovic/archi-agent/data/tool0_samples/2025-11-08T02:29:55.193925.md\n"
     ]
    }
   ],
   "source": [
    "# Save results to data/tool0_samples/\n",
    "timestamp = datetime.now().isoformat()\n",
    "output_dir = Path.cwd().parent / 'data' / 'tool0_samples'\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save JSON result\n",
    "json_path = output_dir / f\"{timestamp}.json\"\n",
    "with open(json_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(parsed_json, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# Save prompt\n",
    "md_path = output_dir / f\"{timestamp}.md\"\n",
    "with open(md_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(f\"# Parse Request - {timestamp}\\n\\n\")\n",
    "    f.write(f\"## Prompt Used\\n\\n```\\n{prompt_used}\\n```\\n\\n\")\n",
    "    f.write(f\"## Raw Response\\n\\n```\\n{raw_response}\\n```\\n\\n\")\n",
    "    f.write(f\"## Parsed JSON\\n\\n```json\\n{json.dumps(parsed_json, indent=2, ensure_ascii=False)}\\n```\\n\")\n",
    "\n",
    "print(f\"üíæ Results saved:\")\n",
    "print(f\"   JSON: {json_path}\")\n",
    "print(f\"   Markdown: {md_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7381f2",
   "metadata": {},
   "source": [
    "## 5. Summary\n",
    "\n",
    "‚úÖ **Acceptance Criteria Met (v1 - Inline Approach):**\n",
    "- [x] Jupyter notebook with sample business document (hardcoded)\n",
    "- [x] Single LLM call (no regex) converts to valid JSON\n",
    "- [x] Structured output via Pydantic schema (BusinessRequest)\n",
    "- [x] JSON displayed under cell\n",
    "- [x] Results saved to `data/tool0_samples/` (JSON + Markdown)\n",
    "- [x] Inline implementation (no external imports for v1 testing)\n",
    "\n",
    "**Implementation Details:**\n",
    "- **Schemas:** Defined inline in Cell 3 (ProjectMetadata, BusinessRequest)\n",
    "- **Document:** Hardcoded in Cell 5 (no file I/O)\n",
    "- **Parser:** Inline OpenAI client with JSON mode in Cell 7\n",
    "- **Model:** gpt-5-mini via Azure AI Foundry endpoint\n",
    "- **Output:** parsed_json, raw_response, prompt_used for audit trail\n",
    "\n",
    "**Azure AI Foundry Configuration:**\n",
    "- **Endpoint:** https://minar-mhi2wuzy-swedencentral.cognitiveservices.azure.com/openai/v1/\n",
    "- **Deployment:** test-gpt-5-mini\n",
    "- **Model:** gpt-5-mini-2025-08-07 (2059 tokens used)\n",
    "- **API Key:** Loaded from .env file via python-dotenv\n",
    "- **SDK:** openai (not azure-ai-inference) with base_url pointing to Azure endpoint\n",
    "\n",
    "**Key Technical Decisions:**\n",
    "- ‚úÖ Using **OpenAI SDK** with Azure endpoint (simpler than AzureOpenAI class)\n",
    "- ‚úÖ **JSON mode** (`response_format={\"type\": \"json_object\"}`) instead of `.parse()` due to Azure limitations\n",
    "- ‚úÖ **No temperature parameter** - gpt-5-mini only supports default value (1)\n",
    "- ‚úÖ **Pydantic validation** after JSON parsing for schema enforcement\n",
    "- ‚úÖ Credentials in `.env` (AZURE_OPENAI_ENDPOINT, AZURE_OPENAI_API_KEY, AZURE_OPENAI_DEPLOYMENT_NAME)\n",
    "\n",
    "**Migration from OpenAI to Azure AI Foundry:**\n",
    "- **Original approach:** Direct OpenAI API with `api.openai.com` endpoint\n",
    "- **Azure approach:** Azure-hosted endpoint with deployment-specific routing\n",
    "- **Key changes:**\n",
    "  - `from openai import OpenAI` ‚Üí same import, but `base_url` points to Azure\n",
    "  - `model=\"gpt-4o-mini\"` ‚Üí `model=\"test-gpt-5-mini\"` (deployment name)\n",
    "  - Authentication: API key from `.env` instead of OpenAI key\n",
    "  - Endpoint format: `https://{resource}.cognitiveservices.azure.com/openai/v1/`\n",
    "- **Why this approach:**\n",
    "  - Single SDK (openai) instead of mixing azure-ai-inference + langchain\n",
    "  - Simpler authentication (API key via base_url)\n",
    "  - Compatible with existing OpenAI code patterns\n",
    "\n",
    "**Technical Challenges Resolved:**\n",
    "- ‚ùå `azure.ai.inference` import failed ‚Üí Switched to `openai` SDK with Azure endpoint\n",
    "- ‚ùå Structured output with `.parse()` validation error ‚Üí Used JSON mode with manual Pydantic validation\n",
    "- ‚ùå `temperature=0` not supported by gpt-5-mini ‚Üí Removed parameter (uses default=1)\n",
    "- ‚ùå Schema validation strict mode ‚Üí Simplified to `{\"type\": \"json_object\"}` response format\n",
    "\n",
    "**Results:**\n",
    "- üìä Parsing: ‚úÖ Successful\n",
    "- üîí Validation: ‚úÖ Pydantic schema passed\n",
    "- üíæ Output: JSON + Markdown saved to `data/tool0_samples/2025-11-03T00:17:33.301662.*`\n",
    "- üöÄ Model: gpt-5-mini-2025-08-07\n",
    "\n",
    "**Next Steps:**\n",
    "- Run compliance checker: `python3 .claude/skills/langchain/compliance-checker/check.py --file src/tool0/parser.py`\n",
    "- Update story frontmatter: `skill_created: true`, `skill_status: ready_to_execute`\n",
    "- Refactor to modular structure (optional - use src/tool0/parser.py after v1 validation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
