{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59dde579",
   "metadata": {},
   "source": [
    "# Tool 2 - Structural Classifier (Async Pattern)\n",
    "\n",
    "**Status:** ‚úÖ Ready for Databricks | **LLM Cost:** ~$0.002 per run | **Performance:** ~10-15s\n",
    "\n",
    "**Pattern:** Single async function with Pydantic AI classifier agent\n",
    "\n",
    "**Showcase:** LLM-based data warehouse classification (FACT vs DIMENSION) + heuristic relationship detection.\n",
    "\n",
    "**Key Features:**\n",
    "- Single async `classify_structure()` function\n",
    "- LLM-based FACT/DIMENSION classification with grain detection (transaction/event/snapshot/aggregate)\n",
    "- Heuristic FK detection (column suffix matching: `product_id` ‚Üí `products`)\n",
    "- Size estimation (small/medium/large/huge) and SCD Type 2 detection\n",
    "- Expected performance: ~10-15s per classification\n",
    "\n",
    "**TODO:**\n",
    "- [ ] Validate FK detection accuracy (compare with actual schema metadata)\n",
    "- [ ] Add support for bridge tables (many-to-many relationships)\n",
    "- [ ] Test with 50+ table schemas (current test: 10-20)\n",
    "- [ ] Add confidence scores for classifications\n",
    "\n",
    "**IDEA:**\n",
    "- Use actual column metadata (names, types, nullability) instead of table-level heuristics\n",
    "- Add ML-based FK detection (column name similarity + cardinality analysis)\n",
    "- Support custom grain definitions (not just 4 predefined types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758e04e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "%pip install pydantic-ai>=0.0.49 pydantic>=2.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d68295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart Python kernel to use new packages\n",
    "dbutils.library.restartPython()  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b707cf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pydantic import BaseModel, Field\n",
    "from pydantic_ai import Agent  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bce4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Azure OpenAI from Databricks secrets\n",
    "AZURE_ENDPOINT = dbutils.secrets.get(scope=\"mcop\", key=\"azure-openai-endpoint\").strip()  # type: ignore\n",
    "AZURE_API_KEY = dbutils.secrets.get(scope=\"mcop\", key=\"azure-openai-api-key\").strip()  # type: ignore\n",
    "DEPLOYMENT_NAME = dbutils.secrets.get(scope=\"mcop\", key=\"azure-openai-deployment-name\").strip()  # type: ignore\n",
    "\n",
    "# Clean endpoint (remove /openai/v1/ if present - Pydantic AI will handle routing)\n",
    "azure_endpoint_clean = AZURE_ENDPOINT.replace(\"/openai/v1/\", \"\").replace(\"/openai/v1\", \"\").rstrip(\"/\")\n",
    "\n",
    "# Set environment variables for Pydantic AI (Azure OpenAI compatible)\n",
    "os.environ[\"OPENAI_BASE_URL\"] = f\"{azure_endpoint_clean}/openai/deployments/{DEPLOYMENT_NAME}\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = AZURE_API_KEY\n",
    "\n",
    "MODEL_NAME = f\"openai:{DEPLOYMENT_NAME}\"\n",
    "print(f\"‚úÖ Configured model: {MODEL_NAME}\")\n",
    "print(f\"   Base URL: {os.environ['OPENAI_BASE_URL']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e99044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# PYDANTIC SCHEMAS - FACT/DIMENSION klasifikace\n",
    "# ==========================================\n",
    "# Tool 2 √∫ƒçel: Klasifikovat entity z Tool 0 jako FACT nebo DIMENSION tabulky\n",
    "# podle data warehouse koncept≈Ø (Kimball dimensional modeling)\n",
    "\n",
    "# ==========================================\n",
    "# SCHEMA 1: FactTable (transakƒçn√≠/event data)\n",
    "# ==========================================\n",
    "# Co je FACT table:\n",
    "# - Obsahuje mƒõ≈ôiteln√° data (metrics, measures)\n",
    "# - Vysok√Ω poƒçet ≈ô√°dk≈Ø (transactional scale)\n",
    "# - ƒåasovƒõ z√°visl√° data (daily/hourly updates)\n",
    "# - Ciz√≠ kl√≠ƒçe na dimension tables\n",
    "#\n",
    "# P≈ô√≠klady:\n",
    "# - orders (objedn√°vky: amount, quantity, date)\n",
    "# - clickstreams (web events: page_id, user_id, timestamp)\n",
    "# - sensor_readings (IoT: temperature, pressure, device_id)\n",
    "class FactTable(BaseModel):\n",
    "    \"\"\"Fact table (transakƒçn√≠/event data).\n",
    "\n",
    "    Pydantic pou≈æit√≠:\n",
    "    - name: N√°zev tabulky (str required)\n",
    "    - entity_id: ID z Tool 1 mapping (propojen√≠ s business po≈æadavkem)\n",
    "    - grain: Granularita dat (transaction/event/snapshot/aggregate)\n",
    "      * transaction: jednotliv√© transakce (ka≈æd√Ω ≈ô√°dek = 1 objedn√°vka)\n",
    "      * event: ud√°losti (ka≈æd√Ω ≈ô√°dek = 1 kliknut√≠)\n",
    "      * snapshot: sn√≠mky stavu (ka≈æd√Ω den = 1 ≈ô√°dek per customer)\n",
    "      * aggregate: agregace (ka≈æd√Ω mƒõs√≠c = 1 ≈ô√°dek per product)\n",
    "    - estimated_row_count: Velikostn√≠ odhad (small <1M, medium 1-10M, large 10-100M, huge >100M)\n",
    "    \"\"\"\n",
    "    name: str = Field(description=\"Table name\")\n",
    "    entity_id: str = Field(description=\"Mapped entity ID from Tool 1\")\n",
    "    description: str = Field(default=\"\", description=\"Table description\")\n",
    "    grain: str = Field(description=\"Granularity: transaction, event, snapshot, aggregate\")\n",
    "    estimated_row_count: str = Field(description=\"Size estimate: small/medium/large/huge\")\n",
    "\n",
    "# ==========================================\n",
    "# SCHEMA 2: DimensionTable (referenƒçn√≠/master data)\n",
    "# ==========================================\n",
    "# Co je DIMENSION table:\n",
    "# - Popisn√© atributy (descriptive data)\n",
    "# - N√≠zk√Ω poƒçet ≈ô√°dk≈Ø (reference scale)\n",
    "# - Relativnƒõ statick√° data (weekly/monthly updates)\n",
    "# - Prim√°rn√≠ kl√≠ƒçe (pro FK z fact tables)\n",
    "#\n",
    "# P≈ô√≠klady:\n",
    "# - products (katalog produkt≈Ø: name, category, price)\n",
    "# - customers (klienti: name, email, segment)\n",
    "# - locations (lokace: city, country, region)\n",
    "class DimensionTable(BaseModel):\n",
    "    \"\"\"Dimension table (referenƒçn√≠/master data).\n",
    "\n",
    "    Pydantic pou≈æit√≠:\n",
    "    - type: Typ dimension tabulky\n",
    "      * master: Hlavn√≠ master data (products, customers)\n",
    "      * reference: ƒå√≠seln√≠ky (countries, categories)\n",
    "      * lookup: Drobn√© lookup tables (status codes, types)\n",
    "      * bridge: Propojovac√≠ tabulky (many-to-many relationships)\n",
    "    - slowly_changing: SCD Type 2 flag (zda se trackuj√≠ historick√© zmƒõny)\n",
    "      * True: Pokud tabulka m√° history (nap≈ô. customer mƒõnil adresu)\n",
    "      * False: Pokud tabulka nem√° history (nap≈ô. country list)\n",
    "    \"\"\"\n",
    "    name: str = Field(description=\"Table name\")\n",
    "    entity_id: str = Field(description=\"Mapped entity ID from Tool 1\")\n",
    "    description: str = Field(default=\"\", description=\"Table description\")\n",
    "    type: str = Field(description=\"Type: master, reference, lookup, bridge\")\n",
    "    slowly_changing: bool = Field(default=False, description=\"SCD Type 2?\")\n",
    "\n",
    "# ==========================================\n",
    "# SCHEMA 3: Relationship (ciz√≠ kl√≠ƒç vztah)\n",
    "# ==========================================\n",
    "# Co je Relationship:\n",
    "# - Propojen√≠ mezi FACT a DIMENSION (FK constraint)\n",
    "# - Detekov√°no heuristikou (column suffix matching)\n",
    "# - Confidence score pro kvalitu detekce\n",
    "#\n",
    "# P≈ô√≠klad:\n",
    "# orders (fact) --(product_id)--> products (dimension)\n",
    "# clickstreams (fact) --(user_id)--> users (dimension)\n",
    "class Relationship(BaseModel):\n",
    "    \"\"\"Foreign key relationship mezi tabulkami.\n",
    "\n",
    "    Pydantic pou≈æit√≠:\n",
    "    - from_table: Source tabulka (obvykle FACT)\n",
    "    - to_table: Target tabulka (obvykle DIMENSION)\n",
    "    - relationship_type: Kardinalita vztahu\n",
    "      * one-to-one: 1 ≈ô√°dek v source ‚Üí max 1 ≈ô√°dek v target\n",
    "      * one-to-many: 1 ≈ô√°dek v target ‚Üí N ≈ô√°dk≈Ø v source (nejƒçastƒõj≈°√≠)\n",
    "      * many-to-many: N ≈ô√°dk≈Ø v source ‚Üí M ≈ô√°dk≈Ø v target (pot≈ôebuje bridge table)\n",
    "    - confidence: Jak jistƒõ jsme FK detekovali (0.0 = guess, 1.0 = confirmed)\n",
    "    \"\"\"\n",
    "    from_table: str = Field(description=\"Source table name\")\n",
    "    to_table: str = Field(description=\"Target table name\")\n",
    "    relationship_type: str = Field(description=\"one-to-one, one-to-many, many-to-many\")\n",
    "    confidence: float = Field(description=\"Detection confidence 0-1\")\n",
    "\n",
    "# ==========================================\n",
    "# SCHEMA 4: StructuralClassification (ROOT MODEL pro classifier_agent)\n",
    "# ==========================================\n",
    "# Co je StructuralClassification:\n",
    "# - ROOT MODEL = Hlavn√≠ v√Ωstup classifier_agent (result_type)\n",
    "# - Obsahuje 2 pole: facts[] a dimensions[]\n",
    "# - LLM mus√≠ vr√°tit OBOJ√ç (ne jen facts nebo jen dimensions)\n",
    "#\n",
    "# Proƒç 2 pole:\n",
    "# - Jasn√° separace FACT vs DIMENSION\n",
    "# - Validace, ≈æe ka≈æd√° entita je klasifikov√°na\n",
    "# - Type-safe access (result.data.facts, result.data.dimensions)\n",
    "class StructuralClassification(BaseModel):\n",
    "    \"\"\"Complete structural classification (ROOT MODEL pro classifier_agent).\n",
    "\n",
    "    Pydantic pou≈æit√≠:\n",
    "    - facts: Seznam v≈°ech FACT tables (transactional data)\n",
    "    - dimensions: Seznam v≈°ech DIMENSION tables (reference data)\n",
    "\n",
    "    Agent workflow:\n",
    "    1. classifier_agent dostane entities + metadata\n",
    "    2. LLM klasifikuje ka≈ædou entitu jako FACT nebo DIMENSION\n",
    "    3. Pydantic AI validuje, ≈æe output m√° structure: {facts: [...], dimensions: [...]}\n",
    "    4. result.data vr√°t√≠ StructuralClassification object\n",
    "    \"\"\"\n",
    "    facts: list[FactTable] = Field(description=\"Fact tables\")\n",
    "    dimensions: list[DimensionTable] = Field(description=\"Dimension tables\")\n",
    "\n",
    "# ==========================================\n",
    "# SCHEMA 5: StructuralMetrics (summary statistiky)\n",
    "# ==========================================\n",
    "# Co je StructuralMetrics:\n",
    "# - Metadata o v√Ωsledku klasifikace\n",
    "# - Poƒçty tabulek a vztah≈Ø\n",
    "# - Timestamp pro audit trail\n",
    "class StructuralMetrics(BaseModel):\n",
    "    \"\"\"Metrics about the structure (pro reporting a audit).\n",
    "\n",
    "    Pydantic pou≈æit√≠:\n",
    "    - fact_count, dimension_count: Poƒçet klasifikovan√Ωch tabulek\n",
    "    - relationship_count: Poƒçet detekovan√Ωch FK vztah≈Ø\n",
    "    - classification_timestamp: ISO 8601 timestamp (pro tracking zmƒõn)\n",
    "    \"\"\"\n",
    "    fact_count: int\n",
    "    dimension_count: int\n",
    "    relationship_count: int\n",
    "    classification_timestamp: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e665efdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# PYDANTIC AI AGENT - classifier_agent\n",
    "# ==========================================\n",
    "# √öƒçel: LLM agent pro FACT/DIMENSION klasifikaci\n",
    "# Input: business entities + technical metadata\n",
    "# Output: StructuralClassification (facts[] + dimensions[])\n",
    "#\n",
    "# Proƒç pot≈ôebujeme LLM:\n",
    "# - S√©mantick√° anal√Ωza (product vs orders - kter√Ω je FACT?)\n",
    "# - Kontext z business po≈æadavku (goal, scope)\n",
    "# - Metadata quality assessment (granularity, size estimate)\n",
    "# - Human-like reasoning (LLM um√≠ vysvƒõtlit klasifikaci)\n",
    "\n",
    "classifier_agent = Agent(\n",
    "    MODEL_NAME,                               # \"openai:test-gpt-5-mini\"\n",
    "    result_type=StructuralClassification,     # LLM MUS√ç vr√°tit {facts: [...], dimensions: [...]}\n",
    "    system_prompt=\"\"\"You are a data warehouse architect.\n",
    "\n",
    "Classify tables as FACT or DIMENSION:\n",
    "- FACT: Transactional/event data (orders, clickstreams, sensor readings)\n",
    "  * High row count\n",
    "  * Time-dependent\n",
    "  * Contains metrics/measures\n",
    "  * Foreign keys to dimensions\n",
    "\n",
    "- DIMENSION: Reference/master data (products, customers, locations)\n",
    "  * Lower row count\n",
    "  * Relatively static\n",
    "  * Descriptive attributes\n",
    "  * Primary keys\n",
    "\n",
    "For each table, determine:\n",
    "- Fact grain (transaction/event/snapshot/aggregate)\n",
    "- Dimension type (master/reference/lookup/bridge)\n",
    "- Size estimate (small/medium/large/huge)\n",
    "- SCD Type 2 (slowly changing dimension)\n",
    "\n",
    "Be specific and data-driven.\"\"\"\n",
    ")\n",
    "# System prompt pravidla:\n",
    "# 1. Definuje FACT vs DIMENSION krit√©ria (row count, time-dependency, metrics)\n",
    "# 2. Specifikuje co urƒçit (grain, type, size, SCD)\n",
    "# 3. Instruuje \"be specific and data-driven\" (vyhni se guesswork)\n",
    "#\n",
    "# LLM dostane:\n",
    "# - System prompt (v√Ω≈°e)\n",
    "# - User prompt (entities + metadata JSON)\n",
    "# - JSON Schema pro StructuralClassification (automaticky p≈ôid√°no Pydantic AI)\n",
    "#\n",
    "# LLM vr√°t√≠:\n",
    "# - JSON matching StructuralClassification schema\n",
    "# - Pydantic AI validuje a vr√°t√≠ type-safe object\n",
    "\n",
    "print(\"‚úÖ Classifier agent created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34576ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# HEURISTIC FK DETECTION - detect_fk_relationships()\n",
    "# ==========================================\n",
    "# Proƒç heuristika (ne LLM):\n",
    "# - FK detection je pattern matching probl√©m\n",
    "# - Rychl√© (<1ms per table)\n",
    "# - Deterministick√© (konzistentn√≠ v√Ωsledky)\n",
    "# - Nepot≈ôebuje s√©mantick√© porozumƒõn√≠\n",
    "#\n",
    "# Pattern matching pravidla:\n",
    "# 1. Singularizace dimension names (products ‚Üí product)\n",
    "# 2. Hled√°n√≠ dimension name v fact name\n",
    "#    P≈ô√≠klad: fact \"order_items\" obsahuje \"product\" ‚Üí FK na products dimension\n",
    "# 3. Confidence 0.7 (heuristika nen√≠ 100% jist√°)\n",
    "#\n",
    "# V produkci bychom pou≈æili:\n",
    "# - Skuteƒçn√© column names z metadata katalogu\n",
    "# - FK suffix patterns (product_id, customer_key, location_fk)\n",
    "# - Actual FK constraints z database schema\n",
    "\n",
    "def detect_fk_relationships(facts: list[FactTable], dimensions: list[DimensionTable]) -> list[Relationship]:\n",
    "    \"\"\"Heuristic FK detection based on column suffix matching.\n",
    "\n",
    "    Args:\n",
    "        facts: Seznam FACT tables (z classifier_agent)\n",
    "        dimensions: Seznam DIMENSION tables (z classifier_agent)\n",
    "\n",
    "    Returns:\n",
    "        list[Relationship]: Detekovan√© FK vztahy s confidence score\n",
    "\n",
    "    Algoritmus:\n",
    "        1. Singularizuj dimension names (products ‚Üí product)\n",
    "        2. Pro ka≈æd√Ω fact, zkontroluj jestli obsahuje dimension name\n",
    "        3. Pokud ano, p≈ôidej relationship (one-to-many) s confidence 0.7\n",
    "\n",
    "    P≈ô√≠klad:\n",
    "        facts = [FactTable(name=\"order_items\", ...)]\n",
    "        dimensions = [DimensionTable(name=\"products\", ...), DimensionTable(name=\"customers\", ...)]\n",
    "\n",
    "        Detekce:\n",
    "        - \"order_items\" obsahuje \"product\" ‚Üí FK na products (confidence 0.7)\n",
    "        - \"order_items\" neobsahuje \"customer\" ‚Üí no FK detected\n",
    "\n",
    "    Limitations:\n",
    "        - Heuristika je n√°chyln√° na false positives (product_categories ‚Üí products FK?)\n",
    "        - V produkci pou≈æ√≠t skuteƒçn√© column names z metadata\n",
    "        - Lep≈°√≠: analyze actual FK constraints from database schema\n",
    "    \"\"\"\n",
    "    relationships = []\n",
    "    dimension_names = {d.name.lower().rstrip('s') for d in dimensions}  # Singularize\n",
    "    # Singularizace: products ‚Üí product, categories ‚Üí categorie (simple plural removal)\n",
    "\n",
    "    for fact in facts:\n",
    "        # Simulate column names (in real scenario, from metadata)\n",
    "        # Example: orders fact might have product_id, customer_id columns\n",
    "        # For demo, assume naming convention: <entity>_id, <entity>_key, <entity>_fk\n",
    "\n",
    "        for dim in dimensions:\n",
    "            dim_singular = dim.name.lower().rstrip('s')\n",
    "            # Heuristic: if fact name contains dimension name, likely FK\n",
    "            # Example: \"order_items\" contains \"product\" ‚Üí FK to products\n",
    "            if dim_singular in fact.name.lower():\n",
    "                relationships.append(Relationship(\n",
    "                    from_table=fact.name,           # Source (FACT table)\n",
    "                    to_table=dim.name,              # Target (DIMENSION table)\n",
    "                    relationship_type=\"one-to-many\",  # Standard DW pattern (1 dimension ‚Üí N facts)\n",
    "                    confidence=0.7  # Heuristic confidence (not 100% certain without actual schema)\n",
    "                ))\n",
    "\n",
    "    return relationships\n",
    "\n",
    "print(\"‚úÖ FK detection function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a891242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# ASYNC CLASSIFICATION FUNCTION - classify_structure()\n",
    "# ==========================================\n",
    "# Tool 2 hlavn√≠ workflow:\n",
    "# 1. Vezmi business context (Tool 0) + entity mappings (Tool 1)\n",
    "# 2. Zavolej LLM classifier_agent (FACT/DIMENSION klasifikace)\n",
    "# 3. Detekuj FK relationships (heuristika)\n",
    "# 4. Spoƒç√≠tej metrics (fact_count, dimension_count, relationship_count)\n",
    "# 5. Vra≈• kompletn√≠ strukturu\n",
    "#\n",
    "# Proƒç async:\n",
    "# - classifier_agent.run() je async (Pydantic AI pattern)\n",
    "# - Umo≈æ≈àuje pozdƒõji p≈ôidat paralel processing (multiple classifications)\n",
    "\n",
    "async def classify_structure(tool0_context: dict, tool1_mappings: dict, metadata: dict) -> dict:\n",
    "    \"\"\"Classify tables into facts and dimensions.\n",
    "\n",
    "    Args:\n",
    "        tool0_context: Business request context from Tool 0\n",
    "            Obsahuje: goal, scope_in, entities[], metrics[], sources[]\n",
    "\n",
    "        tool1_mappings: Entity mappings from Tool 1\n",
    "            Obsahuje: ranking (top 10 candidates), mapping (1:1 entity‚Üícandidate)\n",
    "\n",
    "        metadata: Technical metadata (Collibra/Unity Catalog)\n",
    "            Obsahuje: table schemas, column definitions, statistics\n",
    "\n",
    "    Returns:\n",
    "        dict with keys:\n",
    "        - facts: List of FactTable objects (as dicts)\n",
    "        - dimensions: List of DimensionTable objects (as dicts)\n",
    "        - relationships: List of Relationship objects (FK detections)\n",
    "        - metrics: StructuralMetrics (counts + timestamp)\n",
    "\n",
    "    Workflow:\n",
    "        Step 1: Prepare LLM prompt (combine business + technical context)\n",
    "        Step 2: Call classifier_agent (LLM classification)\n",
    "        Step 3: Detect FK relationships (heuristic post-processing)\n",
    "        Step 4: Calculate metrics (summary statistics)\n",
    "        Step 5: Assemble final structure (convert Pydantic ‚Üí dict)\n",
    "    \"\"\"\n",
    "\n",
    "    # ==========================================\n",
    "    # STEP 1: Prepare LLM prompt\n",
    "    # ==========================================\n",
    "    # LLM pot≈ôebuje kontext:\n",
    "    # - Business goal/scope (proƒç klasifikujeme?)\n",
    "    # - Entities (co klasifikujeme?)\n",
    "    # - Mappings from Tool 1 (kter√© kandid√°ty jsou relevantn√≠?)\n",
    "    # - Technical metadata sample (jak vypadaj√≠ data?)\n",
    "\n",
    "    entities = tool0_context.get(\"entities\", [])\n",
    "    mappings = tool1_mappings.get(\"mappings\", [])\n",
    "\n",
    "    prompt = f\"\"\"Classify these entities into FACT and DIMENSION tables:\n",
    "\n",
    "Business Context:\n",
    "- Goal: {tool0_context.get('goal', 'N/A')}\n",
    "- Scope: {tool0_context.get('scope_in', 'N/A')}\n",
    "\n",
    "Entities from Business Request:\n",
    "{json.dumps(entities, indent=2)}\n",
    "\n",
    "Mapped Candidates (Tool 1):\n",
    "{json.dumps(mappings[:10], indent=2)}  # Top 10 to avoid token overflow\n",
    "\n",
    "Technical Metadata Sample:\n",
    "{json.dumps(list(metadata.items())[:5], indent=2)}\n",
    "\n",
    "Classify each entity as FACT or DIMENSION with justification.\"\"\"\n",
    "    # Prompt strategie:\n",
    "    # - Business context first (framework pro decision-making)\n",
    "    # - Entities second (co klasifikovat)\n",
    "    # - Mappings third (Tool 1 suggestions)\n",
    "    # - Metadata last (technical details)\n",
    "    # - Token limit: Top 10 mappings + 5 metadata samples (avoid overflow)\n",
    "\n",
    "    # ==========================================\n",
    "    # STEP 2: Call LLM classifier\n",
    "    # ==========================================\n",
    "    # Pydantic AI workflow:\n",
    "    # 1. agent.run(prompt) zavol√° LLM\n",
    "    # 2. LLM vr√°t√≠ JSON matching StructuralClassification schema\n",
    "    # 3. Pydantic AI validuje JSON ‚Üí Pydantic object\n",
    "    # 4. result.data = StructuralClassification instance\n",
    "\n",
    "    result = await classifier_agent.run(prompt)\n",
    "    classified = result.data  # Type: StructuralClassification\n",
    "    # classified.facts = list[FactTable]\n",
    "    # classified.dimensions = list[DimensionTable]\n",
    "\n",
    "    # ==========================================\n",
    "    # STEP 3: Detect FK relationships\n",
    "    # ==========================================\n",
    "    # Post-processing: Heuristic FK detection\n",
    "    # Input: classified facts + dimensions\n",
    "    # Output: list[Relationship] (FK constraints)\n",
    "    # Pattern: name matching (product in order_items ‚Üí FK to products)\n",
    "\n",
    "    relationships = detect_fk_relationships(classified.facts, classified.dimensions)\n",
    "\n",
    "    # ==========================================\n",
    "    # STEP 4: Calculate metrics\n",
    "    # ==========================================\n",
    "    # Summary statistics pro reporting a audit:\n",
    "    # - Kolik facts/dimensions jsme na≈°li?\n",
    "    # - Kolik FK relationships jsme detekovali?\n",
    "    # - Kdy probƒõhla klasifikace? (timestamp)\n",
    "\n",
    "    metrics = StructuralMetrics(\n",
    "        fact_count=len(classified.facts),\n",
    "        dimension_count=len(classified.dimensions),\n",
    "        relationship_count=len(relationships),\n",
    "        classification_timestamp=datetime.now().isoformat()  # ISO 8601 format\n",
    "    )\n",
    "\n",
    "    # ==========================================\n",
    "    # STEP 5: Assemble final structure\n",
    "    # ==========================================\n",
    "    # Convert Pydantic objects ‚Üí dict (pro JSON serialization)\n",
    "    # .model_dump() = Pydantic metoda (convert BaseModel ‚Üí dict)\n",
    "    # List comprehension: [obj.model_dump() for obj in list]\n",
    "\n",
    "    return {\n",
    "        \"facts\": [f.model_dump() for f in classified.facts],\n",
    "        \"dimensions\": [d.model_dump() for d in classified.dimensions],\n",
    "        \"relationships\": [r.model_dump() for r in relationships],\n",
    "        \"metrics\": metrics.model_dump()\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Async classification function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43e8284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load input data from DBFS\n",
    "tool0_path = \"/dbfs/FileStore/mcop/tool0_samples/sample_business_request.json\"\n",
    "tool1_path = \"/dbfs/FileStore/mcop/tool1/filtered_dataset.json\"\n",
    "metadata_path = \"/dbfs/FileStore/mcop/metadata/BA-BS_Datamarts_metadata.json\"\n",
    "\n",
    "with open(tool0_path, \"r\") as f:\n",
    "    tool0_context = json.load(f)\n",
    "\n",
    "with open(tool1_path, \"r\") as f:\n",
    "    tool1_mappings = json.load(f)\n",
    "\n",
    "with open(metadata_path, \"r\") as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "print(f\"‚úÖ Loaded Tool 0 context: {len(tool0_context.get('entities', []))} entities\")\n",
    "print(f\"‚úÖ Loaded Tool 1 mappings: {len(tool1_mappings.get('mappings', []))} mappings\")\n",
    "print(f\"‚úÖ Loaded metadata: {len(metadata)} items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d73662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run classification\n",
    "structure = await classify_structure(tool0_context, tool1_mappings, metadata)\n",
    "\n",
    "print(f\"\\n‚úÖ Classification complete\")\n",
    "print(f\"   Facts: {structure['metrics']['fact_count']}\")\n",
    "print(f\"   Dimensions: {structure['metrics']['dimension_count']}\")\n",
    "print(f\"   Relationships: {structure['metrics']['relationship_count']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f19ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to DBFS\n",
    "output_path = \"/dbfs/FileStore/mcop/tool2/structure.json\"\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "with open(output_path, \"w\") as f:\n",
    "    json.dump(structure, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Structure saved: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c977b727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STRUCTURAL CLASSIFICATION RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä Metrics:\")\n",
    "print(f\"   Facts: {structure['metrics']['fact_count']}\")\n",
    "print(f\"   Dimensions: {structure['metrics']['dimension_count']}\")\n",
    "print(f\"   Relationships: {structure['metrics']['relationship_count']}\")\n",
    "print(f\"   Timestamp: {structure['metrics']['classification_timestamp']}\")\n",
    "\n",
    "print(f\"\\nüì¶ Sample Facts (top 3):\")\n",
    "for fact in structure['facts'][:3]:\n",
    "    print(f\"   - {fact['name']} (grain: {fact['grain']}, size: {fact['estimated_row_count']})\")\n",
    "\n",
    "print(f\"\\nüóÇÔ∏è  Sample Dimensions (top 3):\")\n",
    "for dim in structure['dimensions'][:3]:\n",
    "    print(f\"   - {dim['name']} (type: {dim['type']}, SCD2: {dim['slowly_changing']})\")\n",
    "\n",
    "print(f\"\\nüîó Sample Relationships (top 3):\")\n",
    "for rel in structure['relationships'][:3]:\n",
    "    print(f\"   - {rel['from_table']} ‚Üí {rel['to_table']} ({rel['relationship_type']}, confidence: {rel['confidence']:.1%})\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
