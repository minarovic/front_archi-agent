{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371d1bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages on Databricks cluster\n",
    "%pip install openai pydantic python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e25e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart Python kernel to load new packages\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095e0468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "from pydantic import BaseModel, Field, field_validator\n",
    "from datetime import datetime\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# ==========================================\n",
    "# PYDANTIC SCHEMAS - Definice struktury dat\n",
    "# ==========================================\n",
    "# Tyto t≈ô√≠dy definuj√≠ OƒåEK√ÅVAN√ù FORM√ÅT v√Ωstupu z LLM.\n",
    "# Pydantic zaji≈°≈•uje:\n",
    "# 1. Type safety - ka≈æd√© pole m√° jasn√Ω datov√Ω typ (str, list[str], dict...)\n",
    "# 2. Validation - automatick√° kontrola form√°tu (nap≈ô. ISO 8601 datum)\n",
    "# 3. Default values - pokud LLM pole vynech√°, pou≈æije se v√Ωchoz√≠ hodnota\n",
    "# 4. Field descriptions - metadata pro LLM prompt (vysvƒõtluje, co pole obsahuje)\n",
    "\n",
    "class ProjectMetadata(BaseModel):\n",
    "    \"\"\"Metadata o business projektu.\n",
    "\n",
    "    Obsahuje z√°kladn√≠ identifikaƒçn√≠ √∫daje:\n",
    "    - project_name: N√°zev projektu (nap≈ô. \"Supplier Risk Insights 2.0\")\n",
    "    - sponsor: Jm√©no sponzora/zadavatele (nap≈ô. \"Marek Hrub√Ω\")\n",
    "    - submitted_at: Datum pod√°n√≠ ≈æ√°dosti ve form√°tu ISO 8601 (YYYY-MM-DD)\n",
    "    - extra: Slovn√≠k pro dodateƒçn√° pole (nap≈ô. \"oddƒõlen√≠\", \"priorita\")\n",
    "\n",
    "    Pydantic automaticky validuje, ≈æe v≈°echna pole maj√≠ spr√°vn√Ω typ.\n",
    "    \"\"\"\n",
    "\n",
    "    project_name: str = Field(description=\"Name of the project\")\n",
    "    sponsor: str = Field(description=\"Name of the project sponsor\")\n",
    "    submitted_at: str = Field(description=\"Date when the request was submitted, in ISO 8601 format (YYYY-MM-DD)\")\n",
    "    extra: dict[str, str] = Field(default_factory=dict, description=\"Additional metadata fields as key-value pairs\")\n",
    "\n",
    "    @field_validator('submitted_at')\n",
    "    @classmethod\n",
    "    def validate_iso_date(cls, v: str) -> str:\n",
    "        \"\"\"Custom validator: Zkontroluje, ≈æe datum je ve spr√°vn√©m ISO 8601 form√°tu.\n",
    "\n",
    "        Jak to funguje:\n",
    "        1. Pydantic zavol√° tuto funkci PO kontrole typu (≈æe 'v' je str)\n",
    "        2. Pokus√≠me se datum parsovat pomoc√≠ datetime.fromisoformat()\n",
    "        3. Pokud √∫spƒõch ‚Üí vr√°t√≠me p≈Øvodn√≠ string (validace OK)\n",
    "        4. Pokud chyba ‚Üí vyhod√≠me ValueError s jasnou chybovou zpr√°vou\n",
    "\n",
    "        P≈ô√≠klad:\n",
    "        - \"2025-10-28\" ‚Üí ‚úÖ projde\n",
    "        - \"28.10.2025\" ‚Üí ‚ùå ValueError\n",
    "        - \"unknown\" ‚Üí ‚ùå ValueError\n",
    "        \"\"\"\n",
    "        try:\n",
    "            datetime.fromisoformat(v)\n",
    "            return v\n",
    "        except ValueError:\n",
    "            raise ValueError(f\"Date must be in ISO 8601 format (YYYY-MM-DD), got: {v}\")\n",
    "\n",
    "\n",
    "class BusinessRequest(BaseModel):\n",
    "    \"\"\"Kompletn√≠ struktura parsovan√©ho business dokumentu.\n",
    "\n",
    "    Obsahuje 9 hlavn√≠ch ƒç√°st√≠ business po≈æadavku:\n",
    "    1. project_metadata - z√°kladn√≠ √∫daje o projektu (nested object)\n",
    "    2. goal - hlavn√≠ c√≠l projektu (string)\n",
    "    3. scope_in - co JE v rozsahu (string)\n",
    "    4. scope_out - co NEN√ç v rozsahu (string)\n",
    "    5. entities - kl√≠ƒçov√© business entity (array of strings)\n",
    "    6. metrics - KPI metriky (array of strings)\n",
    "    7. sources - datov√© zdroje (array of strings)\n",
    "    8. constraints - omezen√≠ a po≈æadavky (array of strings)\n",
    "    9. deliverables - po≈æadovan√© v√Ωstupy (array of strings)\n",
    "\n",
    "    Default hodnoty:\n",
    "    - Stringy: \"unknown\" (pokud LLM pole nenajde)\n",
    "    - Listy: [] (pr√°zdn√Ω array m√≠sto None)\n",
    "\n",
    "    D√≠ky Pydantic m≈Ø≈æeme:\n",
    "    - validated = BusinessRequest(**json_dict) ‚Üí automatick√° validace\n",
    "    - validated.model_dump() ‚Üí zpƒõt do dict pro ulo≈æen√≠ do JSON\n",
    "    - validated.project_metadata.sponsor ‚Üí type-safe p≈ô√≠stup k pol√≠m\n",
    "    \"\"\"\n",
    "\n",
    "    project_metadata: ProjectMetadata = Field(description=\"Project metadata including name, sponsor, and submission date\")\n",
    "    goal: str = Field(default=\"unknown\", description=\"Main goal or objective of the project\")\n",
    "    scope_in: str = Field(default=\"unknown\", description=\"What is included in the project scope\")\n",
    "    scope_out: str = Field(default=\"unknown\", description=\"What is explicitly excluded from the project scope\")\n",
    "    entities: list[str] = Field(default_factory=list, description=\"Key business entities involved in the project\")\n",
    "    metrics: list[str] = Field(default_factory=list, description=\"Key metrics or KPIs to be tracked\")\n",
    "    sources: list[str] = Field(default_factory=list, description=\"Expected data sources for the project\")\n",
    "    constraints: list[str] = Field(default_factory=list, description=\"Constraints, limitations, or special requirements\")\n",
    "    deliverables: list[str] = Field(default_factory=list, description=\"Required deliverables or artifacts from the project\")\n",
    "\n",
    "print(\"‚úÖ Schemas defined successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13968507",
   "metadata": {},
   "source": [
    "## 1. Configure Azure OpenAI with Databricks Secrets\n",
    "\n",
    "**Setup Instructions:**\n",
    "```bash\n",
    "# Create secret scope (run once in Databricks CLI or UI)\n",
    "databricks secrets create-scope --scope mcop\n",
    "\n",
    "# Add Azure OpenAI credentials\n",
    "databricks secrets put --scope mcop --key azure-openai-endpoint\n",
    "databricks secrets put --scope mcop --key azure-openai-api-key\n",
    "databricks secrets put --scope mcop --key azure-openai-deployment-name\n",
    "```\n",
    "\n",
    "**Expected values:**\n",
    "- `azure-openai-endpoint`: https://minar-mhi2wuzy-swedencentral.cognitiveservices.azure.com/openai/v1/\n",
    "- `azure-openai-deployment-name`: test-gpt-5-mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fc79eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Azure configuration from Databricks secrets\n",
    "AZURE_ENDPOINT = dbutils.secrets.get(scope=\"mcop\", key=\"azure-openai-endpoint\").strip()\n",
    "AZURE_API_KEY = dbutils.secrets.get(scope=\"mcop\", key=\"azure-openai-api-key\").strip()\n",
    "DEPLOYMENT_NAME = dbutils.secrets.get(scope=\"mcop\", key=\"azure-openai-deployment-name\").strip()\n",
    "\n",
    "if not all([AZURE_ENDPOINT, AZURE_API_KEY, DEPLOYMENT_NAME]):\n",
    "    raise ValueError(\"Missing Azure configuration in Databricks secrets (scope: mcop)\")\n",
    "\n",
    "print(f\"‚òÅÔ∏è Azure OpenAI configured from Databricks secrets\")\n",
    "print(f\"   Endpoint: {AZURE_ENDPOINT}\")\n",
    "print(f\"   Deployment: {DEPLOYMENT_NAME}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62413f6b",
   "metadata": {},
   "source": [
    "## 2. Load Sample Business Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa02e72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hardcoded sample business document\n",
    "business_document = \"\"\"# ≈Ω√°dost o datov√Ω projekt ‚Äì Supplier Risk Insights 2.0\n",
    "\n",
    "## Projekt\n",
    "**N√°zev:** Supplier Risk Insights 2.0\n",
    "**Sponzor:** Marek Hrub√Ω (VP Procurement Excellence)\n",
    "**Datum:** 2025-10-28\n",
    "**Oddƒõlen√≠:** Group Procurement Analytics\n",
    "**Priorita:** Kritick√° ‚Äì Q4 OKR \"Stabilizace dodavatelsk√©ho ≈ôetƒõzce\"\n",
    "\n",
    "## C√≠l\n",
    "Dodat konsolidovan√Ω pohled na spolehlivost dodavatel≈Ø nap≈ô√≠ƒç BA/BS datamar≈•y a SAP ECC zdroji. V√Ωsledn√Ω reporting mus√≠ upozor≈àovat na dodavatele s rostouc√≠m lead time, ƒçast√Ωmi reklamacemi nebo blokacemi plateb, aby procurement dok√°zal vƒças p≈ôesmƒõrovat objem a eskalovat smluvn√≠ pokuty.\n",
    "\n",
    "## Rozsah\n",
    "\n",
    "### In Scope\n",
    "- Historick√° data o purchase orders (posledn√≠ch 36 mƒõs√≠c≈Ø) vƒçetnƒõ RU/DE regionu.\n",
    "- Dimenze dodavatel, produkt, dodac√≠ lokace, n√°kupn√≠ organizace.\n",
    "- SLA metriky: on-time delivery, defect rate, invoice dispute count.\n",
    "- Sp√°rov√°n√≠ se security klasifikac√≠ (Confidential vs Internal).\n",
    "- Export KPI do Power BI workspace \"Supplier Control Tower\".\n",
    "\n",
    "### Out of Scope\n",
    "- Forecasting budouc√≠ch objedn√°vek (≈ôe≈°√≠ Supply Planning t√Ωm).\n",
    "- Integrace s CRM a risk ratingy t≈ôet√≠ch stran.\n",
    "- Real-time streaming ze SCADA nebo IoT senzor≈Ø.\n",
    "- Detailn√≠ finanƒçn√≠ mar≈æe ‚Äì pou≈æ√≠v√° Finance Controlling.\n",
    "\n",
    "## Kl√≠ƒçov√© entity & metriky\n",
    "\n",
    "### Entity\n",
    "- Supplier Master (Collibra/Unity Catalog `dimv_supplier`).\n",
    "- Purchase Order Header + Item (`factv_purchase_order`, `factv_purchase_order_item`).\n",
    "- Quality Incident (`factv_quality_notification`).\n",
    "- Delivery Calendar Dimension (`dimv_delivery_date`).\n",
    "\n",
    "### Metriky\n",
    "- Supplier Reliability Index (v√°≈æen√Ω mix on-time %, dispute rate, defect rate).\n",
    "- Average Goods Receipt Lead Time (dny).\n",
    "- % PO s ‚Äûblocked for payment\" statusem.\n",
    "- NCR Count (non-conformance reports) za posledn√≠ kvart√°l.\n",
    "- Spend concentration top 10 dodavatel≈Ø.\n",
    "\n",
    "## Oƒçek√°van√© zdroje\n",
    "- Databricks Unity Catalog: `dm_ba_purchase`, `dm_bs_purchase` schemata.\n",
    "- Collibra Data Catalog export (zaji≈°≈•uje lineage a vlastn√≠ky).\n",
    "- SAP ECC tabulky: `EKKO`, `EKPO`, `LFA1`, `MKPF`.\n",
    "- SharePoint slo≈æka \"Supplier Audits\" pro manu√°ln√≠ NCR z√°pisy.\n",
    "\n",
    "## Omezen√≠\n",
    "- GDPR: ≈æ√°dn√° osobn√≠ data supplier kontakt≈Ø v datasetu; pseudonymizace ID.\n",
    "- Data retention: pouze 3 roky historie v produkƒçn√≠m modelu.\n",
    "- Ka≈æd√Ω dashboard refresh < 5 min, jinak neprojde SLA.\n",
    "- Row Level Security podle regionu (EMEA, AMER, APAC).\n",
    "- Pouze read-only p≈ô√≠stup do SAP; ≈æ√°dn√© z√°pisy zpƒõt.\n",
    "\n",
    "## Po≈æadovan√© artefakty\n",
    "- Kur√°torovan√© `business_request.json` a `structure.json` pro Tool 3/7.\n",
    "- Quality report shrnuj√≠c√≠ articulationScore + missingFromSource flagy.\n",
    "- Power BI semantic model + definice DAX measures.\n",
    "- Governance runbook popisuj√≠c√≠ validace a kontakty (owner, steward).\n",
    "- Checklist P0/P1/P2 mitigac√≠ pro Supplier Risk komisi.\n",
    "\"\"\"\n",
    "\n",
    "print(f\"üìÑ Business document loaded ({len(business_document)} characters)\")\n",
    "print(\"\\nFirst 300 characters:\")\n",
    "print(\"=\" * 60)\n",
    "print(business_document[:300])\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c48e99",
   "metadata": {},
   "source": [
    "## 3. Parse Document Using Azure OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8016de77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# AZURE OPENAI CLIENT - Inicializace LLM\n",
    "# ==========================================\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "print(f\"üîÑ Parsing document with Azure OpenAI ({DEPLOYMENT_NAME})...\")\n",
    "\n",
    "# System prompt - instrukce pro LLM, jak m√° parsovat dokument\n",
    "# Popisuje:\n",
    "# - Jak√Ω je √∫kol (extract structured information)\n",
    "# - Jak√© sekce oƒçek√°v√°me (Projekt, C√≠l, Rozsah...)\n",
    "# - Jak m√° LLM form√°tovat v√Ωstup (JSON, ISO 8601 datumy, arrays...)\n",
    "# - Co dƒõlat, kdy≈æ nƒõco chyb√≠ (pou≈æ√≠t \"unknown\")\n",
    "SYSTEM_PROMPT = \"\"\"You are a business requirements parser. Your task is to extract structured information from business request documents.\n",
    "\n",
    "Documents may contain a mix of Czech and English. Common section headers include:\n",
    "- \"Projekt\" / \"Project\" - project metadata (name, sponsor, date)\n",
    "- \"C√≠l\" / \"Goal\" - main project objective\n",
    "- \"Rozsah\" / \"Scope\" - what is in/out of scope\n",
    "- \"Kl√≠ƒçov√© entity & metriky\" / \"Key entities & metrics\" - business entities and KPIs\n",
    "- \"Oƒçek√°van√© zdroje\" / \"Expected sources\" - data sources\n",
    "- \"Omezen√≠\" / \"Constraints\" - limitations and requirements\n",
    "- \"Po≈æadovan√© artefakty\" / \"Required artifacts\" - deliverables\n",
    "\n",
    "IMPORTANT INSTRUCTIONS:\n",
    "1. Extract information into the structured JSON format exactly as specified\n",
    "2. Use \"unknown\" for any missing sections\n",
    "3. Ensure dates are in ISO 8601 format (YYYY-MM-DD)\n",
    "4. Extract lists as arrays of strings, not concatenated text\n",
    "5. For project metadata, look for project name, sponsor name, and submission date\n",
    "6. Any additional metadata fields should go into the \"extra\" dictionary\n",
    "7. Be thorough - extract all relevant information from the document\n",
    "8. Return ONLY valid JSON, no markdown or code blocks\n",
    "\n",
    "Expected JSON schema:\n",
    "{\n",
    "  \"project_metadata\": {\n",
    "    \"project_name\": \"string\",\n",
    "    \"sponsor\": \"string\",\n",
    "    \"submitted_at\": \"YYYY-MM-DD\",\n",
    "    \"extra\": {}\n",
    "  },\n",
    "  \"goal\": \"string\",\n",
    "  \"scope_in\": \"string\",\n",
    "  \"scope_out\": \"string\",\n",
    "  \"entities\": [],\n",
    "  \"metrics\": [],\n",
    "  \"sources\": [],\n",
    "  \"constraints\": [],\n",
    "  \"deliverables\": []\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# ==========================================\n",
    "# AZURE OPENAI CLIENT SETUP\n",
    "# ==========================================\n",
    "# D≈Øle≈æit√©: AzureOpenAI SDK vy≈æaduje endpoint BEZ /openai/v1/ suffixu\n",
    "# SDK automaticky p≈ôid√° spr√°vnou cestu podle API version\n",
    "azure_endpoint = AZURE_ENDPOINT.replace(\"/openai/v1/\", \"\").replace(\"/openai/v1\", \"\").rstrip(\"/\")\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=azure_endpoint,      # https://minar-mhi2wuzy-swedencentral.cognitiveservices.azure.com/\n",
    "    api_key=AZURE_API_KEY,               # API kl√≠ƒç z Databricks secrets\n",
    "    api_version=\"2024-10-21\"             # Azure OpenAI API verze\n",
    ")\n",
    "\n",
    "# User message - spoj√≠me business dokument s instrukcemi\n",
    "user_message = f\"\"\"Parse the following business request document:\n",
    "\n",
    "{business_document}\n",
    "\n",
    "Extract all information into the structured JSON format.\"\"\"\n",
    "\n",
    "# ==========================================\n",
    "# LLM CALL - Vol√°n√≠ Azure OpenAI\n",
    "# ==========================================\n",
    "# Kl√≠ƒçov√© parametry:\n",
    "# - model: deployment name (test-gpt-5-mini)\n",
    "# - messages: system prompt + user message (standard Chat Completion format)\n",
    "# - response_format: {\"type\": \"json_object\"} ‚Üí LLM MUS√ç vr√°tit valid JSON\n",
    "#\n",
    "# JSON mode zaji≈°≈•uje:\n",
    "# 1. LLM v≈ædycky vr√°t√≠ parsovateln√Ω JSON (ne markdown, ne prose text)\n",
    "# 2. Nutn√© explicitnƒõ ≈ô√≠ct v system promptu \"return JSON\"\n",
    "# 3. Pydantic pak JSON validuje proti na≈°emu sch√©matu\n",
    "response = client.chat.completions.create(\n",
    "    model=DEPLOYMENT_NAME,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": user_message}\n",
    "    ],\n",
    "    response_format={\"type\": \"json_object\"}  # ‚úÖ KRITICK√â: Vynut√≠ JSON output\n",
    ")\n",
    "\n",
    "# Extract JSON string from response\n",
    "raw_response = response.choices[0].message.content\n",
    "\n",
    "# ==========================================\n",
    "# PYDANTIC VALIDATION - Kontrola struktury\n",
    "# ==========================================\n",
    "try:\n",
    "    # 1. Parse JSON string ‚Üí Python dict\n",
    "    parsed_json = json.loads(raw_response)\n",
    "\n",
    "    # 2. Validate dict against Pydantic schema\n",
    "    #    Toto provede:\n",
    "    #    - Type checking (str? list? dict?)\n",
    "    #    - Custom validators (@field_validator pro datum)\n",
    "    #    - Default value injection (pokud pole chyb√≠)\n",
    "    #    - Vyhod√≠ ValidationError, pokud struktura nesed√≠\n",
    "    validated = BusinessRequest(**parsed_json)\n",
    "\n",
    "    # 3. Convert validated model back to dict (pro ulo≈æen√≠)\n",
    "    #    model_dump() vr√°t√≠ ƒçist√Ω dict, ale u≈æ validovan√Ω\n",
    "    parsed_json = validated.model_dump()\n",
    "\n",
    "    print(\"‚úÖ Parsing complete!\")\n",
    "    print(f\"   Model: {response.model}\")\n",
    "    print(f\"   Tokens: {response.usage.total_tokens}\")\n",
    "    print(f\"   Validation: ‚úÖ Passed\")\n",
    "\n",
    "except json.JSONDecodeError as e:\n",
    "    # JSON mode selhal ‚Üí LLM vr√°til nevalidn√≠ JSON (shouldn't happen)\n",
    "    print(f\"‚ùå JSON parsing error: {e}\")\n",
    "    print(f\"Raw response: {raw_response}\")\n",
    "    raise\n",
    "\n",
    "except Exception as e:\n",
    "    # Pydantic validation selhala ‚Üí JSON m√° ≈°patnou strukturu\n",
    "    # (nap≈ô. chyb√≠ povinn√© pole, ≈°patn√Ω typ, custom validator error)\n",
    "    print(f\"‚ùå Validation error: {e}\")\n",
    "    print(f\"Parsed JSON: {parsed_json}\")\n",
    "    raise\n",
    "\n",
    "# Full prompt for audit (save to DBFS later)\n",
    "prompt_used = f\"System: {SYSTEM_PROMPT}\\n\\nUser: {user_message}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55145cf",
   "metadata": {},
   "source": [
    "## 4. Display Parsed JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98751e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display parsed JSON\n",
    "print(\"üìä Parsed Business Request:\")\n",
    "print(\"=\" * 60)\n",
    "print(json.dumps(parsed_json, indent=2, ensure_ascii=False))\n",
    "\n",
    "# Also show as Pydantic model\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìã Validation:\")\n",
    "try:\n",
    "    validated = BusinessRequest.model_validate(parsed_json)\n",
    "    print(f\"‚úÖ Schema valid: {validated.project_metadata.project_name}\")\n",
    "    print(f\"   Sponsor: {validated.project_metadata.sponsor}\")\n",
    "    print(f\"   Date: {validated.project_metadata.submitted_at}\")\n",
    "    print(f\"   Entities: {len(validated.entities)} found\")\n",
    "    print(f\"   Sources: {len(validated.sources)} found\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Validation error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9e8512",
   "metadata": {},
   "source": [
    "## 5. Save Results to DBFS\n",
    "\n",
    "Save to `/dbfs/FileStore/mcop/tool0_samples/` for persistence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a2227c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to DBFS\n",
    "timestamp = datetime.now().isoformat().replace(':', '-')  # DBFS-safe filename\n",
    "output_dir = Path('/dbfs/FileStore/mcop/tool0_samples')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save JSON result\n",
    "json_path = output_dir / f\"{timestamp}.json\"\n",
    "with open(json_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(parsed_json, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# Save prompt\n",
    "md_path = output_dir / f\"{timestamp}.md\"\n",
    "with open(md_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(f\"# Parse Request - {timestamp}\\n\\n\")\n",
    "    f.write(f\"## Prompt Used\\n\\n```\\n{prompt_used}\\n```\\n\\n\")\n",
    "    f.write(f\"## Raw Response\\n\\n```\\n{raw_response}\\n```\\n\\n\")\n",
    "    f.write(f\"## Parsed JSON\\n\\n```json\\n{json.dumps(parsed_json, indent=2, ensure_ascii=False)}\\n```\\n\")\n",
    "\n",
    "print(f\"üíæ Results saved to DBFS:\")\n",
    "print(f\"   JSON: {json_path}\")\n",
    "print(f\"   Markdown: {md_path}\")\n",
    "print(f\"\\nüìÇ View files in Databricks:\")\n",
    "print(f\"   dbfs:/FileStore/mcop/tool0_samples/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4072ccb",
   "metadata": {},
   "source": [
    "## 6. Summary\n",
    "\n",
    "‚úÖ **Databricks Deployment Complete:**\n",
    "- [x] Azure OpenAI credentials from Databricks secrets (scope: `mcop`)\n",
    "- [x] Pydantic schemas for validation (ProjectMetadata, BusinessRequest)\n",
    "- [x] Sample business document (Czech/English mix)\n",
    "- [x] OpenAI SDK with Azure endpoint and JSON mode\n",
    "- [x] Pydantic validation after parsing\n",
    "- [x] Results saved to DBFS (`/dbfs/FileStore/mcop/tool0_samples/`)\n",
    "\n",
    "**Databricks-Specific Changes:**\n",
    "- ‚úÖ `dbutils.secrets.get()` instead of `.env` file\n",
    "- ‚úÖ `/dbfs/FileStore/` paths instead of local `data/` directory\n",
    "- ‚úÖ `%pip install` cell for package installation\n",
    "- ‚úÖ `dbutils.library.restartPython()` to reload packages\n",
    "- ‚úÖ DBFS-safe filenames (replaced `:` with `-` in timestamps)\n",
    "\n",
    "**Azure AI Foundry Configuration:**\n",
    "- **Endpoint:** https://minar-mhi2wuzy-swedencentral.cognitiveservices.azure.com/openai/v1/\n",
    "- **Deployment:** test-gpt-5-mini\n",
    "- **Model:** gpt-5-mini-2025-08-07\n",
    "- **Pattern A:** Direct OpenAI SDK with JSON mode + Pydantic validation\n",
    "\n",
    "**Next Steps:**\n",
    "1. Create secret scope: `databricks secrets create-scope --scope mcop`\n",
    "2. Add credentials: `databricks secrets put --scope mcop --key azure-openai-endpoint`\n",
    "3. Run notebook on Databricks cluster\n",
    "4. Verify files in DBFS: `dbfs:/FileStore/mcop/tool0_samples/`"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
