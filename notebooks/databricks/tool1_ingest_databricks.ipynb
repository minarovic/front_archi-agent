{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a27e9430",
   "metadata": {},
   "source": [
    "# Tool 1 - Entity to Candidate Mapper (Parallel Multi-Agent Pattern)\n",
    "\n",
    "**Status:** âœ… Ready for Databricks | **LLM Cost:** ~$0.003 per run | **Performance:** ~10s\n",
    "\n",
    "**Pattern:** 2 Pydantic AI agents running in parallel via `asyncio.gather`\n",
    "\n",
    "**Showcase:** Parallel multi-agent execution - both agents run simultaneously, results merged with consistency validation.\n",
    "\n",
    "**Key Features:**\n",
    "- 2 specialized agents: `ranking_agent` (top 10 candidates) + `mapping_agent` (1:1 entity mappings)\n",
    "- Parallel execution with `asyncio.gather` (both agents run simultaneously)\n",
    "- Consistency check validates overlap between ranked and mapped candidates (target: >70%)\n",
    "- Expected performance: ~10s for parallel execution\n",
    "\n",
    "**TODO:**\n",
    "- [ ] Add retry logic for LLM failures (timeout, rate limit)\n",
    "- [ ] Cache ranking results for repeated entity sets\n",
    "- [ ] Test with 20+ entities (current test: 5-10)\n",
    "- [ ] Add metadata quality score (completeness, freshness)\n",
    "\n",
    "**IDEA:**\n",
    "- Consider hybrid: embedding similarity for initial filtering + LLM for final ranking\n",
    "- Add configurable top-N parameter (currently hardcoded to 10)\n",
    "- Export consistency report as separate artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b187655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "%pip install pydantic-ai>=0.0.49 pydantic>=2.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b198e404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart Python kernel to use new packages\n",
    "dbutils.library.restartPython()  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42758af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "from pydantic import BaseModel, Field\n",
    "from pydantic_ai import Agent  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5145844b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Azure OpenAI from Databricks secrets\n",
    "AZURE_ENDPOINT = dbutils.secrets.get(scope=\"mcop\", key=\"azure-openai-endpoint\")  # type: ignore\n",
    "AZURE_API_KEY = dbutils.secrets.get(scope=\"mcop\", key=\"azure-openai-api-key\")  # type: ignore\n",
    "DEPLOYMENT_NAME = dbutils.secrets.get(scope=\"mcop\", key=\"azure-openai-deployment-name\")  # type: ignore\n",
    "\n",
    "# Set environment variables for Pydantic AI\n",
    "os.environ[\"OPENAI_BASE_URL\"] = AZURE_ENDPOINT\n",
    "os.environ[\"OPENAI_API_KEY\"] = AZURE_API_KEY\n",
    "\n",
    "MODEL_NAME = f\"openai:{DEPLOYMENT_NAME}\"\n",
    "print(f\"âœ… Configured model: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acb2aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pydantic schemas\n",
    "class CandidateRank(BaseModel):\n",
    "    \"\"\"Ranked candidate with score.\"\"\"\n",
    "    entity_id: str = Field(description=\"Entity ID\")\n",
    "    rank: int = Field(description=\"Rank position (1=best)\")\n",
    "    relevance_score: float = Field(description=\"Relevance 0-1\")\n",
    "    justification: str = Field(description=\"Why this candidate is relevant\")\n",
    "\n",
    "class CandidateRanking(BaseModel):\n",
    "    \"\"\"Top candidates ranked by relevance.\"\"\"\n",
    "    top_candidates: list[CandidateRank] = Field(description=\"Top 10 candidates\")\n",
    "\n",
    "class EntityMapping(BaseModel):\n",
    "    \"\"\"Entity mapped to best candidate.\"\"\"\n",
    "    entity_name: str = Field(description=\"Business entity name\")\n",
    "    matched_candidate_id: str = Field(description=\"Mapped candidate ID\")\n",
    "    confidence: float = Field(description=\"Mapping confidence 0-1\")\n",
    "    reasoning: str = Field(description=\"Mapping rationale\")\n",
    "\n",
    "class MappingSuggestions(BaseModel):\n",
    "    \"\"\"All entity-to-candidate mappings.\"\"\"\n",
    "    mappings: list[EntityMapping] = Field(description=\"Entity mappings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb1d96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 2 specialized agents\n",
    "ranking_agent = Agent(\n",
    "    MODEL_NAME,\n",
    "    result_type=CandidateRanking,\n",
    "    system_prompt=\"\"\"You are a data catalog expert.\n",
    "\n",
    "Rank candidates by relevance to business entities:\n",
    "- Consider semantic similarity (name, description)\n",
    "- Evaluate data quality signals (completeness, freshness)\n",
    "- Prioritize candidates with metadata (owner, lineage, tags)\n",
    "- Top 10 most relevant candidates\n",
    "\n",
    "Return structured ranking with scores and justifications.\"\"\"\n",
    ")\n",
    "\n",
    "mapping_agent = Agent(\n",
    "    MODEL_NAME,\n",
    "    result_type=MappingSuggestions,\n",
    "    system_prompt=\"\"\"You are a metadata mapping specialist.\n",
    "\n",
    "Map each business entity to the BEST matching candidate:\n",
    "- 1:1 mapping (one entity â†’ one candidate)\n",
    "- High confidence mappings (>0.7) preferred\n",
    "- Consider semantic alignment and context\n",
    "- Provide clear reasoning for each mapping\n",
    "\n",
    "Return structured mappings with confidence scores.\"\"\"\n",
    ")\n",
    "\n",
    "print(\"âœ… Ranking and mapping agents created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8698e27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def map_entities_to_candidates(entities: list[dict], metadata: dict) -> dict:\n",
    "    \"\"\"Parallel multi-agent entity mapping.\n",
    "\n",
    "    Args:\n",
    "        entities: Business entities from Tool 0\n",
    "        metadata: Technical metadata (Collibra/Unity Catalog)\n",
    "\n",
    "    Returns:\n",
    "        Mapping results with consistency check\n",
    "    \"\"\"\n",
    "    # Prepare prompts for both agents\n",
    "    entities_str = json.dumps(entities, indent=2)\n",
    "    metadata_sample = json.dumps(list(metadata.items())[:20], indent=2)  # Top 20 to avoid overflow\n",
    "\n",
    "    ranking_prompt = f\"\"\"Rank these candidates for the business entities:\n",
    "\n",
    "Entities:\n",
    "{entities_str}\n",
    "\n",
    "Available Metadata:\n",
    "{metadata_sample}\n",
    "\n",
    "Return top 10 candidates ranked by relevance.\"\"\"\n",
    "\n",
    "    mapping_prompt = f\"\"\"Map each business entity to the best matching candidate:\n",
    "\n",
    "Entities:\n",
    "{entities_str}\n",
    "\n",
    "Available Metadata:\n",
    "{metadata_sample}\n",
    "\n",
    "Return 1:1 mappings with confidence scores.\"\"\"\n",
    "\n",
    "    # **KEY PATTERN: Parallel execution with asyncio.gather**\n",
    "    ranking_task = asyncio.create_task(ranking_agent.run(ranking_prompt))\n",
    "    mapping_task = asyncio.create_task(mapping_agent.run(mapping_prompt))\n",
    "\n",
    "    # Wait for both agents to complete (runs in parallel)\n",
    "    ranking_result, mapping_result = await asyncio.gather(\n",
    "        ranking_task,\n",
    "        mapping_task\n",
    "    )\n",
    "\n",
    "    ranked = ranking_result.data\n",
    "    mapped = mapping_result.data\n",
    "\n",
    "    # Consistency check: validate overlap between ranking and mapping\n",
    "    ranked_ids = {c.entity_id for c in ranked.top_candidates}\n",
    "    mapped_ids = {m.matched_candidate_id for m in mapped.mappings}\n",
    "\n",
    "    overlap = ranked_ids & mapped_ids\n",
    "    consistency_ratio = len(overlap) / len(mapped_ids) if mapped_ids else 0.0\n",
    "\n",
    "    return {\n",
    "        \"rankings\": [c.model_dump() for c in ranked.top_candidates],\n",
    "        \"mappings\": [m.model_dump() for m in mapped.mappings],\n",
    "        \"consistency_check\": {\n",
    "            \"overlap_count\": len(overlap),\n",
    "            \"total_mappings\": len(mapped_ids),\n",
    "            \"consistency_ratio\": consistency_ratio,\n",
    "            \"status\": \"PASS\" if consistency_ratio > 0.7 else \"WARN\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "print(\"âœ… Parallel mapping function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6792c185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load input data from DBFS\n",
    "tool0_path = \"/dbfs/FileStore/mcop/tool0_samples/sample_business_request.json\"\n",
    "metadata_path = \"/dbfs/FileStore/mcop/metadata/BA-BS_Datamarts_metadata.json\"\n",
    "\n",
    "with open(tool0_path, \"r\") as f:\n",
    "    tool0_data = json.load(f)\n",
    "\n",
    "with open(metadata_path, \"r\") as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "entities = tool0_data.get(\"entities\", [])\n",
    "print(f\"âœ… Loaded {len(entities)} entities from Tool 0\")\n",
    "print(f\"âœ… Loaded {len(metadata)} metadata items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84d6d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run parallel mapping\n",
    "result = await map_entities_to_candidates(entities, metadata)\n",
    "\n",
    "print(f\"\\nâœ… Parallel mapping complete\")\n",
    "print(f\"   Rankings: {len(result['rankings'])}\")\n",
    "print(f\"   Mappings: {len(result['mappings'])}\")\n",
    "print(f\"   Consistency: {result['consistency_check']['consistency_ratio']:.1%} ({result['consistency_check']['status']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371f37b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to DBFS\n",
    "output_path = \"/dbfs/FileStore/mcop/tool1/filtered_dataset.json\"\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "with open(output_path, \"w\") as f:\n",
    "    json.dump(result, f, indent=2)\n",
    "\n",
    "print(f\"âœ… Results saved: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f523f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample results with consistency check\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PARALLEL MULTI-AGENT MAPPING RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nðŸ† Top 5 Ranked Candidates:\")\n",
    "for candidate in result['rankings'][:5]:\n",
    "    print(f\"   {candidate['rank']}. {candidate['entity_id']} (score: {candidate['relevance_score']:.2f})\")\n",
    "    print(f\"      â†’ {candidate['justification'][:80]}...\")\n",
    "\n",
    "print(f\"\\nðŸ”— Sample Mappings (top 5):\")\n",
    "for mapping in result['mappings'][:5]:\n",
    "    print(f\"   {mapping['entity_name']} â†’ {mapping['matched_candidate_id']}\")\n",
    "    print(f\"      Confidence: {mapping['confidence']:.1%}\")\n",
    "    print(f\"      Reasoning: {mapping['reasoning'][:80]}...\")\n",
    "\n",
    "print(f\"\\nâœ… Consistency Check:\")\n",
    "print(f\"   Overlap: {result['consistency_check']['overlap_count']} / {result['consistency_check']['total_mappings']}\")\n",
    "print(f\"   Ratio: {result['consistency_check']['consistency_ratio']:.1%}\")\n",
    "print(f\"   Status: {result['consistency_check']['status']}\")\n",
    "print(f\"   Expected: >70% for good quality (ranking and mapping agree)\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
