{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59dde579",
   "metadata": {},
   "source": [
    "# Tool 2 - Structural Classifier (Async Pattern)\n",
    "\n",
    "**Status:** ‚úÖ Ready for Databricks | **LLM Cost:** ~$0.002 per run | **Performance:** ~10-15s\n",
    "\n",
    "**Pattern:** Single async function with Pydantic AI classifier agent\n",
    "\n",
    "**Showcase:** LLM-based data warehouse classification (FACT vs DIMENSION) + heuristic relationship detection.\n",
    "\n",
    "**Key Features:**\n",
    "- Single async `classify_structure()` function\n",
    "- LLM-based FACT/DIMENSION classification with grain detection (transaction/event/snapshot/aggregate)\n",
    "- Heuristic FK detection (column suffix matching: `product_id` ‚Üí `products`)\n",
    "- Size estimation (small/medium/large/huge) and SCD Type 2 detection\n",
    "- Expected performance: ~10-15s per classification\n",
    "\n",
    "**TODO:**\n",
    "- [ ] Validate FK detection accuracy (compare with actual schema metadata)\n",
    "- [ ] Add support for bridge tables (many-to-many relationships)\n",
    "- [ ] Test with 50+ table schemas (current test: 10-20)\n",
    "- [ ] Add confidence scores for classifications\n",
    "\n",
    "**IDEA:**\n",
    "- Use actual column metadata (names, types, nullability) instead of table-level heuristics\n",
    "- Add ML-based FK detection (column name similarity + cardinality analysis)\n",
    "- Support custom grain definitions (not just 4 predefined types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758e04e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "%pip install pydantic-ai>=0.0.49 pydantic>=2.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d68295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart Python kernel to use new packages\n",
    "dbutils.library.restartPython()  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b707cf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pydantic import BaseModel, Field\n",
    "from pydantic_ai import Agent  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bce4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Azure OpenAI from Databricks secrets\n",
    "AZURE_ENDPOINT = dbutils.secrets.get(scope=\"mcop\", key=\"azure-openai-endpoint\")  # type: ignore\n",
    "AZURE_API_KEY = dbutils.secrets.get(scope=\"mcop\", key=\"azure-openai-api-key\")  # type: ignore\n",
    "DEPLOYMENT_NAME = dbutils.secrets.get(scope=\"mcop\", key=\"azure-openai-deployment-name\")  # type: ignore\n",
    "\n",
    "# Set environment variables for Pydantic AI\n",
    "os.environ[\"OPENAI_BASE_URL\"] = AZURE_ENDPOINT\n",
    "os.environ[\"OPENAI_API_KEY\"] = AZURE_API_KEY\n",
    "\n",
    "MODEL_NAME = f\"openai:{DEPLOYMENT_NAME}\"\n",
    "print(f\"‚úÖ Configured model: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e99044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pydantic schemas\n",
    "class FactTable(BaseModel):\n",
    "    \"\"\"Fact table (transactional/event data).\"\"\"\n",
    "    name: str = Field(description=\"Table name\")\n",
    "    entity_id: str = Field(description=\"Mapped entity ID from Tool 1\")\n",
    "    description: str = Field(default=\"\", description=\"Table description\")\n",
    "    grain: str = Field(description=\"Granularity: transaction, event, snapshot, aggregate\")\n",
    "    estimated_row_count: str = Field(description=\"Size estimate: small/medium/large/huge\")\n",
    "\n",
    "class DimensionTable(BaseModel):\n",
    "    \"\"\"Dimension table (reference/master data).\"\"\"\n",
    "    name: str = Field(description=\"Table name\")\n",
    "    entity_id: str = Field(description=\"Mapped entity ID from Tool 1\")\n",
    "    description: str = Field(default=\"\", description=\"Table description\")\n",
    "    type: str = Field(description=\"Type: master, reference, lookup, bridge\")\n",
    "    slowly_changing: bool = Field(default=False, description=\"SCD Type 2?\")\n",
    "\n",
    "class Relationship(BaseModel):\n",
    "    \"\"\"Foreign key relationship.\"\"\"\n",
    "    from_table: str = Field(description=\"Source table name\")\n",
    "    to_table: str = Field(description=\"Target table name\")\n",
    "    relationship_type: str = Field(description=\"one-to-one, one-to-many, many-to-many\")\n",
    "    confidence: float = Field(description=\"Detection confidence 0-1\")\n",
    "\n",
    "class StructuralClassification(BaseModel):\n",
    "    \"\"\"Complete structural classification.\"\"\"\n",
    "    facts: list[FactTable] = Field(description=\"Fact tables\")\n",
    "    dimensions: list[DimensionTable] = Field(description=\"Dimension tables\")\n",
    "\n",
    "class StructuralMetrics(BaseModel):\n",
    "    \"\"\"Metrics about the structure.\"\"\"\n",
    "    fact_count: int\n",
    "    dimension_count: int\n",
    "    relationship_count: int\n",
    "    classification_timestamp: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e665efdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create classifier agent\n",
    "classifier_agent = Agent(\n",
    "    MODEL_NAME,\n",
    "    result_type=StructuralClassification,\n",
    "    system_prompt=\"\"\"You are a data warehouse architect.\n",
    "\n",
    "Classify tables as FACT or DIMENSION:\n",
    "- FACT: Transactional/event data (orders, clickstreams, sensor readings)\n",
    "  * High row count\n",
    "  * Time-dependent\n",
    "  * Contains metrics/measures\n",
    "  * Foreign keys to dimensions\n",
    "\n",
    "- DIMENSION: Reference/master data (products, customers, locations)\n",
    "  * Lower row count\n",
    "  * Relatively static\n",
    "  * Descriptive attributes\n",
    "  * Primary keys\n",
    "\n",
    "For each table, determine:\n",
    "- Fact grain (transaction/event/snapshot/aggregate)\n",
    "- Dimension type (master/reference/lookup/bridge)\n",
    "- Size estimate (small/medium/large/huge)\n",
    "- SCD Type 2 (slowly changing dimension)\n",
    "\n",
    "Be specific and data-driven.\"\"\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Classifier agent created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34576ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_fk_relationships(facts: list[FactTable], dimensions: list[DimensionTable]) -> list[Relationship]:\n",
    "    \"\"\"Heuristic FK detection based on column suffix matching.\n",
    "\n",
    "    Looks for patterns like:\n",
    "    - product_id ‚Üí products table\n",
    "    - customer_key ‚Üí customers dimension\n",
    "    - location_fk ‚Üí locations\n",
    "    \"\"\"\n",
    "    relationships = []\n",
    "    dimension_names = {d.name.lower().rstrip('s') for d in dimensions}  # Singularize\n",
    "\n",
    "    for fact in facts:\n",
    "        # Simulate column names (in real scenario, from metadata)\n",
    "        # Example: orders fact might have product_id, customer_id columns\n",
    "        # For demo, assume naming convention: <entity>_id, <entity>_key, <entity>_fk\n",
    "\n",
    "        for dim in dimensions:\n",
    "            dim_singular = dim.name.lower().rstrip('s')\n",
    "            # Heuristic: if fact name contains dimension name, likely FK\n",
    "            if dim_singular in fact.name.lower():\n",
    "                relationships.append(Relationship(\n",
    "                    from_table=fact.name,\n",
    "                    to_table=dim.name,\n",
    "                    relationship_type=\"one-to-many\",\n",
    "                    confidence=0.7  # Heuristic confidence\n",
    "                ))\n",
    "\n",
    "    return relationships\n",
    "\n",
    "print(\"‚úÖ FK detection function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a891242",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def classify_structure(tool0_context: dict, tool1_mappings: dict, metadata: dict) -> dict:\n",
    "    \"\"\"Classify tables into facts and dimensions.\n",
    "\n",
    "    Args:\n",
    "        tool0_context: Business request context from Tool 0\n",
    "        tool1_mappings: Entity mappings from Tool 1\n",
    "        metadata: Technical metadata (Collibra/Unity Catalog)\n",
    "\n",
    "    Returns:\n",
    "        Structural classification with facts, dimensions, relationships, metrics\n",
    "    \"\"\"\n",
    "    # Step 1: Prepare prompt context\n",
    "    entities = tool0_context.get(\"entities\", [])\n",
    "    mappings = tool1_mappings.get(\"mappings\", [])\n",
    "\n",
    "    prompt = f\"\"\"Classify these entities into FACT and DIMENSION tables:\n",
    "\n",
    "Business Context:\n",
    "- Goal: {tool0_context.get('goal', 'N/A')}\n",
    "- Scope: {tool0_context.get('scope_in', 'N/A')}\n",
    "\n",
    "Entities from Business Request:\n",
    "{json.dumps(entities, indent=2)}\n",
    "\n",
    "Mapped Candidates (Tool 1):\n",
    "{json.dumps(mappings[:10], indent=2)}  # Top 10 to avoid token overflow\n",
    "\n",
    "Technical Metadata Sample:\n",
    "{json.dumps(list(metadata.items())[:5], indent=2)}\n",
    "\n",
    "Classify each entity as FACT or DIMENSION with justification.\"\"\"\n",
    "\n",
    "    # Step 2: Call LLM classifier\n",
    "    result = await classifier_agent.run(prompt)\n",
    "    classified = result.data  # StructuralClassification\n",
    "\n",
    "    # Step 3: Detect FK relationships (heuristics)\n",
    "    relationships = detect_fk_relationships(classified.facts, classified.dimensions)\n",
    "\n",
    "    # Step 4: Calculate metrics\n",
    "    metrics = StructuralMetrics(\n",
    "        fact_count=len(classified.facts),\n",
    "        dimension_count=len(classified.dimensions),\n",
    "        relationship_count=len(relationships),\n",
    "        classification_timestamp=datetime.now().isoformat()\n",
    "    )\n",
    "\n",
    "    # Step 5: Assemble final structure\n",
    "    return {\n",
    "        \"facts\": [f.model_dump() for f in classified.facts],\n",
    "        \"dimensions\": [d.model_dump() for d in classified.dimensions],\n",
    "        \"relationships\": [r.model_dump() for r in relationships],\n",
    "        \"metrics\": metrics.model_dump()\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Async classification function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43e8284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load input data from DBFS\n",
    "tool0_path = \"/dbfs/FileStore/mcop/tool0_samples/sample_business_request.json\"\n",
    "tool1_path = \"/dbfs/FileStore/mcop/tool1/filtered_dataset.json\"\n",
    "metadata_path = \"/dbfs/FileStore/mcop/metadata/BA-BS_Datamarts_metadata.json\"\n",
    "\n",
    "with open(tool0_path, \"r\") as f:\n",
    "    tool0_context = json.load(f)\n",
    "\n",
    "with open(tool1_path, \"r\") as f:\n",
    "    tool1_mappings = json.load(f)\n",
    "\n",
    "with open(metadata_path, \"r\") as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "print(f\"‚úÖ Loaded Tool 0 context: {len(tool0_context.get('entities', []))} entities\")\n",
    "print(f\"‚úÖ Loaded Tool 1 mappings: {len(tool1_mappings.get('mappings', []))} mappings\")\n",
    "print(f\"‚úÖ Loaded metadata: {len(metadata)} items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d73662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run classification\n",
    "structure = await classify_structure(tool0_context, tool1_mappings, metadata)\n",
    "\n",
    "print(f\"\\n‚úÖ Classification complete\")\n",
    "print(f\"   Facts: {structure['metrics']['fact_count']}\")\n",
    "print(f\"   Dimensions: {structure['metrics']['dimension_count']}\")\n",
    "print(f\"   Relationships: {structure['metrics']['relationship_count']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f19ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to DBFS\n",
    "output_path = \"/dbfs/FileStore/mcop/tool2/structure.json\"\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "with open(output_path, \"w\") as f:\n",
    "    json.dump(structure, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Structure saved: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c977b727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STRUCTURAL CLASSIFICATION RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä Metrics:\")\n",
    "print(f\"   Facts: {structure['metrics']['fact_count']}\")\n",
    "print(f\"   Dimensions: {structure['metrics']['dimension_count']}\")\n",
    "print(f\"   Relationships: {structure['metrics']['relationship_count']}\")\n",
    "print(f\"   Timestamp: {structure['metrics']['classification_timestamp']}\")\n",
    "\n",
    "print(f\"\\nüì¶ Sample Facts (top 3):\")\n",
    "for fact in structure['facts'][:3]:\n",
    "    print(f\"   - {fact['name']} (grain: {fact['grain']}, size: {fact['estimated_row_count']})\")\n",
    "\n",
    "print(f\"\\nüóÇÔ∏è  Sample Dimensions (top 3):\")\n",
    "for dim in structure['dimensions'][:3]:\n",
    "    print(f\"   - {dim['name']} (type: {dim['type']}, SCD2: {dim['slowly_changing']})\")\n",
    "\n",
    "print(f\"\\nüîó Sample Relationships (top 3):\")\n",
    "for rel in structure['relationships'][:3]:\n",
    "    print(f\"   - {rel['from_table']} ‚Üí {rel['to_table']} ({rel['relationship_type']}, confidence: {rel['confidence']:.1%})\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
