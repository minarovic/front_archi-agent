# Mosaic AI Agent Framework - PodrobnÃ¡ analÃ½za

**Datum:** 2025-11-08
**Kontext:** SrovnÃ¡nÃ­ LangGraph vs Mosaic AI Agent Framework pro MCOP projekt

---

## ğŸ¯ TL;DR - DoporuÄenÃ­ pro MCOP

**HYBRID APPROACH = Best of Both Worlds**

```python
# LangGraph workflow + Mosaic AI deployment (100% Databricks-native)
from langgraph.graph import StateGraph
from databricks import agents
import mlflow

# Custom checkpointer - persistence v Unity Catalog (viz 01_langgraph_analysis.md)
from custom_checkpointers import DeltaLakeCheckpointer

# 1. Build complex workflow in LangGraph
graph = StateGraph(MCOPState)
graph.add_node("tool0_parse", tool0_node)
graph.add_node("tool1_ingest", tool1_node)
graph.add_conditional_edges("tool3_quality", should_retry)  # âœ… Conditionals

# Unity Catalog checkpointer - Å½ÃDNÃ external Postgres!
checkpointer = DeltaLakeCheckpointer(table_name="main.mcop.agent_checkpoints")
compiled_graph = graph.compile(checkpointer=checkpointer, interrupt_before=["human_review"])  # âœ… Human-in-loop

# 2. Wrap do MLflow model
class MCOPAgent(mlflow.pyfunc.PythonModel):
    def load_context(self, context):
        self.graph = compiled_graph
    def predict(self, context, model_input):
        return self.graph.invoke(...)

# 3. Deploy pÅ™es Mosaic AI â†’ automaticky do Unity Catalog!
mlflow.pyfunc.log_model("agent", python_model=MCOPAgent())
agents.deploy(model_name="main.mcop.metadata_agent")  # âœ… Model v UC: main.mcop.metadata_agent
```

**Co zÃ­skÃ¡Å¡ (100% Unity Catalog):**
- âœ… **LangGraph**: Complex workflows, human-in-loop, conditional retry logic
- âœ… **Mosaic AI**: Unity Catalog governance, automatic monitoring, Review App
- âœ… **MLflow**: Automatic tracing vÅ¡ech LangGraph stepÅ¯
- âœ… **Production-ready**: REST API, dashboards, inference tables
- âœ… **Checkpoints v UC**: `main.mcop.agent_checkpoints` Delta table
- âœ… **Agent model v UC**: `main.mcop.metadata_agent` (verzovÃ¡nÃ­, lineage, ACLs)
- âœ… **Å½ÃDNÃ‰ external dependencies**: Postgres âŒ, Redis âŒ

**VÅ¡e v Unity Catalog:**
- Agent model: `main.mcop.metadata_agent`
- Checkpoints: `main.mcop.agent_checkpoints`
- Inference logs: `main.mcop.metadata_agent_inference_table`
- Business data: `main.mcop.*` (Collibra mappings, quality scores)

---

## Co je Mosaic AI Agent Framework?

**Mosaic AI Agent Framework** je **Databricks-native platforma** pro vÃ½voj, deployment a monitoring AI agentÅ¯. Je postavenÃ¡ na **MLflow 3** a tight integrovanÃ¡ s **Unity Catalog**, **Model Serving** a **Agent Evaluation**.

### KlÃ­ÄovÃ© vlastnosti

1. **Library-agnostic**
   - Podporuje LangChain, LangGraph, OpenAI SDK, pure Python
   - WrappujeÅ¡ libovolnÃ½ agent do MLflow Model

2. **MLflow 3 integrace**
   - Automatic tracing (kaÅ¾dÃ½ krok agenta logovanÃ½)
   - Experiment tracking (parametry, metriky, artifacts)
   - Model Registry (verzovÃ¡nÃ­ v Unity Catalog)

3. **Deployment na Model Serving**
   - REST API endpoint s auto-scaling
   - Built-in authentication (Unity Catalog passthrough)
   - Monitoring dashboards (latency, cost, error rates)

4. **Agent Evaluation**
   - LLM-as-judge metrics (relevance, groundedness)
   - Human feedback collection (Review App)
   - Cost & latency tracking

5. **Unity Catalog governance**
   - Model lineage (data â†’ features â†’ agent â†’ deployment)
   - Access control (kdo mÅ¯Å¾e deploy/invoke)
   - Audit log (kdo, kdy, co zmÄ›nil)

---

## Architektura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Agent Code (LangChain/LangGraph/OpenAI/Python) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  MLflow 3 Wrapper    â”‚
        â”‚  - ChatAgent         â”‚
        â”‚  - predict()         â”‚
        â”‚  - Tracing enabled   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Unity Catalog       â”‚
        â”‚  Model Registry      â”‚
        â”‚  - Versioning        â”‚
        â”‚  - Lineage           â”‚
        â”‚  - ACLs              â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Model Serving       â”‚
        â”‚  - REST API          â”‚
        â”‚  - Auto-scaling      â”‚
        â”‚  - Monitoring        â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Inference Tables    â”‚
        â”‚  - Request/Response  â”‚
        â”‚  - Traces            â”‚
        â”‚  - Feedback          â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Persistence & State Management

### RozdÃ­l oproti LangGraph

**LangGraph:**
- ExplicitnÃ­ checkpointers (Postgres, Redis, custom)
- Thread-based conversation memory
- Cross-thread stores

**Mosaic AI:**
- **Stateless by default** (kaÅ¾dÃ½ request je independent)
- State management je na tobÄ›:
  - MÅ¯Å¾eÅ¡ pouÅ¾Ã­t LangGraph checkpointer UVNITÅ˜ agent kÃ³du
  - NEBO custom state storage (Delta table, Redis)

### Jak Å™eÅ¡it persistence v Mosaic AI?

**Option 1: Stateless agent (recommended pro MCOP)**

KaÅ¾dÃ½ request obsahuje vÅ¡echen kontext:
```python
from mlflow.models import ChatAgent, ChatAgentResponses

class MCOPAgent(ChatAgent):
    def predict(self, request):
        # Request obsahuje celÃ½ business request + pÅ™edchozÃ­ kroky
        business_request = request["messages"][-1]["content"]

        # Run Tool 0-7 pipeline
        parsed = tool0_parse(business_request)
        ingested = tool1_ingest(parsed)
        mappings = tool2_structure(ingested)
        quality = tool3_quality(mappings)

        # Return final result
        return ChatAgentResponses(messages=[{
            "role": "assistant",
            "content": json.dumps(quality)
        }])

# Log to Unity Catalog
mlflow.pyfunc.log_model(
    artifact_path="agent",
    python_model=MCOPAgent(),
    registered_model_name="main.mcop.metadata_agent"
)
```

**Pro/Con:**
- âœ… JednoduchÃ©, Å¾Ã¡dnÃ½ external state store
- âœ… Scale-to-zero friendly (Å¾Ã¡dnÃ½ warm state)
- âŒ NemÅ¯Å¾eÅ¡ interrompovat pipeline (human-in-loop sloÅ¾itÄ›jÅ¡Ã­)
- âŒ KaÅ¾dÃ½ request musÃ­ rekalkulovat vÅ¡echno

---

**Option 2: State v Delta table**

UklÃ¡dÃ¡Å¡ mezivÃ½sledky do Unity Catalog:
```python
class MCOPStatefulAgent(ChatAgent):
    def __init__(self):
        self.state_table = "main.mcop.agent_state"

    def predict(self, request):
        thread_id = request.get("thread_id", uuid4())

        # Load previous state
        state = self._load_state(thread_id)

        # Run next step
        if not state.get("parsed"):
            parsed = tool0_parse(request["messages"][-1]["content"])
            state["parsed"] = parsed
            self._save_state(thread_id, state)

        elif not state.get("ingested"):
            ingested = tool1_ingest(state["parsed"])
            state["ingested"] = ingested
            self._save_state(thread_id, state)

        # ... atd

        return ChatAgentResponses(messages=[{
            "role": "assistant",
            "content": f"Completed step: {state.get('current_step')}"
        }])

    def _save_state(self, thread_id, state):
        spark.createDataFrame([{
            "thread_id": thread_id,
            "state_json": json.dumps(state),
            "updated_at": datetime.now()
        }]).write.mode("append").saveAsTable(self.state_table)
```

**Pro/Con:**
- âœ… State je v Unity Catalog (governance)
- âœ… MÅ¯Å¾eÅ¡ resume pipeline
- âŒ Latence (SQL warehouse overhead)
- âŒ MusÃ­Å¡ spravovat state lifecycle (TTL, cleanup)

---

## â­ **Option 3: Embedded LangGraph s DeltaLakeCheckpointer (DOPORUÄŒENO pro MCOP)**

**Tohle je nejlepÅ¡Ã­ Å™eÅ¡enÃ­ pro production MCOP agenta - 100% Unity Catalog native!**

PouÅ¾ijeÅ¡ LangGraph UVNITÅ˜ agent kÃ³du s custom checkpointerem:
```python
from langgraph.graph import StateGraph
from custom_checkpointers import DeltaLakeCheckpointer  # Implementace v 01_langgraph_analysis.md

class MCOPLangGraphAgent(ChatAgent):
    def __init__(self):
        # Build LangGraph
        builder = StateGraph(MCOPState)
        # ... add nodes

        # Unity Catalog checkpointer - Å½ÃDNÃ external Postgres!
        checkpointer = DeltaLakeCheckpointer(
            table_name="main.mcop.agent_checkpoints",
            catalog="main",
            schema="mcop"
        )
        self.graph = builder.compile(checkpointer=checkpointer)

    def predict(self, request):
        thread_id = request.get("thread_id", "default")
        config = {"configurable": {"thread_id": thread_id}}

        # Run graph (uses checkpointer internally)
        result = self.graph.invoke(
            {"business_request_md": request["messages"][-1]["content"]},
            config=config
        )

        return ChatAgentResponses(messages=[{
            "role": "assistant",
            "content": json.dumps(result)
        }])
```

**Pro/Con:**
- âœ…âœ…âœ… **Best of both worlds** (LangGraph flexibility + Mosaic AI deployment)
- âœ… **VÅ¡echny LangGraph features**: human-in-loop, time-travel, conditional edges, subgraphs
- âœ… **VÅ¡echny Mosaic AI benefits**: Unity Catalog governance, automatic monitoring, Review App
- âœ… **MLflow tracing**: Automaticky loguje vÅ¡echny LangGraph node executions
- âœ… **Zero-config deployment**: Jeden pÅ™Ã­kaz `agents.deploy()`
- âœ… **100% Unity Catalog**: Checkpoints v Delta table, Å¾Ã¡dnÃ½ external Postgres/Redis
- âœ… **SQL analytics**: `SELECT * FROM main.mcop.agent_checkpoints WHERE thread_id = '...'`
- âœ… **Lineage tracking**: Agent â†’ checkpoints â†’ business data (vÅ¡e v UC)
- âš ï¸ Custom checkpointer (~100 Å™Ã¡dkÅ¯ kÃ³du, ale reusable)

**ProÄ je tohle ideÃ¡lnÃ­ pro MCOP:**
- Tool 3 quality check mÅ¯Å¾e conditional retry Tool 2 (LangGraph edges)
- Business analyst mÅ¯Å¾e approve kaÅ¾dÃ½ step (interrupt_before)
- Mosaic AI zajistÃ­ governance, monitoring, cost tracking
- **DeltaLakeCheckpointer = vÅ¡e v Unity Catalog (agent model, checkpoints, inference logs, business data)**
- **Å½ÃDNÃ‰ external dependencies** - Postgres âŒ, Redis âŒ, jen Databricks

**Unity Catalog integrace - vÅ¡echno na jednom mÃ­stÄ›:**
```sql
-- VÅ¡e v catalog main.mcop
SHOW TABLES IN main.mcop;
-- metadata_agent (model - Mosaic AI)
-- agent_checkpoints (LangGraph state - DeltaLakeCheckpointer)
-- metadata_agent_inference_table (request/response logs - Mosaic AI)
-- collibra_mappings (business data - Tool 2 output)

-- Lineage query
SELECT
  'Agent: main.mcop.metadata_agent' as component,
  'Reads from: main.mcop.agent_checkpoints' as dependency
UNION ALL
SELECT
  'Agent: main.mcop.metadata_agent',
  'Writes to: main.mcop.metadata_agent_inference_table'
UNION ALL
SELECT
  'Agent: main.mcop.metadata_agent',
  'Writes to: main.mcop.collibra_mappings';
```

---

## ğŸš€ MCOP Hybrid Architecture - KompletnÃ­ pÅ™Ã­klad

**LangGraph workflow + Mosaic AI deployment = Production-ready MCOP agent**

### 1. Build LangGraph workflow

```python
from langgraph.graph import StateGraph, END
from custom_checkpointers import DeltaLakeCheckpointer  # Unity Catalog native
from typing import TypedDict, Annotated
import operator

# State schema
class MCOPState(TypedDict):
    business_request_md: str
    parsed: dict
    metadata: dict
    mappings: dict
    quality: dict
    retry_count: int
    needs_human_review: bool
    human_approved: bool

# Nodes (Tool 0-7)
def tool0_parse(state: MCOPState):
    parsed = parse_business_request(state["business_request_md"])
    return {"parsed": parsed}

def tool1_ingest(state: MCOPState):
    metadata = fetch_metadata(state["parsed"]["entities"])
    return {"metadata": metadata}

def tool2_structure(state: MCOPState):
    mappings = create_mappings(state["metadata"], state["parsed"])
    return {"mappings": mappings}

def tool3_quality(state: MCOPState):
    quality = validate_quality(state["mappings"])
    return {"quality": quality, "retry_count": state.get("retry_count", 0) + 1}

def human_review_node(state: MCOPState):
    # Interrupt here - ÄekÃ¡ na human_approved flag
    return {"needs_human_review": True}

# Conditional edges
def should_retry(state: MCOPState):
    if state["quality"]["score"] < 0.7 and state["retry_count"] < 3:
        return "tool2_structure"  # Retry mapping
    elif state["quality"]["critical_issues"]:
        return "human_review"  # Human approval needed
    else:
        return END

def after_human_review(state: MCOPState):
    if state.get("human_approved"):
        return END
    else:
        return "tool2_structure"  # Human requested changes

# Build graph
builder = StateGraph(MCOPState)
builder.add_node("tool0_parse", tool0_parse)
builder.add_node("tool1_ingest", tool1_ingest)
builder.add_node("tool2_structure", tool2_structure)
builder.add_node("tool3_quality", tool3_quality)
builder.add_node("human_review", human_review_node)

builder.set_entry_point("tool0_parse")
builder.add_edge("tool0_parse", "tool1_ingest")
builder.add_edge("tool1_ingest", "tool2_structure")
builder.add_edge("tool2_structure", "tool3_quality")
builder.add_conditional_edges("tool3_quality", should_retry)
builder.add_conditional_edges("human_review", after_human_review)

# Compile with Unity Catalog checkpointer
checkpointer = DeltaLakeCheckpointer(
    table_name="main.mcop.agent_checkpoints",
    catalog="main",
    schema="mcop"
)

mcop_graph = builder.compile(
    checkpointer=checkpointer,
    interrupt_before=["human_review"]  # âœ… Human-in-loop
)
```

### 2. Wrap do MLflow model

```python
import mlflow
from mlflow.pyfunc import PythonModel

class MCOPLangGraphAgent(PythonModel):
    def load_context(self, context):
        # Load artifacts (graph uÅ¾ mÃ¡Å¡ compiled)
        self.graph = mcop_graph

    def predict(self, context, model_input):
        thread_id = model_input.get("thread_id", str(uuid4()))
        config = {
            "configurable": {"thread_id": thread_id}
        }

        # Check if resuming interrupted workflow
        if model_input.get("human_approved") is not None:
            # Resume after human review
            state = self.graph.get_state(config)
            state.values["human_approved"] = model_input["human_approved"]
            self.graph.update_state(config, state.values)

        # Run graph (nebo resume)
        result = self.graph.invoke(
            {"business_request_md": model_input.get("business_request", "")},
            config=config
        )

        return {
            "thread_id": thread_id,
            "parsed": result.get("parsed"),
            "mappings": result.get("mappings"),
            "quality": result.get("quality"),
            "needs_human_review": result.get("needs_human_review", False)
        }
```

### 3. Log & deploy pÅ™es Mosaic AI

```python
import mlflow
from databricks import agents

mlflow.set_experiment("/Users/minarovic@metawizards.com/mcop-langgraph-hybrid")

# Enable automatic tracing
mlflow.langchain.autolog()  # âœ… Traces all LangGraph nodes

with mlflow.start_run():
    logged_model = mlflow.pyfunc.log_model(
        artifact_path="agent",
        python_model=MCOPLangGraphAgent(),
        registered_model_name="main.mcop.metadata_agent_langgraph",
        pip_requirements=[
            "langgraph>=0.2.0",
            "langchain>=0.3.0",
            "databricks-sdk>=0.73.0",
            # Å½ÃDNÃ psycopg2 - pouÅ¾Ã­vÃ¡me DeltaLakeCheckpointer!
        ],
        signature=mlflow.models.infer_signature(
            model_input={"business_request": "...", "thread_id": "..."},
            model_output={"parsed": {}, "mappings": {}, "quality": {}}
        )
    )

# Deploy to Model Serving â†’ automaticky do Unity Catalog!
deployment = agents.deploy(
    model_name="main.mcop.metadata_agent_langgraph",  # UC: main.mcop.metadata_agent_langgraph
    model_version=logged_model.registered_model_version,
    scale_to_zero_enabled=True,
    environment_vars={
        # Å½ÃDNÃ Postgres URI - DeltaLakeCheckpointer pouÅ¾Ã­vÃ¡ Databricks natively!
        "AZURE_OPENAI_ENDPOINT": "{{secrets/mcop/azure-endpoint}}",
        "AZURE_OPENAI_API_KEY": "{{secrets/mcop/azure-key}}"
    }
)

print(f"âœ… Hybrid agent deployed: {deployment.endpoint_url}")
print(f"âœ… Review App: {deployment.review_app_url}")
print(f"âœ… Monitoring: Databricks ML > Serving > main-mcop-metadata_agent_langgraph")
print(f"âœ… Unity Catalog model: main.mcop.metadata_agent_langgraph")
print(f"âœ… Checkpoints table: main.mcop.agent_checkpoints")
```

### 4. Invoke s human-in-loop workflow

```python
import requests

# Initial request
response = requests.post(
    deployment.endpoint_url,
    headers={"Authorization": f"Bearer {os.getenv('DATABRICKS_TOKEN')}"},
    json={
        "business_request": open("data/sample_request.md").read(),
        "thread_id": "req-2025-11-08-001"
    }
)

result = response.json()

# Check if human review needed
if result["needs_human_review"]:
    print("â¸ï¸  Workflow paused - human approval required")
    print(f"Thread ID: {result['thread_id']}")
    print(f"Quality issues: {result['quality']['critical_issues']}")

    # Business analyst reviews in Review App or via API
    # ...

    # Resume with approval
    resume_response = requests.post(
        deployment.endpoint_url,
        headers={"Authorization": f"Bearer {os.getenv('DATABRICKS_TOKEN')}"},
        json={
            "thread_id": "req-2025-11-08-001",
            "human_approved": True  # âœ… Approved
        }
    )

    final_result = resume_response.json()
    print("âœ… Workflow completed after human approval")
else:
    print("âœ… Workflow completed without human intervention")
```

### 5. Co zÃ­skÃ¡Å¡ tÃ­mto pÅ™Ã­stupem?

**LangGraph benefits:**
- âœ… Conditional retry logic (Tool 3 â†’ retry Tool 2)
- âœ… Human-in-loop breaks (interrupt_before)
- âœ… State persistence (PostgresSaver nebo DeltaLakeCheckpointer)
- âœ… Time-travel debugging (get_state, update_state)
- âœ… Multi-step workflows s branching

**Mosaic AI benefits:**
- âœ… Zero-config deployment (`agents.deploy()`)
- âœ… Unity Catalog governance (lineage, ACLs)
- âœ… Automatic monitoring dashboards
- âœ… Inference tables (request/response logging)
- âœ… Review App (stakeholder feedback)
- âœ… MLflow tracing (vÅ¡echny LangGraph nodes viditelnÃ©)
- âœ… Cost tracking (token usage per request)

**Production monitoring:**
```sql
-- AnalÃ½za success rate v MLflow
SELECT
  DATE(timestamp) as date,
  COUNT(*) as total_requests,
  SUM(CASE WHEN json_extract(output, '$.needs_human_review') = 'true' THEN 1 ELSE 0 END) as human_review_needed,
  AVG(CAST(json_extract(output, '$.quality.score') AS DOUBLE)) as avg_quality_score,
  AVG(latency_ms) as avg_latency_ms
FROM main.mcop.metadata_agent_langgraph_inference_table
WHERE timestamp >= current_date() - INTERVAL 7 DAYS
GROUP BY date
ORDER BY date
```

---

## SilnÃ© strÃ¡nky Mosaic AI

### 1. **Zero-config deployment**

Jedno volÃ¡nÃ­ `agents.deploy()`:
```python
from databricks import agents

deployment = agents.deploy(
    model_name="main.mcop.metadata_agent",
    model_version=1,
    scale_to_zero_enabled=True
)

# âœ… REST API endpoint ready
# âœ… Monitoring dashboards enabled
# âœ… Review App available
```

Oproti LangGraph (manual setup):
- MLflow model packaging
- Serving endpoint creation
- Authentication config
- Monitoring setup

---

### 2. **Built-in Review App**

AutomatickÃ½ chat UI pro stakeholder feedback:
```python
deployment = agents.deploy(..., enable_review_app=True)

# Stakeholders mÅ¯Å¾ou chatovat s agentem
# Thumbs up/down feedback automaticky logovÃ¡n
# LLM judges vyhodnotÃ­ kvalitu odpovÄ›dÃ­
```

**Use case pro MCOP:**
- Business analyst testuje parsed requests
- Data steward validuje entity mappings
- Feedback jde do MLflow â†’ iterative improvement

---

### 3. **Agent Evaluation framework**

```python
import mlflow

# Evaluation set (ground truth)
eval_data = pd.DataFrame([
    {"request": "...", "expected_entities": ["Customer", "Order"]},
    # ...
])

# Run evaluation
with mlflow.start_run():
    results = mlflow.evaluate(
        model="main.mcop.metadata_agent",
        data=eval_data,
        model_type="question-answering",
        evaluators=["default"],  # Includes LLM-as-judge
    )

# Metrics logged to MLflow
# - Relevance score
# - Groundedness
# - Token usage
# - Latency
```

**Pro/Con:**
- âœ… Out-of-box metrics
- âœ… LLM judges (no manual labeling)
- âœ… Cost tracking (token usage)
- âŒ MÃ©nÄ› flexibilnÃ­ neÅ¾ custom evaluace

---

### 4. **Unity Catalog governance**

KaÅ¾dÃ½ agent model v UC mÃ¡:
- **Lineage:** KterÃ© datasety/features pouÅ¾il
- **Access control:** Kdo mÅ¯Å¾e deploy/invoke
- **Audit log:** Historie zmÄ›n (kdo, kdy, proÄ)

```python
# Model je v UC
model_fqn = "main.mcop.metadata_agent"

# Access control
spark.sql(f"GRANT USE SCHEMA main.mcop TO `data-analysts`")
spark.sql(f"GRANT EXECUTE ON FUNCTION {model_fqn} TO `business-users`")

# Lineage (automatic)
# Agent â†’ Unity Catalog tables â†’ Collibra API â†’ SAP HANA
```

---

### 5. **MLflow Tracing (automatic)**

KaÅ¾dÃ½ krok agenta je automaticky tracovÃ¡n:
```python
import mlflow

mlflow.langchain.autolog()  # Pro LangChain/LangGraph
# NEBO
mlflow.openai.autolog()     # Pro OpenAI SDK

# Traces obsahujÃ­:
# - Input/output kaÅ¾dÃ©ho LLM call
# - Tool calls (function names + args)
# - Latency per step
# - Token usage per step
```

**Tracing UI v MLflow:**
- VidÃ­Å¡ celÃ½ execution tree
- MÅ¯Å¾eÅ¡ kliknout na kaÅ¾dÃ½ span â†’ vidÃ­Å¡ prompt/response
- FiltrujeÅ¡ slow requests, high-cost requests

---

### 6. **Production monitoring (Inference Tables)**

KaÅ¾dÃ½ request/response automaticky logovÃ¡n do Delta table:
```python
# Databricks creates inference table automatically
# main.mcop.metadata_agent_inference_table

# Contains:
# - request_id, timestamp
# - input (user message)
# - output (agent response)
# - trace_id (link to MLflow trace)
# - latency, token_count
```

MÅ¯Å¾eÅ¡ analyzovat:
```sql
SELECT
  DATE(timestamp) as date,
  AVG(latency_ms) as avg_latency,
  SUM(token_count) as total_tokens,
  COUNT(*) as request_count
FROM main.mcop.metadata_agent_inference_table
GROUP BY date
ORDER BY date DESC
```

---

### 7. **AI Gateway integration**

Centralized LLM proxy s governance:
```python
from databricks.ai_gateway import AIGateway

# VÅ¡echny LLM calls jdou pÅ™es Gateway
gateway = AIGateway()

# Benefits:
# - Rate limiting
# - Cost tracking per user/department
# - PII redaction
# - Request logging
```

---

## SlabÃ© strÃ¡nky Mosaic AI

### 1. **Vendor lock-in**

- Tight coupled s Databricks (Model Serving, Unity Catalog)
- Migrace mimo Databricks = rewrite deployment layer

### 2. **MÃ©nÄ› flexibilnÃ­ orchestrace**

- NenÃ­ nativnÃ­ graph-based workflow
- Conditionals/loops musÃ­Å¡ Å™eÅ¡it v Python kÃ³du (ne jako LangGraph edges)

### 3. **State management nenÃ­ built-in**

- Pokud potÅ™ebujeÅ¡ stateful agent, musÃ­Å¡ Å™eÅ¡it persistence sÃ¡m
- LangGraph mÃ¡ checkpointers out-of-box

### 4. **Human-in-the-loop sloÅ¾itÄ›jÅ¡Ã­**

- NenÃ­ nativnÃ­ interrupt mechanismus jako LangGraph
- MusÃ­Å¡ implementovat custom workflow:
  - Agent vrÃ¡tÃ­ "needs_approval" status
  - Frontend poÅ¡le approval request
  - Agent pokraÄuje

### 5. **OmezenÃ½ multi-agent support**

- MÅ¯Å¾eÅ¡ mÃ­t vÃ­ce agentÅ¯, ale orchestrace je manuÃ¡lnÃ­
- LangGraph mÃ¡ subgraphs, delegation patterns built-in

---

## Mosaic AI pro MCOP - Use case analÃ½za

### Kdy pouÅ¾Ã­t Mosaic AI?

âœ… **Ano, kdyÅ¾:**

1. **Fast time-to-market je priorita**
   - Out-of-box deployment, monitoring, evaluation
   - MÃ©nÄ› boilerplate neÅ¾ LangGraph

2. **Unity Catalog governance je must-have**
   - Lineage tracking (agent â†’ data â†’ Collibra)
   - Access control (kdo mÅ¯Å¾e invoke MCOP)
   - Audit log pro compliance

3. **ChceÅ¡ built-in Review App**
   - Business analyst/data steward feedback collection
   - Thumbs up/down + comments â†’ iterative improvement

4. **Cost/latency monitoring je kritickÃ©**
   - Automatic token counting
   - Latency per request v dashboards

5. **TÃ½m nemÃ¡ zkuÅ¡enosti s complex orchestration**
   - Simple function-calling model (predict())
   - NenÃ­ nutnÃ© myslet v grafech

6. **Databricks je long-term platforma**
   - Å½Ã¡dnÃ© plÃ¡ny na migraci mimo Azure Databricks

---

âŒ **Ne, kdyÅ¾:**

1. **PotÅ™ebujeÅ¡ complex workflow s conditionals/loops**
   - Tool 3 quality check â†’ retry Tool 2 s jinÃ½m promptem
   - LangGraph je lepÅ¡Ã­ pro tyto scÃ©nÃ¡Å™e

2. **Human-in-the-loop je mandatory**
   - Business analyst musÃ­ approve kaÅ¾dÃ½ step
   - LangGraph mÃ¡ interrupt built-in

3. **Multi-agent orchestration je Å¾Ã¡doucÃ­**
   - KaÅ¾dÃ½ tool je samostatnÃ½ specialist agent
   - Coordinator deleguje tasks

4. **Vendor neutralita je priorita**
   - MoÅ¾nost bÄ›hu na AWS/GCP/on-prem
   - Mosaic AI je Databricks-only

5. **PotÅ™ebujeÅ¡ custom evaluation logic**
   - SloÅ¾itÄ›jÅ¡Ã­ neÅ¾ LLM-as-judge metrics
   - Custom eval pipeline mimo Databricks

---

## PÅ™Ã­klad MCOP architektury s Mosaic AI

### Agent code (simple stateless)

```python
from mlflow.models import ChatAgent, ChatAgentResponses
from databricks.sdk import WorkspaceClient
import json

class MCOPAgent(ChatAgent):
    """Stateless MCOP agent - kaÅ¾dÃ½ request je independent."""

    def __init__(self):
        self.w = WorkspaceClient()

    def predict(self, request):
        # Extract business request from message
        user_message = request["messages"][-1]["content"]

        # Tool 0: Parse business request
        parsed = self._tool0_parse(user_message)

        # Tool 1: Ingest metadata
        metadata = self._tool1_ingest(parsed["entities"])

        # Tool 2: Structure mappings
        mappings = self._tool2_structure(metadata, parsed)

        # Tool 3: Quality check
        quality = self._tool3_quality(mappings)

        # Return final result
        return ChatAgentResponses(messages=[{
            "role": "assistant",
            "content": json.dumps({
                "parsed_request": parsed,
                "entity_mappings": mappings,
                "quality_score": quality["score"],
                "quality_issues": quality["issues"]
            })
        }])

    def _tool0_parse(self, md_text):
        # Call Azure OpenAI to parse
        from openai import OpenAI
        client = OpenAI(base_url=os.getenv("AZURE_OPENAI_ENDPOINT"), ...)
        # ... parsing logic
        return parsed_dict

    def _tool1_ingest(self, entities):
        # Fetch from Collibra, Databricks UC, SAP
        collibra_assets = fetch_collibra(entities)
        uc_tables = self.w.tables.list(catalog_name="main", schema_name="prod")
        return {"collibra": collibra_assets, "databricks": list(uc_tables)}

    def _tool2_structure(self, metadata, parsed):
        # LLM-based mapping
        # ...
        return mappings

    def _tool3_quality(self, mappings):
        # Quality validation
        score = validate_completeness(mappings)
        issues = find_conflicts(mappings)
        return {"score": score, "issues": issues}
```

### Logging & deployment

```python
import mlflow
from databricks import agents

# Set experiment
mlflow.set_experiment("/Users/minarovic@metawizards.com/mcop-agent")

# Log model
with mlflow.start_run():
    # Enable tracing
    mlflow.openai.autolog()

    # Log agent
    logged_agent = mlflow.pyfunc.log_model(
        artifact_path="agent",
        python_model=MCOPAgent(),
        registered_model_name="main.mcop.metadata_agent",
        pip_requirements=[
            "databricks-sdk>=0.73.0",
            "openai>=1.0.0",
            "requests>=2.31.0"
        ]
    )

# Deploy to serving endpoint
deployment = agents.deploy(
    model_name="main.mcop.metadata_agent",
    model_version=logged_agent.registered_model_version,
    scale_to_zero_enabled=True,
    environment_vars={
        "AZURE_OPENAI_ENDPOINT": "{{secrets/mcop/azure-endpoint}}",
        "AZURE_OPENAI_API_KEY": "{{secrets/mcop/azure-key}}"
    }
)

print(f"Agent deployed: {deployment.endpoint_url}")
# https://<workspace>/serving-endpoints/main-mcop-metadata_agent-1/invocations
```

### Invoke via API

```python
import requests

response = requests.post(
    deployment.endpoint_url,
    headers={"Authorization": f"Bearer {os.getenv('DATABRICKS_TOKEN')}"},
    json={
        "messages": [
            {"role": "user", "content": open("data/sample_request.md").read()}
        ]
    }
)

result = response.json()
print(result["choices"][0]["message"]["content"])
```

---

### Stateful variant (s Delta state table)

```python
class MCOPStatefulAgent(ChatAgent):
    def __init__(self):
        self.state_table = "main.mcop.agent_state"
        self._init_state_table()

    def _init_state_table(self):
        spark = SparkSession.builder.getOrCreate()
        spark.sql(f"""
            CREATE TABLE IF NOT EXISTS {self.state_table} (
                thread_id STRING,
                step STRING,
                state_json STRING,
                updated_at TIMESTAMP
            ) USING DELTA
            PARTITIONED BY (thread_id)
        """)

    def predict(self, request):
        thread_id = request.get("thread_id", str(uuid4()))

        # Load current state
        state = self._load_state(thread_id)

        # Determine next step
        if not state.get("parsed"):
            parsed = self._tool0_parse(request["messages"][-1]["content"])
            state["parsed"] = parsed
            state["current_step"] = "tool0_completed"
            self._save_state(thread_id, state)

            return ChatAgentResponses(messages=[{
                "role": "assistant",
                "content": f"Step 1/4 completed: Business request parsed. Continue to ingest metadata."
            }])

        elif not state.get("ingested"):
            ingested = self._tool1_ingest(state["parsed"]["entities"])
            state["ingested"] = ingested
            state["current_step"] = "tool1_completed"
            self._save_state(thread_id, state)

            return ChatAgentResponses(messages=[{
                "role": "assistant",
                "content": f"Step 2/4 completed: Metadata ingested ({len(ingested['collibra'])} Collibra assets). Continue to structure."
            }])

        # ... tool2, tool3

        else:
            # All steps done
            return ChatAgentResponses(messages=[{
                "role": "assistant",
                "content": json.dumps(state["final_result"])
            }])

    def _load_state(self, thread_id):
        spark = SparkSession.builder.getOrCreate()
        result = spark.sql(f"""
            SELECT state_json FROM {self.state_table}
            WHERE thread_id = '{thread_id}'
            ORDER BY updated_at DESC LIMIT 1
        """).collect()

        return json.loads(result[0]["state_json"]) if result else {}

    def _save_state(self, thread_id, state):
        spark = SparkSession.builder.getOrCreate()
        spark.createDataFrame([{
            "thread_id": thread_id,
            "step": state.get("current_step", "unknown"),
            "state_json": json.dumps(state),
            "updated_at": datetime.now()
        }]).write.mode("append").saveAsTable(self.state_table)
```

**PouÅ¾itÃ­:**
```python
# First request
response1 = invoke_agent({
    "thread_id": "req-2025-11-08-001",
    "messages": [{"role": "user", "content": "Parse business request..."}]
})
# â†’ "Step 1/4 completed"

# Second request (same thread_id)
response2 = invoke_agent({
    "thread_id": "req-2025-11-08-001",
    "messages": [{"role": "user", "content": "Continue"}]
})
# â†’ "Step 2/4 completed"
```

---

## Evaluation workflow

```python
import mlflow
import pandas as pd

# Evaluation dataset
eval_data = pd.DataFrame([
    {
        "request": "Create metadata for Customer entity from SAP CRM...",
        "expected_entities": ["Customer", "Address", "ContactInfo"],
        "expected_quality_score": 0.9
    },
    # ... more examples
])

# Run evaluation
with mlflow.start_run():
    results = mlflow.evaluate(
        model=f"models:/main.mcop.metadata_agent/1",
        data=eval_data,
        model_type="question-answering",
        evaluators=["default"],  # LLM-as-judge
        extra_metrics=[
            mlflow.metrics.latency(),
            mlflow.metrics.token_count(),
        ]
    )

# View results
print(results.metrics)
# {
#   "relevance/v1/mean": 0.85,
#   "groundedness/v1/mean": 0.92,
#   "latency/mean": 3.2,  # seconds
#   "token_count/mean": 1500
# }
```

---

## Monitoring & alerting

```python
# Inference table je auto-created
inference_table = "main.mcop.metadata_agent_inference_table"

# AnalÃ½za kvality v Äase
spark.sql(f"""
SELECT
  DATE(timestamp) as date,
  AVG(CAST(json_extract(output, '$.quality_score') AS DOUBLE)) as avg_quality,
  COUNT(*) as request_count,
  AVG(latency_ms) as avg_latency_ms
FROM {inference_table}
WHERE timestamp >= current_date() - INTERVAL 7 DAYS
GROUP BY date
ORDER BY date
""").display()

# Alert na quality degradation
alert_query = f"""
SELECT * FROM {inference_table}
WHERE timestamp >= current_timestamp() - INTERVAL 1 HOUR
  AND CAST(json_extract(output, '$.quality_score') AS DOUBLE) < 0.7
"""

# Create Databricks SQL alert
# (via UI nebo API)
```

---

## Summary - Mosaic AI pro MCOP

| Aspekt                        | HodnocenÃ­ | PoznÃ¡mka                                       |
| ----------------------------- | --------- | ---------------------------------------------- |
| **Workflow flexibilita**      | â­â­â­       | OK pro simple pipelines, limited conditionals  |
| **Persistence na Databricks** | â­â­â­â­      | Delta table integration moÅ¾nÃ¡ (custom code)    |
| **Developer experience**      | â­â­â­â­â­     | Minimal boilerplate, simple predict() model    |
| **Governance & lineage**      | â­â­â­â­â­     | NativnÃ­ Unity Catalog, automatic lineage       |
| **Monitoring & debugging**    | â­â­â­â­â­     | Built-in dashboards, inference tables, tracing |
| **Human-in-the-loop**         | â­â­â­       | MoÅ¾nÃ©, ale nenÃ­ native (custom workflow)       |
| **Multi-agent support**       | â­â­â­       | MÅ¯Å¾eÅ¡ mÃ­t N agentÅ¯, ale orchestrace je manual  |
| **Time-to-market**            | â­â­â­â­â­     | Fastest - deploy(), monitoring, eval included  |
| **Vendor lock-in**            | â­â­        | Databricks-only, migrace = rewrite             |

---

## ğŸ¯ FinÃ¡lnÃ­ doporuÄenÃ­ pro MCOP

### â­ **DoporuÄenÃ½ pÅ™Ã­stup: LangGraph + Mosaic AI Hybrid**

**PouÅ¾ij tento approach pokud:**

1. âœ… **PotÅ™ebujeÅ¡ complex workflows**
   - Tool 3 quality check â†’ conditional retry Tool 2 s jinÃ½m promptem
   - Human approval breaks (business analyst review)
   - Multi-step conditional logic

2. âœ… **Unity Catalog governance je must-have**
   - Lineage tracking (agent â†’ Collibra â†’ UC tables)
   - Access control (kdo mÅ¯Å¾e invoke/deploy)
   - Audit log pro compliance

3. âœ… **ChceÅ¡ production-ready deployment rychle**
   - Zero-config deployment pÅ™es `agents.deploy()`
   - Out-of-box monitoring, Review App, inference tables
   - MLflow tracing vÅ¡ech LangGraph stepÅ¯

4. âœ… **Databricks je long-term platforma**
   - Å½Ã¡dnÃ© plÃ¡ny na migraci mimo Azure Databricks
   - Custom DeltaLakeCheckpointer = vÅ¡e v Unity Catalog

**Implementace:**
- LangGraph pro workflow orchestration (StateGraph, conditional edges)
- PostgresSaver nebo custom DeltaLakeCheckpointer pro persistence
- MLflow wrapper (PythonModel)
- Mosaic AI deployment (`agents.deploy()`)

---

### Option B: Pure Mosaic AI (stateless)

**PouÅ¾ij pokud:**

1. **Tool 0-7 pipeline je ÄistÄ› lineÃ¡rnÃ­**
   - Å½Ã¡dnÃ© conditional retries
   - Å½Ã¡dnÃ½ human-in-loop
   - Simple sequential execution

2. **Fast MVP je priorita #1**
   - ChceÅ¡ demo za tÃ½den
   - Minimal boilerplate code

3. **TÃ½m nemÃ¡ zkuÅ¡enosti s LangGraph**
   - Simple predict() model je intuitivnÃ­
   - NenÃ­ Äas uÄit se grafy

**OmezenÃ­:**
- âŒ NemÅ¯Å¾eÅ¡ interrupt pipeline (Å¾Ã¡dnÃ½ human-in-loop)
- âŒ NemÅ¯Å¾eÅ¡ conditional retry (Tool 3 â†’ Tool 2)
- âŒ KaÅ¾dÃ½ request musÃ­ rekalkulovat vÅ¡echno

---

### Option C: Pure LangGraph (mimo Databricks)

**PouÅ¾ij pokud:**

1. **Vendor neutrality je priorita**
   - MoÅ¾nost bÄ›hu na AWS/GCP/on-prem
   - Databricks nenÃ­ long-term platforma

2. **Custom deployment pipeline existuje**
   - VlastnÃ­ Kubernetes cluster
   - Custom monitoring stack

**NevÃ½hody:**
- âŒ MusÃ­Å¡ buildnout vlastnÃ­ deployment (FastAPI, Docker, K8s)
- âŒ Å½Ã¡dnÃ¡ Unity Catalog governance
- âŒ MusÃ­Å¡ implementovat vlastnÃ­ monitoring/tracing

---

**Next:** Side-by-side srovnÃ¡nÃ­ a finÃ¡lnÃ­ doporuÄenÃ­ â†’ `03_comparison_summary.md`
