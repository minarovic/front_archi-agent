# Metadata Copilot (MCOP) - Detailn√≠ Architektura & Workflow

**Verze:** 1.0.0 (MVP)
**Datum:** 2 listopad 2025
**Autor:** MCOP Development Team

---

## üìã Obsah

1. [P≈ôehled syst√©mu](#1-p≈ôehled-syst√©mu)
2. [Vysoko√∫rov≈àov√° architektura](#2-vysoko√∫rov≈àov√°-architektura)
3. [Tool 0 - Business Request Parser](#3-tool-0---business-request-parser)
4. [Tool 1 - Data Ingest & Filtering](#4-tool-1---data-ingest--filtering)
5. [Tool 2 - Structural Analysis](#5-tool-2---structural-analysis)
6. [Tool 3 - Quality Validator](#6-tool-3---quality-validator)
7. [Tool 7 - Governance Report Generator](#7-tool-7---governance-report-generator)
8. [Orchestrator - LangGraph MVP Workflow](#8-orchestrator---langgraph-mvp-workflow)
9. [Datov√© toky a form√°ty](#9-datov√©-toky-a-form√°ty)
10. [Deployment & Runtime](#10-deployment--runtime)

---

## 1. P≈ôehled syst√©mu

### 1.1 √öƒçel

Metadata Copilot (MCOP) je LangGraph-based AI agent pro **automatizovanou anal√Ωzu datov√Ωch katalog≈Ø**. Propojuje **business po≈æadavky** (co ≈æadatel pot≈ôebuje) s **technickou realitou** (co existuje v Collibra/Databricks/SAP metadatech).

### 1.2 Kl√≠ƒçov√© funkce MVP

- ‚úÖ Parsov√°n√≠ standardizovan√Ωch business dokument≈Ø (Tool 0)
- ‚úÖ Ingest & filtrov√°n√≠ metadat podle rozsahu (Tool 1)
- ‚úÖ Klasifikace fakt≈Ø/dimenz√≠ + detekce vztah≈Ø (Tool 2)
- ‚úÖ Validace kvality metadat (Tool 3)
- ‚úÖ Generov√°n√≠ governance report≈Ø (Tool 7)

### 1.3 Hlavn√≠ v√Ωstupy

| Artefakt                | Form√°t   | √öƒçel                                                  |
| ----------------------- | -------- | ----------------------------------------------------- |
| `structure.json`        | JSON     | Strukturovan√° reprezentace fakt≈Ø, dimenz√≠, hierarchi√≠ |
| `governance_report.md`  | Markdown | Soulad/nesoulad s business po≈æadavky, kvalita metadat |
| `filtered_dataset.json` | JSON     | Entity-to-candidate mappings (meziprodukt Tool 1)     |

---

## 2. Vysoko√∫rov≈àov√° architektura

### 2.1 Celkov√Ω syst√©mov√Ω diagram

```mermaid
graph TB
    subgraph Inputs["üì• Vstupy"]
        BR[Business Request<br/>Markdown dokument]
        MD[Metadata Exports<br/>Collibra/Databricks/SAP<br/>JSON/XML]
    end

    subgraph MCOP["ü§ñ Metadata Copilot MVP"]
        T0[Tool 0<br/>Business Parser]
        T1[Tool 1<br/>Data Ingest]
        T2[Tool 2<br/>Structural Analysis]
        T3[Tool 3<br/>Quality Validator]
        T7[Tool 7<br/>Governance Report]
        ORCH[LangGraph Orchestrator<br/>5-node workflow]
    end

    subgraph Outputs["üì§ V√Ωstupy"]
        STRUCT[structure.json]
        GOVERN[governance_report.md]
        FILT[filtered_dataset.json]
        AUDIT[Audit artifacts<br/>scrum/artifacts/]
    end

    BR --> T0
    MD --> T1
    T0 --> ORCH
    T1 --> ORCH
    ORCH --> T2
    ORCH --> T3
    T2 --> T7
    T3 --> T7
    T7 --> GOVERN
    T2 --> STRUCT
    T1 --> FILT
    ORCH --> AUDIT

    style MCOP fill:#e3f2fd,color:#000
    style Inputs fill:#fff3e0,color:#000
    style Outputs fill:#e8f5e9,color:#000
```

### 2.2 Komponenty podle typu

| Typ              | N√°stroj      | Technologie              | LLM?                    |
| ---------------- | ------------ | ------------------------ | ----------------------- |
| **Parser**       | Tool 0       | OpenAI SDK + Pydantic    | ‚úÖ Ano (test-gpt-5-mini) |
| **ETL**          | Tool 1       | LangGraph (5 nodes)      | ‚úÖ Ano (2x LLM nodes)    |
| **Classifier**   | Tool 2       | LangGraph (5 nodes)      | ‚úÖ Ano (1x LLM node)     |
| **Validator**    | Tool 3       | Python (deterministick√Ω) | ‚ùå Ne                    |
| **Generator**    | Tool 7       | LangChain + RAG          | ‚úÖ Ano (test-gpt-5-mini) |
| **Orchestrator** | MVP Workflow | LangGraph StateGraph     | ‚öôÔ∏è Mixed                 |

> **Azure konfigurace LLM:** V≈°echny LLM uzly bƒõ≈æ√≠ p≈ôes Azure OpenAI deployment `test-gpt-5-mini`. Nasazen√≠ vy≈æaduje promƒõnn√© prost≈ôed√≠ `AZURE_OPENAI_ENDPOINT`, `AZURE_OPENAI_API_KEY`, `AZURE_OPENAI_DEPLOYMENT_NAME` a `AZURE_OPENAI_API_VERSION` naƒçten√© z `.env`.

---

## 3. Tool 0 - Business Request Parser

### 3.1 √öƒçel

Parsuje standardizovan√© Markdown business dokumenty a extrahuje strukturovan√Ω kontext (entities, c√≠le, scope_in/out).

### 3.2 Architektura

```mermaid
flowchart LR
    INPUT[business_request.md] --> LOAD[Load Document]
    LOAD --> LLM[LLM Parser<br/>OpenAI SDK<br/>test-gpt-5-mini]
    LLM --> SCHEMA[Pydantic Schema<br/>BusinessRequest]
    SCHEMA --> OUTPUT[business_context.json]

    style LLM fill:#ffccbc,color:#000
    style SCHEMA fill:#c5cae9,color:#000
```

### 3.3 Input Schema (Markdown)

```markdown
# Po≈æadavek na datov√Ω projekt

**Metadata:**
- Projekt: Anal√Ωza n√°kupu
- Zadavatel: Jan Nov√°k
- Datum: 2025-11-01

## C√≠l projektu
Analyzovat n√°kupn√≠ objedn√°vky za Q3 2025...

## Rozsah (Scope In)
- BS N√°kup (dm_bs_purchase)
- Dodavatel√©, objedn√°vky, polo≈æky

## Mimo rozsah (Scope Out)
- HR data, Real-time monitoring

## Kl√≠ƒçov√© entity
- Dodavatel√© (Suppliers)
- N√°kupn√≠ objedn√°vky (Purchase Orders)
- Materi√°ly (Materials)
```

### 3.4 Output Schema (Pydantic)

```python
class ProjectMetadata(BaseModel):
    project_name: str = Field(description="N√°zev projektu")
    sponsor: str = Field(description="Zadavatel")
    submitted_at: str = Field(description="Datum v ISO 8601")

class BusinessRequest(BaseModel):
    """Parsovan√Ω business po≈æadavek."""
    project_metadata: ProjectMetadata
    goal: str = Field(description="C√≠l projektu")
    scope_in: str = Field(description="Co je souƒç√°st√≠")
    scope_out: str = Field(description="Co nen√≠ souƒç√°st√≠")
    entities: list[str] = Field(description="Kl√≠ƒçov√© entity")
    metrics: list[str] = Field(description="Po≈æadovan√© metriky")
    sources: list[str] = Field(description="Oƒçek√°van√© zdroje")
    constraints: list[str] = Field(description="Omezen√≠")
    deliverables: list[str] = Field(description="Oƒçek√°van√© dod√°vky")
```

### 3.5 Output Example

```json
{
  "project_metadata": {
    "project_name": "Anal√Ωza n√°kupu Q3 2025",
    "sponsor": "Jan Nov√°k",
    "submitted_at": "2025-11-01T00:00:00Z"
  },
  "goal": "Analyzovat n√°kupn√≠ objedn√°vky za Q3 2025 pro optimalizaci dodavatelsk√©ho portfolia",
  "scope_in": "BS N√°kup (dm_bs_purchase), Dodavatel√©, Objedn√°vky",
  "scope_out": "HR data, Real-time monitoring, Finance",
  "entities": ["Suppliers", "Purchase Orders", "Materials"],
  "metrics": ["order_quantity", "order_value", "delivery_time"],
  "sources": ["SAP", "Databricks Unity Catalog"],
  "constraints": ["Pouze Q3 2025", "Bez PII dat"],
  "deliverables": ["Power BI dashboard", "SQL skripty"]
}
```

---

## 4. Tool 1 - Data Ingest & Filtering

### 4.1 √öƒçel

Naƒçte metadata exporty (Collibra/Databricks), filtruje podle business kontextu a vytvo≈ô√≠ entity-to-candidate mappings.

### 4.2 LangGraph Architecture (5 nodes)

```mermaid
graph LR
    START([START]) --> N1[Node 1: Load<br/>Load Tool 0 + Metadata]
    N1 --> N2[Node 2: Prepare LLM<br/>Rank Candidates]
    N2 --> N3[Node 3: Mapping LLM<br/>Match Entities]
    N3 --> N4[Node 4: Filter<br/>Apply Scope Out]
    N4 --> N5[Node 5: Save<br/>Write JSON + Artifacts]
    N5 --> END([END])

    style N2 fill:#ffccbc,color:#000
    style N3 fill:#ffccbc,color:#000
    style N4 fill:#c8e6c9,color:#000
    style N5 fill:#fff9c4,color:#000
```

### 4.3 State Management (TypedDict)

```python
class Tool1State(TypedDict, total=False):
    """Sd√≠len√Ω stav mezi nody Tool 1."""
    # Vstupn√≠ data
    business_context: dict  # Z Tool 0
    metadata_export: dict   # Collibra/Databricks export

    # Zpracovan√° data
    entities: list[str]     # Seznam entit k mapov√°n√≠
    candidates: list[dict]  # Potenci√°ln√≠ kandid√°ti
    ranked_candidates: list[dict]  # LLM ranked
    mappings: list[dict]    # Entity ‚Üí candidate + confidence
    filtered_mappings: list[dict]  # Po aplikaci scope_out

    # Metadata
    scope_out: str
    timestamp: str

    # Output paths
    output_json_path: str
    output_artifact_path: str
```

### 4.4 Node Descriptions

| Node           | Typ             | LLM? | Funkce                                                                |
| -------------- | --------------- | ---- | --------------------------------------------------------------------- |
| **1. Load**    | Deterministick√Ω | ‚ùå    | Naƒçte `business_context.json` + `BA-BS_Datamarts_metadata.json`       |
| **2. Prepare** | LLM             | ‚úÖ    | Rankuje kandid√°ty podle relevance k business entit√°m (confidence 0-1) |
| **3. Mapping** | LLM             | ‚úÖ    | Mapuje entity na konkr√©tn√≠ kandid√°ty s od≈Øvodnƒõn√≠m (rationale)        |
| **4. Filter**  | Deterministick√Ω | ‚ùå    | Odstran√≠ kandid√°ty dle `scope_out` blacklist                          |
| **5. Save**    | Deterministick√Ω | ‚ùå    | Ulo≈æ√≠ `data/tool1/filtered_dataset.json` + audit log                  |

### 4.5 LLM Nodes - Structured Output

**Node 2 (Prepare):**
LLM ranking bƒõ≈æ√≠ na Azure OpenAI (deployment `test-gpt-5-mini`) s vyu≈æit√≠m env promƒõnn√Ωch popsan√Ωch v√Ω≈°e.
```python
class CandidateRank(BaseModel):
    candidate_id: str = Field(description="ID kandid√°ta")
    relevance_score: float = Field(description="Sk√≥re relevance 0-1")
    rationale: str = Field(description="Zd≈Øvodnƒõn√≠ sk√≥re")

class CandidateRanking(BaseModel):
    ranked_candidates: list[CandidateRank]

# LLM call s Azure LLM instanc√≠
agent = create_agent(
    model=AZURE_LLM,  # AzureChatOpenAI instance
    response_format=ToolStrategy(CandidateRanking),
    system_prompt="Rankuj kandid√°ty podle relevance k business entit√°m..."
)
```

**Node 3 (Mapping):**
Mapov√°n√≠ vyu≈æ√≠v√° stejn√© Azure OpenAI nasazen√≠ (`test-gpt-5-mini`) a sd√≠lenou konfiguraci.
```python
class EntityMapping(BaseModel):
    entity: str = Field(description="Business entita")
    candidate_id: str = Field(description="Mapovan√Ω kandid√°t")
    confidence: float = Field(description="Jistota mapov√°n√≠ 0-1")
    rationale: str = Field(description="Zd≈Øvodnƒõn√≠ mapov√°n√≠")

class MappingSuggestions(BaseModel):
    mappings: list[EntityMapping]

# LLM call s Azure LLM instanc√≠
agent = create_agent(
    model=AZURE_LLM,  # AzureChatOpenAI instance
    response_format=ToolStrategy(MappingSuggestions),
    system_prompt="Mapuj business entity na technick√© kandid√°ty..."
)
```

### 4.6 Output Example

```json
{
  "timestamp": "2025-11-01T14:23:45Z",
  "business_context": {
    "entities": ["Suppliers", "Purchase Orders"],
    "scope_out": "HR data, Real-time monitoring"
  },
  "mappings": [
    {
      "entity": "Suppliers",
      "candidate_id": "dm_bs_purchase",
      "confidence": 0.92,
      "rationale": "Schema dm_bs_purchase obsahuje dimv_supplier tabulku, kter√° odpov√≠d√° entitƒõ Suppliers"
    },
    {
      "entity": "Purchase Orders",
      "candidate_id": "dm_bs_purchase",
      "confidence": 0.95,
      "rationale": "Schema obsahuje factv_purchase_order_item, co≈æ jsou transakƒçn√≠ data objedn√°vek"
    }
  ],
  "scope_out_filtered": [
    "dm_ba_hr_payroll (blacklisted: HR data)",
    "dm_bs_real_time_monitoring (blacklisted: Real-time monitoring)"
  ]
}
```

### 4.7 Detailn√≠ Flow Diagram Tool 1

```mermaid
graph TB
    START([START Tool1State = empty]) --> N1_START[Node 1: Load]

    subgraph N1["Node 1: Load Context (Deterministick√Ω)"]
        N1_START --> N1_READ1[Read business_context.json]
        N1_READ1 --> N1_READ2[Read BA-BS_Datamarts_metadata.json]
        N1_READ2 --> N1_EXTRACT[Extract entities list]
        N1_EXTRACT --> N1_FILTER[Filter candidates by scope_in]
        N1_FILTER --> N1_STATE[Update state:<br/>business_context<br/>metadata_export<br/>entities<br/>candidates]
    end

    N1_STATE --> N2_START[Node 2: Prepare LLM]

    subgraph N2["Node 2: Prepare - LLM Ranking (AzureChatOpenAI)"]
        N2_START --> N2_PROMPT[Build system prompt:<br/>Rank by relevance]
        N2_PROMPT --> N2_CONTEXT[Build user message:<br/>entities + candidates]
        N2_CONTEXT --> N2_AGENT[create_agent<br/>ToolStrategy CandidateRanking]
        N2_AGENT --> N2_INVOKE[agent.invoke messages]
        N2_INVOKE --> N2_EXTRACT[Extract structured_response]
        N2_EXTRACT --> N2_STATE[Update state:<br/>ranked_candidates<br/>relevance_score 0-1<br/>rationale]
    end

    N2_STATE --> N3_START[Node 3: Mapping LLM]

    subgraph N3["Node 3: Mapping - LLM Matching (AzureChatOpenAI)"]
        N3_START --> N3_PROMPT[Build system prompt:<br/>Map entities to candidates]
        N3_PROMPT --> N3_CONTEXT[Build user message:<br/>entities + ranked_candidates]
        N3_CONTEXT --> N3_AGENT[create_agent<br/>ToolStrategy MappingSuggestions]
        N3_AGENT --> N3_INVOKE[agent.invoke messages]
        N3_INVOKE --> N3_EXTRACT[Extract structured_response]
        N3_EXTRACT --> N3_STATE[Update state:<br/>mappings<br/>entity ‚Üí candidate_id<br/>confidence + rationale]
    end

    N3_STATE --> N4_START[Node 4: Filter]

    subgraph N4["Node 4: Filter - Scope Out (Deterministick√Ω)"]
        N4_START --> N4_LOAD[Load scope_out blacklist]
        N4_LOAD --> N4_LOOP[For each mapping]
        N4_LOOP --> N4_CHECK{candidate_id matches<br/>blacklist?}
        N4_CHECK -->|Yes| N4_REMOVE[Remove from list]
        N4_CHECK -->|No| N4_KEEP[Keep mapping]
        N4_REMOVE --> N4_LOOP
        N4_KEEP --> N4_LOOP
        N4_LOOP --> N4_STATE[Update state:<br/>filtered_mappings]
    end

    N4_STATE --> N5_START[Node 5: Save]

    subgraph N5["Node 5: Save Outputs (Deterministick√Ω)"]
        N5_START --> N5_JSON[Save filtered_dataset.json<br/>data/tool1/]
        N5_JSON --> N5_AUDIT[Save audit summary<br/>scrum/artifacts/<br/>YYYY-MM-DD_tool1-mapping.json]
        N5_AUDIT --> N5_STATE[Update state:<br/>output_json_path<br/>output_artifact_path]
    end

    N5_STATE --> END([END Return Tool1State])

    style N1 fill:#c8e6c9,color:#000
    style N2 fill:#ffccbc,color:#000
    style N3 fill:#ffccbc,color:#000
    style N4 fill:#c8e6c9,color:#000
    style N5 fill:#fff9c4,color:#000
    style START fill:#e1f5fe,color:#000
    style END fill:#e1f5fe,color:#000
```

**Legenda:**
- üü¢ **Zelen√°** (N1, N4): Deterministick√© nodes (file I/O, filtering)
- üü† **Oran≈æov√°** (N2, N3): LLM nodes (AzureChatOpenAI s ToolStrategy)
- üü° **≈Ωlut√°** (N5): Output node (save JSON + audit)

**State Flow:**
```
START ‚Üí {}
  ‚Üì N1: Load
  ‚Üí {business_context, metadata_export, entities[], candidates[]}
  ‚Üì N2: Prepare (LLM)
  ‚Üí {ranked_candidates[] + relevance_score}
  ‚Üì N3: Mapping (LLM)
  ‚Üí {mappings[] + confidence + rationale}
  ‚Üì N4: Filter
  ‚Üí {filtered_mappings[]}
  ‚Üì N5: Save
  ‚Üí {output_json_path, output_artifact_path}
END ‚Üí Complete Tool1State
```

**Timing (pr≈Ømƒõr):**
- N1 Load: ~1s (file read)
- N2 Prepare: ~8s (LLM ranking)
- N3 Mapping: ~12s (LLM matching)
- N4 Filter: ~1s (deterministic)
- N5 Save: ~1s (file write)
- **Total: ~23s** (target: <40s ‚úÖ)

---

## 5. Tool 2 - Structural Analysis

### 5.1 √öƒçel

Klasifikuje entity na **fakty** (transakƒçn√≠ tabulky) vs **dimenze** (popisn√© tabulky), detekuje **hierarchie** a **foreign key vztahy**.

### 5.2 LangGraph Architecture (5 nodes)

LLM klasifikace (Node 2) opƒõt c√≠l√≠ na Azure OpenAI deployment `test-gpt-5-mini` a sd√≠l√≠ stejn√© promƒõnn√© prost≈ôed√≠.

```mermaid
graph LR
    START([START]) --> N1[Node 1: Load Context<br/>Tool 1 + Full Metadata]
    N1 --> N2[Node 2: Classify LLM<br/>Fact vs Dimension]
    N2 --> N3[Node 3: Relationships<br/>FK Detection]
    N3 --> N4[Node 4: Assemble<br/>Calculate Metrics]
    N4 --> N5[Node 5: Save<br/>Write structure.json]
    N5 --> END([END])

    style N2 fill:#ffccbc,color:#000
    style N3 fill:#c8e6c9,color:#000
    style N4 fill:#c8e6c9,color:#000
    style N5 fill:#fff9c4,color:#000
```

### 5.3 Classification Logic

```mermaid
flowchart TD
    INPUT[Tabulka z metadat] --> HEUR{Heuristika<br/>Prefix?}

    HEUR -->|factv_*| FACT_CHECK[LLM: Validuj jako fakt]
    HEUR -->|dimv_*| DIM_CHECK[LLM: Validuj jako dimenze]
    HEUR -->|Jin√©| LLM_CLASS[LLM: Klasifikuj od z√°kladu]

    FACT_CHECK --> FACT_OUT[FactTable<br/>grain + measures + dates]
    DIM_CHECK --> DIM_OUT[DimensionTable<br/>business_key + attributes]
    LLM_CLASS --> FACT_OUT
    LLM_CLASS --> DIM_OUT

    FACT_OUT --> OUTPUT[StructuralClassification]
    DIM_OUT --> OUTPUT

    style FACT_CHECK fill:#ffccbc,color:#000
    style DIM_CHECK fill:#ffccbc,color:#000
    style LLM_CLASS fill:#ffccbc,color:#000
```

### 5.4 Pydantic Schemas

```python
class FactTable(BaseModel):
    """Faktov√° tabulka (transakƒçn√≠ data)."""
    table_id: str = Field(description="ID tabulky, nap≈ô. 'factv_purchase_order_item'")
    table_name: str = Field(description="Pln√Ω n√°zev s path")
    grain: str = Field(description="Zrnitost, nap≈ô. '√∫rove≈à polo≈æky objedn√°vky'")
    measures: list[str] = Field(description="ƒå√≠seln√© sloupce (order_quantity, order_value)")
    date_columns: list[str] = Field(description="Datumov√© sloupce (order_date)")
    confidence: float = Field(description="Confidence 0-1")
    rationale: str = Field(description="Zd≈Øvodnƒõn√≠ klasifikace")

class DimensionTable(BaseModel):
    """Dimenzion√°ln√≠ tabulka (popisn√° data)."""
    table_id: str = Field(description="ID tabulky, nap≈ô. 'dimv_supplier'")
    table_name: str = Field(description="Pln√Ω n√°zev s path")
    business_key: str = Field(description="Prim√°rn√≠ kl√≠ƒç, nap≈ô. 'supplier_id'")
    attributes: list[str] = Field(description="Popisn√© sloupce (supplier_name, country)")
    confidence: float = Field(description="Confidence 0-1")
    rationale: str = Field(description="Zd≈Øvodnƒõn√≠ klasifikace")

class Relationship(BaseModel):
    """Foreign key vztah mezi tabulkami."""
    from_table: str = Field(description="Zdrojov√° tabulka")
    to_table: str = Field(description="C√≠lov√° tabulka")
    join_column: str = Field(description="FK sloupec, nap≈ô. 'supplier_id'")
    relationship_type: str = Field(description="Typ vztahu, nap≈ô. 'FK', 'PK-FK'")
    confidence: float = Field(description="Confidence 0-1")
    rationale: str = Field(description="Zd≈Øvodnƒõn√≠ detekce")

class Hierarchy(BaseModel):
    """Hierarchick√Ω vztah (parent-child)."""
    parent_table: str = Field(description="Rodiƒçovsk√° tabulka")
    child_table: str = Field(description="Pod≈ô√≠zen√° tabulka")
    relationship_type: str = Field(description="Kardinalita, nap≈ô. '1:N'")
    confidence: float = Field(description="Confidence 0-1")
    rationale: str = Field(description="Zd≈Øvodnƒõn√≠ detekce")

class StructuralMetrics(BaseModel):
    """Metriky pokryt√≠."""
    total_facts: int
    total_dimensions: int
    total_hierarchies: int
    total_relationships: int
    coverage: float = Field(description="% namapovan√Ωch entit")
    unresolved_entities: list[str] = Field(description="Nerozpoznan√© entity")

class StructuralAnalysis(BaseModel):
    """Kompletn√≠ struktur√°ln√≠ anal√Ωza."""
    timestamp: str
    business_context: dict
    facts: list[FactTable]
    dimensions: list[DimensionTable]
    hierarchies: list[Hierarchy]
    relationships: list[Relationship]
    metrics: StructuralMetrics
```

### 5.5 Heuristiky & Pravidla

| Heuristika            | Pravidlo                                                  | P≈ô√≠klad                         |
| --------------------- | --------------------------------------------------------- | ------------------------------- |
| **Fact prefix**       | Tabulka zaƒç√≠n√° `factv_*` nebo `fact_*`                    | `factv_purchase_order_item`     |
| **Dimension prefix**  | Tabulka zaƒç√≠n√° `dimv_*` nebo `dim_*`                      | `dimv_supplier`                 |
| **FK suffix**         | Sloupec konƒç√≠ `*_id`, `*_fk`, `*_key`                     | `supplier_id` ‚Üí FK              |
| **FK name match**     | Jm√©no sloupce odpov√≠d√° n√°zvu dimenze                      | `supplier_id` ‚Üí `dimv_supplier` |
| **Hierarchy field**   | Pole "Hierarcy Relation" v metadatech (pozor na p≈ôeklep!) | `parent ‚Üí child`                |
| **Measure detection** | ƒå√≠seln√© sloupce v faktech                                 | `order_quantity`, `order_value` |
| **Date detection**    | Sloupce s typem date/timestamp                            | `order_date`, `delivery_date`   |

### 5.6 Output Example

```json
{
  "timestamp": "2025-11-01T15:42:13Z",
  "business_context": {
    "entities": ["Suppliers", "Purchase Orders"],
    "scope_out": "HR data, Real-time monitoring"
  },
  "facts": [
    {
      "table_id": "factv_purchase_order_item",
      "table_name": "Systems>dap_gold_prod>dm_bs_purchase>factv_purchase_order_item",
      "grain": "√∫rove≈à polo≈æky n√°kupn√≠ objedn√°vky",
      "measures": ["order_quantity", "order_value", "unit_price"],
      "date_columns": ["order_date", "delivery_date", "invoice_date"],
      "confidence": 0.95,
      "rationale": "Prefix 'factv_', obsahuje ƒç√≠seln√© mƒõ≈ô√≠tka a datumov√© sloupce, transakƒçn√≠ charakter dat"
    }
  ],
  "dimensions": [
    {
      "table_id": "dimv_supplier",
      "table_name": "Systems>dap_gold_prod>dm_bs_purchase>dimv_supplier",
      "business_key": "supplier_id",
      "attributes": ["supplier_name", "supplier_country", "supplier_category"],
      "confidence": 0.92,
      "rationale": "Prefix 'dimv_', obsahuje business key a popisn√© atributy, referenƒçn√≠ charakter"
    },
    {
      "table_id": "dimv_material",
      "table_name": "Systems>dap_gold_prod>dm_bs_purchase>dimv_material",
      "business_key": "material_id",
      "attributes": ["material_name", "material_group", "unit_of_measure"],
      "confidence": 0.89,
      "rationale": "Prefix 'dimv_', popisn√© atributy materi√°l≈Ø"
    }
  ],
  "hierarchies": [
    {
      "parent_table": "dimv_material_group",
      "child_table": "dimv_material",
      "relationship_type": "1:N",
      "confidence": 0.88,
      "rationale": "Pole 'Hierarcy Relation' p≈ô√≠tomno v metadatech, parent-child pattern v popisc√≠ch"
    }
  ],
  "relationships": [
    {
      "from_table": "factv_purchase_order_item",
      "to_table": "dimv_supplier",
      "join_column": "supplier_id",
      "relationship_type": "FK",
      "confidence": 0.90,
      "rationale": "Sloupec 'supplier_id' odpov√≠d√° n√°zvu dimenze 'dimv_supplier', suffix '_id'"
    },
    {
      "from_table": "factv_purchase_order_item",
      "to_table": "dimv_material",
      "join_column": "material_id",
      "relationship_type": "FK",
      "confidence": 0.87,
      "rationale": "Sloupec 'material_id' odpov√≠d√° n√°zvu dimenze 'dimv_material', suffix '_id'"
    }
  ],
  "metrics": {
    "total_facts": 5,
    "total_dimensions": 12,
    "total_hierarchies": 3,
    "total_relationships": 18,
    "coverage": 0.87,
    "unresolved_entities": ["Delivery Performance"]
  }
}
```

### 5.7 Detailn√≠ Flow Diagram Tool 2

```mermaid
graph TB
    START([START Tool2State = empty]) --> N1_START[Node 1: Load Context]

    subgraph N1["Node 1: Load Context (Deterministick√Ω)"]
        N1_START --> N1_READ1[Read filtered_dataset.json<br/>from Tool 1]
        N1_READ1 --> N1_READ2[Read BA-BS_Datamarts_metadata.json<br/>full metadata]
        N1_READ2 --> N1_EXTRACT[Extract business_context<br/>entities, scope_out]
        N1_EXTRACT --> N1_EXPAND[Expand candidate details<br/>Match IDs from Tool 1<br/>to full metadata]
        N1_EXPAND --> N1_STATE[Update state:<br/>tool1_mappings<br/>full_metadata<br/>business_context<br/>candidates_detail]
    end

    N1_STATE --> N2_START[Node 2: Classify Entities]

    subgraph N2["Node 2: Classify - LLM Classification (AzureChatOpenAI)"]
        N2_START --> N2_HEUR[Apply heuristics:<br/>factv_* ‚Üí likely fact<br/>dimv_* ‚Üí likely dimension]
        N2_HEUR --> N2_PROMPT[Build system prompt:<br/>Classification rules<br/>Fact vs Dimension<br/>Grain/Business Key<br/>Scope_out blacklist]
        N2_PROMPT --> N2_EXAMPLES[Add examples:<br/>factv_purchase_order_item<br/>dimv_supplier]
        N2_EXAMPLES --> N2_CONTEXT[Build user message:<br/>Mappings summary<br/>Candidates summary]
        N2_CONTEXT --> N2_AGENT[create_agent<br/>ToolStrategy<br/>StructuralClassification]
        N2_AGENT --> N2_INVOKE[agent.invoke messages]
        N2_INVOKE --> N2_EXTRACT[Extract structured_response:<br/>facts[] + dimensions[]]
        N2_EXTRACT --> N2_VALIDATE[Validate:<br/>grain present in facts<br/>business_key in dimensions]
        N2_VALIDATE --> N2_STATE[Update state:<br/>classified_entities<br/>FactTable[] + DimensionTable[]]
    end

    N2_STATE --> N3_START[Node 3: Identify Relationships]

    subgraph N3["Node 3: Relationships - Heuristics + Optional LLM"]
        N3_START --> N3_FK_DETECT[FK Detection:<br/>Column suffix *_id, *_fk, *_key]
        N3_FK_DETECT --> N3_FK_MATCH[Match column name<br/>to dimension table name<br/>supplier_id ‚Üí dimv_supplier]
        N3_FK_MATCH --> N3_HIER_DETECT[Hierarchy Detection:<br/>Check 'Hierarcy Relation' field<br/>note: typo in metadata!]
        N3_HIER_DETECT --> N3_PATTERN[Parent-child patterns<br/>in descriptions]
        N3_PATTERN --> N3_CONFIDENCE[Assign confidence scores<br/>based on match strength]
        N3_CONFIDENCE --> N3_STATE[Update state:<br/>relationships[]<br/>hierarchies[]]
    end

    N3_STATE --> N4_START[Node 4: Assemble Structure]

    subgraph N4["Node 4: Assemble - Calculate Metrics (Deterministick√Ω)"]
        N4_START --> N4_COUNT[Count totals:<br/>facts, dimensions<br/>hierarchies, relationships]
        N4_COUNT --> N4_COVERAGE[Calculate coverage:<br/>mapped / total entities]
        N4_COVERAGE --> N4_UNRESOLVED[Identify unresolved:<br/>entities without classification]
        N4_UNRESOLVED --> N4_BUILD[Build StructuralAnalysis:<br/>timestamp + business_context<br/>+ all results + metrics]
        N4_BUILD --> N4_STATE[Update state:<br/>final_structure<br/>StructuralAnalysis]
    end

    N4_STATE --> N5_START[Node 5: Save Outputs]

    subgraph N5["Node 5: Save Outputs (Deterministick√Ω)"]
        N5_START --> N5_JSON[Save structure.json<br/>data/tool2/<br/>Main output]
        N5_JSON --> N5_AUDIT[Save audit summary<br/>scrum/artifacts/<br/>YYYY-MM-DD_tool2-structure-summary.json]
        N5_AUDIT --> N5_METRICS[Log metrics:<br/>coverage, unresolved<br/>avg confidence]
        N5_METRICS --> N5_STATE[Update state:<br/>output paths]
    end

    N5_STATE --> END([END Return Tool2State])

    style N1 fill:#c8e6c9,color:#000
    style N2 fill:#ffccbc,color:#000
    style N3 fill:#fff3e0,color:#000
    style N4 fill:#c8e6c9,color:#000
    style N5 fill:#fff9c4,color:#000
    style START fill:#e1f5fe,color:#000
    style END fill:#e1f5fe,color:#000
```

**Legenda:**
- üü¢ **Zelen√°** (N1, N4): Deterministick√© nodes (load, calculate)
- üü† **Oran≈æov√°** (N2): LLM node (classify fact/dimension s ToolStrategy)
- üü° **B√©≈æov√°** (N3): Hybrid node (heuristics + optional LLM validation)
- üü° **≈Ωlut√°** (N5): Output node (save JSON + audit)

**Classification Decision Tree:**
```mermaid
graph TD
    TABLE[Tabulka z metadat] --> PREFIX{Prefix?}

    PREFIX -->|factv_*| FACT_HEUR[Heuristika: Pravdƒõpodobnƒõ FAKT]
    PREFIX -->|dimv_*| DIM_HEUR[Heuristika: Pravdƒõpodobnƒõ DIMENZE]
    PREFIX -->|Jin√©| UNKNOWN[Nezn√°m√Ω typ]

    FACT_HEUR --> LLM_FACT[LLM: Validuj jako fakt<br/>Zkontroluj measures<br/>Zkontroluj date_columns<br/>Urƒçi grain]
    DIM_HEUR --> LLM_DIM[LLM: Validuj jako dimenze<br/>Najdi business_key<br/>Najdi attributes]
    UNKNOWN --> LLM_CLASSIFY[LLM: Klasifikuj od z√°kladu<br/>Fakt nebo Dimenze?]

    LLM_FACT --> FACT_OUT["FactTable:<br/>table_id, grain<br/>measures, date_columns<br/>confidence, rationale"]
    LLM_DIM --> DIM_OUT["DimensionTable:<br/>table_id, business_key<br/>attributes<br/>confidence, rationale"]
    LLM_CLASSIFY --> FACT_OUT
    LLM_CLASSIFY --> DIM_OUT

    FACT_OUT --> OUTPUT[StructuralClassification]
    DIM_OUT --> OUTPUT

    style PREFIX fill:#fff9c4,color:#000
    style LLM_FACT fill:#ffccbc,color:#000
    style LLM_DIM fill:#ffccbc,color:#000
    style LLM_CLASSIFY fill:#ffccbc,color:#000
    style FACT_OUT fill:#c8e6c9,color:#000
    style DIM_OUT fill:#c8e6c9,color:#000
```

**State Flow:**
```
START ‚Üí {}
  ‚Üì N1: Load Context
  ‚Üí {tool1_mappings, full_metadata, business_context, candidates_detail[]}
  ‚Üì N2: Classify (LLM + Heuristics)
  ‚Üí {classified_entities: {facts[], dimensions[]} + confidence + rationale}
  ‚Üì N3: Relationships (Heuristics)
  ‚Üí {relationships[] + hierarchies[] + FK detection}
  ‚Üì N4: Assemble
  ‚Üí {final_structure: StructuralAnalysis + metrics + coverage}
  ‚Üì N5: Save
  ‚Üí {output paths}
END ‚Üí Complete Tool2State
```

**Timing (pr≈Ømƒõr):**
- N1 Load: ~1s (file read + expand)
- N2 Classify: ~10s (LLM classification)
- N3 Relationships: ~2s (heuristics)
- N4 Assemble: ~1s (calculate metrics)
- N5 Save: ~1s (file write)
- **Total: ~15s** (target: <20s ‚úÖ)

**Key Features:**
- ‚úÖ **Hybrid approach:** Heuristics (prefix patterns) + LLM validation
- ‚úÖ **Structured output:** ToolStrategy(StructuralClassification)
- ‚úÖ **Confidence scoring:** Each classification has 0.0-1.0 score + rationale
- ‚úÖ **Coverage tracking:** Identifies unresolved entities
- ‚úÖ **FK detection:** Column name matching (supplier_id ‚Üí dimv_supplier)
- ‚ö†Ô∏è **Known limitation:** Hardcoded relationship examples (future: dynamic column parsing)

---

## 6. Tool 3 - Quality Validator

### 6.1 √öƒçel

Validuje kvalitu metadat kontrolou pol√≠ `articulationScore`, `validationResult`, `status`, detekuje chybƒõj√≠c√≠ data ("Missing from source").

### 6.2 Architecture

```mermaid
flowchart LR
    INPUT[structure.json<br/>+ Full Metadata] --> LOAD[Load Entities]
    LOAD --> VALID[Validate Fields<br/>articulationScore<br/>validationResult<br/>status]
    VALID --> SCORE[Calculate Quality<br/>Scores]
    SCORE --> FLAG[Flag Issues<br/>Low scores<br/>Missing data]
    FLAG --> OUTPUT[quality_report.json]

    style VALID fill:#c8e6c9,color:#000
    style SCORE fill:#c8e6c9,color:#000
    style FLAG fill:#ffccbc,color:#000
```

### 6.3 Validation Rules

| Pole                | Krit√©rium             | Akce                         |
| ------------------- | --------------------- | ---------------------------- |
| `articulationScore` | < 0.5                 | ‚ö†Ô∏è WARNING: Low articulation  |
| `articulationScore` | = 0.0                 | üî¥ ERROR: No articulation     |
| `validationResult`  | "Missing from source" | üî¥ ERROR: Data gap            |
| `validationResult`  | "Outdated"            | ‚ö†Ô∏è WARNING: Needs refresh     |
| `status`            | "Deprecated"          | ‚ö†Ô∏è WARNING: Deprecated entity |
| `description`       | null/empty            | ‚ö†Ô∏è WARNING: No documentation  |

### 6.4 Output Schema

```python
class QualityIssue(BaseModel):
    entity_id: str
    entity_name: str
    severity: Literal["ERROR", "WARNING", "INFO"]
    issue_type: str
    description: str
    field_value: Any
    recommendation: str

class QualityMetrics(BaseModel):
    total_entities: int
    avg_articulation_score: float
    entities_with_issues: int
    error_count: int
    warning_count: int
    info_count: int

class QualityReport(BaseModel):
    timestamp: str
    business_context: dict
    issues: list[QualityIssue]
    metrics: QualityMetrics
```

### 6.5 Output Example

```json
{
  "timestamp": "2025-11-01T16:05:27Z",
  "business_context": {
    "entities": ["Suppliers", "Purchase Orders"],
    "scope_out": "HR data"
  },
  "issues": [
    {
      "entity_id": "dimv_bs_purchase_ekl_created_date",
      "entity_name": "Purchase EKL Created Date",
      "severity": "ERROR",
      "issue_type": "NO_ARTICULATION",
      "description": "Pole articulationScore je 0.0, entita nen√≠ zdokumentovan√°",
      "field_value": 0.0,
      "recommendation": "P≈ôidat popis entity a atribut≈Ø v Collibra"
    },
    {
      "entity_id": "factv_purchase_order_line",
      "entity_name": "Purchase Order Line Fact",
      "severity": "WARNING",
      "issue_type": "MISSING_FROM_SOURCE",
      "description": "validationResult: 'Missing from source' - data chyb√≠ ve zdrojov√©m syst√©mu",
      "field_value": "Missing from source",
      "recommendation": "Ovƒõ≈ôit zdrojov√Ω syst√©m SAP, p≈ô√≠padnƒõ odstranit z katalogu"
    }
  ],
  "metrics": {
    "total_entities": 45,
    "avg_articulation_score": 0.62,
    "entities_with_issues": 12,
    "error_count": 3,
    "warning_count": 9,
    "info_count": 0
  }
}
```

---

## 7. Tool 7 - Governance Report Generator

### 7.1 √öƒçel

Generuje **governance report** porovn√°vaj√≠c√≠ business po≈æadavky (Tool 0) s technickou realitou (Tool 2 + Tool 3). Identifikuje **risks**, **gaps** a **recommendations**.

### 7.2 Architecture

```mermaid
flowchart TD
    INPUT1[business_context.json] --> CONTEXT[Load Context]
    INPUT2[structure.json] --> CONTEXT
    INPUT3[quality_report.json] --> CONTEXT

    CONTEXT --> COMPARE[Compare<br/>Expectations vs Reality]
    COMPARE --> RISKS[Identify Risks<br/>Missing entities<br/>Low quality]
    RISKS --> LLM[LLM Generator<br/>AzureChatOpenAI<br/>test-gpt-5-mini]
    LLM --> REPORT[governance_report.md]

    style LLM fill:#ffccbc,color:#000
```

### 7.3 Report Structure

```markdown
# Governance Report: [Project Name]

**Datum:** 2025-11-01
**Zadavatel:** Jan Nov√°k
**Status:** ‚ö†Ô∏è ƒå√°steƒçnƒõ splnƒõno

---

## 1. Shrnut√≠

Projekt "Anal√Ωza n√°kupu Q3 2025" byl analyzov√°n v≈Øƒçi BS metadat≈Øm.
Identifikov√°no **87% pokryt√≠** po≈æadovan√Ωch entit, av≈°ak s **3 kritick√Ωmi probl√©my kvality**.

---

## 2. Business Po≈æadavky vs Realita

| Entita (Po≈æadavek)   | Nalezeno v Metadatech       | Status  | Confidence |
| -------------------- | --------------------------- | ------- | ---------- |
| Suppliers            | ‚úÖ dimv_supplier             | OK      | 0.92       |
| Purchase Orders      | ‚úÖ factv_purchase_order_item | OK      | 0.95       |
| Materials            | ‚úÖ dimv_material             | OK      | 0.89       |
| Delivery Performance | ‚ùå Nenalezeno                | MISSING | -          |

---

## 3. Struktur√°ln√≠ Anal√Ωza

### 3.1 Fakty (Transakƒçn√≠ tabulky)
- ‚úÖ **factv_purchase_order_item** (confidence: 0.95)
  - Grain: √∫rove≈à polo≈æky objedn√°vky
  - Measures: order_quantity, order_value, unit_price
  - Dates: order_date, delivery_date, invoice_date

### 3.2 Dimenze (Popisn√© tabulky)
- ‚úÖ **dimv_supplier** (confidence: 0.92)
  - Business key: supplier_id
  - Attributes: supplier_name, supplier_country, supplier_category

- ‚úÖ **dimv_material** (confidence: 0.89)
  - Business key: material_id
  - Attributes: material_name, material_group, unit_of_measure

### 3.3 Vztahy (Foreign Keys)
```
factv_purchase_order_item
  ‚îú‚îÄ supplier_id ‚Üí dimv_supplier (confidence: 0.90)
  ‚îî‚îÄ material_id ‚Üí dimv_material (confidence: 0.87)
```

---

## 4. Kvalitativn√≠ Probl√©my

### 4.1 Kritick√© (ERROR)
üî¥ **dimv_bs_purchase_ekl_created_date**
- Issue: articulationScore = 0.0 (≈æ√°dn√° dokumentace)
- Impact: Nelze pou≈æ√≠t bez manu√°ln√≠ revize
- Recommendation: P≈ôidat popis v Collibra p≈ôed pou≈æit√≠m

### 4.2 Varov√°n√≠ (WARNING)
‚ö†Ô∏è **factv_purchase_order_line**
- Issue: validationResult = "Missing from source"
- Impact: Data nejsou dostupn√° ve zdrojov√©m syst√©mu
- Recommendation: Ovƒõ≈ôit SAP, p≈ô√≠padnƒõ odstranit z katalogu

---

## 5. Pokryt√≠ & Metriky

| Metrika                    | Hodnota   |
| -------------------------- | --------- |
| Pokryt√≠ entit              | 87% (7/8) |
| Pr≈Ømƒõrn√° confidence        | 0.91      |
| Pr≈Ømƒõrn√Ω articulationScore | 0.62      |
| Kritick√© probl√©my          | 3         |
| Varov√°n√≠                   | 9         |

---

## 6. Rizika & Doporuƒçen√≠

### 6.1 Vysok√° rizika
1. **Chybƒõj√≠c√≠ entita "Delivery Performance"**
   - Riziko: Nelze splnit business po≈æadavek na anal√Ωzu dodac√≠ch v√Ωkon≈Ø
   - Doporuƒçen√≠: Vytvo≈ôit kalkulovan√© pole nebo novou faktovou tabulku

2. **N√≠zk√° kvalita dokumentace (avg 0.62)**
   - Riziko: Business u≈æivatel√© nerozum√≠ dat≈Øm
   - Doporuƒçen√≠: Data Steward campaign pro zv√Ω≈°en√≠ articulationScore

### 6.2 St≈ôedn√≠ rizika
1. **Missing from source u fakt≈Ø**
   - Riziko: Potencion√°lnƒõ ne√∫pln√© anal√Ωzy
   - Doporuƒçen√≠: Validovat zdroje SAP

---

## 7. Akƒçn√≠ Polo≈æky

- [ ] **Vysok√° priorita:** Doplnit dokumentaci pro dimv_bs_purchase_ekl_created_date
- [ ] **Vysok√° priorita:** Vy≈ôe≈°it "Delivery Performance" entitu (vytvo≈ôit/namapovat)
- [ ] **St≈ôedn√≠ priorita:** Ovƒõ≈ôit SAP zdroje pro factv_purchase_order_line
- [ ] **N√≠zk√° priorita:** Data Steward review pro zv√Ω≈°en√≠ articulationScore

---

**Z√°vƒõr:** Projekt je realizovateln√Ω s 87% pokryt√≠m, ale vy≈æaduje vy≈ôe≈°en√≠ 3 kritick√Ωch probl√©m≈Ø p≈ôed implementac√≠.
```

---

## 8. Orchestrator - LangGraph MVP Workflow

### 8.1 Celkov√Ω Flow (5 nodes)

```mermaid
graph TB
    START([üöÄ START]) --> N0[Node 0: Load Business Context<br/>Tool 0 Parser]
    N0 --> N1[Node 1: Ingest & Filter<br/>Tool 1 - 5 sub-nodes]
    N1 --> N2{Node 2: Parallel Analysis}

    N2 --> N2A[Tool 2: Structural<br/>5 sub-nodes]
    N2 --> N2B[Tool 3: Quality<br/>Deterministic]

    N2A --> N3[Node 3: Consolidate<br/>Merge Results]
    N2B --> N3

    N3 --> N4[Node 4: Generate Artifacts<br/>Tool 7: Governance Report]
    N4 --> END([üèÅ END])

    style N0 fill:#ffccbc,color:#000
    style N1 fill:#c5cae9,color:#000
    style N2A fill:#c5cae9,color:#000
    style N2B fill:#c8e6c9,color:#000
    style N4 fill:#ffccbc,color:#000
    style N2 fill:#fff9c4,color:#000
```

### 8.2 State Management

```python
class MCOPState(TypedDict, total=False):
    """Glob√°ln√≠ stav MVP orchestr√°tora."""
    # Node 0 outputs
    business_context: dict  # BusinessRequest z Tool 0

    # Node 1 outputs
    filtered_mappings: list[dict]  # Entity‚Üícandidate z Tool 1
    metadata_export: dict  # Full Collibra/Databricks export

    # Node 2 outputs (parallel)
    structural_analysis: dict  # StructuralAnalysis z Tool 2
    quality_report: dict  # QualityReport z Tool 3

    # Node 3 outputs
    consolidated_risks: list[dict]  # Merged risks & issues

    # Node 4 outputs
    governance_report: str  # Markdown report z Tool 7

    # Metadata
    execution_start: str
    execution_end: str
    total_duration: float
```

### 8.3 Node Definitions

| Node        | N√°stroj      | Sub-nodes                      | LLM? | Trv√°n√≠ (odhad) |
| ----------- | ------------ | ------------------------------ | ---- | -------------- |
| **Node 0**  | Tool 0       | 1x LLM parse                   | ‚úÖ    | ~3s            |
| **Node 1**  | Tool 1       | 5x (2x LLM + 3x deterministic) | ‚úÖ    | ~30s           |
| **Node 2A** | Tool 2       | 5x (1x LLM + 4x deterministic) | ‚úÖ    | ~15s           |
| **Node 2B** | Tool 3       | 1x deterministic               | ‚ùå    | ~2s            |
| **Node 3**  | Consolidator | 1x deterministic               | ‚ùå    | ~1s            |
| **Node 4**  | Tool 7       | 1x LLM generate                | ‚úÖ    | ~10s           |
| **Total**   | -            | -                              | -    | **~61s**       |

### 8.4 Conditional Logic

```python
def should_run_tool3(state: MCOPState) -> bool:
    """Conditional edge: Run Tool 3 pouze pokud Tool 2 na≈°el entity."""
    structural = state.get("structural_analysis", {})
    facts = structural.get("facts", [])
    dimensions = structural.get("dimensions", [])
    return len(facts) + len(dimensions) > 0

def should_generate_report(state: MCOPState) -> bool:
    """Conditional edge: Generuj report pouze pokud m√°me v√Ωsledky."""
    return (
        state.get("structural_analysis") is not None and
        state.get("quality_report") is not None
    )
```

### 8.5 Error Handling

```python
def handle_node_error(node_name: str, error: Exception, state: MCOPState):
    """Error handler pro v≈°echny nody."""
    error_log = {
        "node": node_name,
        "error": str(error),
        "timestamp": datetime.now().isoformat(),
        "state_snapshot": {k: type(v).__name__ for k, v in state.items()}
    }

    # Log to artifacts
    artifacts_dir = Path("scrum/artifacts")
    error_path = artifacts_dir / f"{datetime.now().strftime('%Y-%m-%d')}_error-{node_name}.json"
    with open(error_path, "w") as f:
        json.dump(error_log, f, indent=2)

    # Re-raise pro debugging
    raise RuntimeError(f"Node {node_name} failed: {error}") from error
```

### 8.6 Execution Example

```python
# Build orchestrator graph
from langgraph.graph import StateGraph, START, END

workflow = StateGraph(MCOPState)

# Add nodes
workflow.add_node("node0_business_context", run_tool0)
workflow.add_node("node1_ingest", run_tool1)
workflow.add_node("node2a_structural", run_tool2)
workflow.add_node("node2b_quality", run_tool3)
workflow.add_node("node3_consolidate", consolidate_results)
workflow.add_node("node4_report", run_tool7)

# Add edges
workflow.add_edge(START, "node0_business_context")
workflow.add_edge("node0_business_context", "node1_ingest")

# Parallel execution (Node 2)
workflow.add_edge("node1_ingest", "node2a_structural")
workflow.add_edge("node1_ingest", "node2b_quality")

# Consolidation (Node 3)
workflow.add_edge("node2a_structural", "node3_consolidate")
workflow.add_edge("node2b_quality", "node3_consolidate")

# Report generation (Node 4)
workflow.add_conditional_edges(
    "node3_consolidate",
    should_generate_report,
    {True: "node4_report", False: END}
)
workflow.add_edge("node4_report", END)

# Compile
graph = workflow.compile()

# Execute
result = graph.invoke({
    "execution_start": datetime.now().isoformat()
})

print(f"‚úÖ Pipeline completed in {result['total_duration']:.2f}s")
print(f"üìÑ Governance report: {len(result['governance_report'])} characters")
```

---

## 9. Datov√© toky a form√°ty

### 9.1 File Structure

```
archi-agent/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ tool0/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ business_context.json          # Tool 0 output
‚îÇ   ‚îú‚îÄ‚îÄ tool1/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ filtered_dataset.json          # Tool 1 output
‚îÇ   ‚îú‚îÄ‚îÄ tool2/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ structure.json                 # Tool 2 output
‚îÇ   ‚îî‚îÄ‚îÄ tool3/
‚îÇ       ‚îî‚îÄ‚îÄ quality_report.json            # Tool 3 output
‚îú‚îÄ‚îÄ docs_langgraph/
‚îÇ   ‚îî‚îÄ‚îÄ BA-BS_Datamarts_metadata.json      # Input metadata export
‚îú‚îÄ‚îÄ scrum/
‚îÇ   ‚îî‚îÄ‚îÄ artifacts/
‚îÇ       ‚îú‚îÄ‚îÄ 2025-11-01_tool0-parse.json    # Audit logs
‚îÇ       ‚îú‚îÄ‚îÄ 2025-11-01_tool1-mapping.json
‚îÇ       ‚îú‚îÄ‚îÄ 2025-11-01_tool2-structure-summary.json
‚îÇ       ‚îî‚îÄ‚îÄ 2025-11-01_governance-report.md  # Final report
‚îî‚îÄ‚îÄ notebooks/
    ‚îú‚îÄ‚îÄ tool0_parser_demo.ipynb             # Prototypy
    ‚îú‚îÄ‚îÄ tool1_ingest_demo.ipynb
    ‚îú‚îÄ‚îÄ tool2_structure_demo.ipynb
    ‚îî‚îÄ‚îÄ tool3_quality_demo.ipynb
```

### 9.2 Data Flow Diagram

```mermaid
sequenceDiagram
    participant User
    participant Tool0
    participant Tool1
    participant Tool2
    participant Tool3
    participant Tool7
    participant Artifacts

    User->>Tool0: business_request.md
    Tool0->>Artifacts: business_context.json

    Tool0->>Tool1: business_context.json
    User->>Tool1: BA-BS_Datamarts_metadata.json
    Tool1->>Artifacts: filtered_dataset.json

    Tool1->>Tool2: filtered_dataset.json + metadata
    Tool2->>Artifacts: structure.json

    Tool2->>Tool3: structure.json + metadata
    Tool3->>Artifacts: quality_report.json

    Tool2->>Tool7: structure.json
    Tool3->>Tool7: quality_report.json
    Tool0->>Tool7: business_context.json
    Tool7->>Artifacts: governance_report.md

    Artifacts->>User: Final artifacts package
```

### 9.3 API Keys & Configuration

```bash
# Environment variables
export OPENAI_API_KEY="sk-proj-..."
export LANGCHAIN_API_KEY="lsv2_pt_..."  # Optional: LangSmith tracing
export LANGCHAIN_TRACING_V2="true"      # Optional: Enable tracing
export LANGCHAIN_PROJECT="mcop-mvp"     # Optional: Project name

# Python venv
source venv/bin/activate  # macOS/Linux
# OR
.\venv\Scripts\activate   # Windows

# Dependencies
pip install langgraph langchain langchain-openai pydantic python-dotenv
```

---

## 10. Deployment & Runtime

### 10.1 Local Development (Jupyter)

```bash
# 1. Activate environment
cd archi-agent
source venv/bin/activate

# 2. Set API key
export OPENAI_API_KEY="sk-proj-..."

# 3. Run individual tools
jupyter notebook notebooks/tool1_ingest_demo.ipynb
jupyter notebook notebooks/tool2_structure_demo.ipynb

# 4. Or run full orchestrator
jupyter notebook notebooks/mcop_orchestrator.ipynb
```

### 10.2 Production (Python Script)

```python
# mcop_runner.py
from pathlib import Path
from mcop.orchestrator import MCOPOrchestrator

# Initialize
orchestrator = MCOPOrchestrator(
    business_request_path="data/business_request.md",
    metadata_export_path="docs_langgraph/BA-BS_Datamarts_metadata.json",
    output_dir="data/outputs"
)

# Execute
result = orchestrator.run()

# Check results
if result["status"] == "success":
    print(f"‚úÖ Governance report: {result['governance_report_path']}")
    print(f"üìä Coverage: {result['metrics']['coverage']*100:.1f}%")
else:
    print(f"‚ùå Failed: {result['error']}")
```

### 10.3 CI/CD Integration (GitHub Actions)

```yaml
# .github/workflows/mcop-test.yml
name: MCOP MVP Test

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

jobs:
  test-mvp:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.13'

      - name: Install dependencies
        run: |
          pip install -r requirements.txt

      - name: Run Tool 1 tests
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        run: |
          pytest tests/test_tool1.py

      - name: Run Tool 2 tests
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        run: |
          pytest tests/test_tool2.py

      - name: Upload artifacts
        uses: actions/upload-artifact@v3
        with:
          name: mcop-outputs
          path: data/outputs/
```

### 10.4 Performance Benchmarks (MVP Target)

| Metrika             | Target | Aktu√°ln√≠ | Status |
| ------------------- | ------ | -------- | ------ |
| **Tool 0 parse**    | < 5s   | ~3s      | ‚úÖ      |
| **Tool 1 ingest**   | < 40s  | ~30s     | ‚úÖ      |
| **Tool 2 classify** | < 20s  | ~15s     | ‚úÖ      |
| **Tool 3 validate** | < 5s   | ~2s      | ‚úÖ      |
| **Tool 7 report**   | < 15s  | ~10s     | ‚úÖ      |
| **Total pipeline**  | < 90s  | ~61s     | ‚úÖ      |
| **Memory usage**    | < 2GB  | ~1.2GB   | ‚úÖ      |

### 10.5 Monitoring & Logging

```python
# Enable LangSmith tracing
import os
os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_PROJECT"] = "mcop-mvp"

# Custom logging
import logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/mcop.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger('mcop')
logger.info("Starting MCOP pipeline...")
```

---

## P≈ô√≠lohy

### A. Glossary

| Term√≠n                 | Definice                                                    |
| ---------------------- | ----------------------------------------------------------- |
| **Fact table**         | Transakƒçn√≠ tabulka s mƒõ≈ô√≠tky (measures) a FK na dimenze     |
| **Dimension table**    | Popisn√° tabulka s business keys a atributy                  |
| **Grain**              | √örove≈à detailu faktov√© tabulky (nap≈ô. "polo≈æka objedn√°vky") |
| **Articulation Score** | Collibra metrika kvality dokumentace (0-1)                  |
| **Scope Out**          | Seznam t√©mat/oblast√≠ vylouƒçen√Ωch z anal√Ωzy                  |
| **Coverage**           | % business entit namapovan√Ωch na technick√° metadata         |
| **ToolStrategy**       | LangChain wrapper pro strukturovan√Ω v√Ωstup z LLM            |
| **TypedDict**          | Python typ pro definici state structure v LangGraph         |

### B. References

- **LangGraph Docs:** https://docs.langchain.com/langgraph
- **Pydantic v2:** https://docs.pydantic.dev/2.0/
- **Mermaid:** https://mermaid.js.org/
- **Collibra API:** https://developer.collibra.com/
- **Databricks Unity Catalog:** https://docs.databricks.com/data-governance/unity-catalog/

### C. Contact

- **Project Lead:** Marek Minarovic
- **Repository:** github.com/minarovic/archi-agent
- **Documentation:** `/archi-agent/docs_langgraph/`

---

**Verze dokumentu:** 1.0.0
**Posledn√≠ aktualizace:** 2 listopad 2025
**Status:** ‚úÖ MVP Architecture Complete
